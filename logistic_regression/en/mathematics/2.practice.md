# Practical Example — Study Hours vs. Probability of Passing

In this practical example, we use a hypothetical dataset that relates each student’s number of study hours to the probability of passing an exam, applying logistic regression to model this relationship in a binary classification setting.  
The dataset consists of four representative observations—students who studied for 1, 2, 4, and 5 hours—and their corresponding outcomes (passed = 1 or failed = 0), illustrating how continuous variables can be mapped to probabilities of a discrete event.

## Application
| Study Hours (x) | Passed (y) |
|------------------|------------|
|        1         |     0      | 
|        2         |     0      |
|        4         |     1      |
|        5         |     1      |

Initially, both $\beta_0$ and $\beta_1$ are assumed to be 0, along with the learning rate (0.1). The learning rate is chosen by the practitioner and not derived mathematically from the model, as it influences the optimization process rather than the statistical structure.

---

### Iteration 1

$\beta_0^{0}$ = 0 ; $\beta_1^{0}$ = 0 ; $\alpha$ = 0.1

#### Step 1: Calculating each $z^{(i)}$:

- $z^{(1)}$ = $\beta_0$ + $\beta_1\cdot x_1$ = $0 + 0\cdot1$ = 0 $\therefore$ $z^{(1)}$ = 0
- $z^{(2)}$ = $\beta_0$ + $\beta_1\cdot x_2$ = $0 + 0\cdot2$ = 0 $\therefore$ $z^{(2)}$ = 0
- $z^{(3)}$ = $\beta_0$ + $\beta_1\cdot x_3$ = $0 + 0\cdot4$ = 0 $\therefore$ $z^{(3)}$ = 0
- $z^{(4)}$ = $\beta_0$ + $\beta_1\cdot x_4$ = $0 + 0\cdot5$ = 0 $\therefore$ $z^{(4)}$ = 0

#### Step 2: Found each value of $\sigma$ given $z^{i}$ - a.k.a doing predictions - $\hat y^{(i)}$

- $\sigma(z^{(1)})$ = $\frac{1}{1 + e^{-z}}$ = $\frac{1}{1 + e^{-0}}$ $\therefore$ $\sigma(z^{(1)})$ = 0.5
- $\sigma(z^{(2)})$ = $\frac{1}{1 + e^{-z}}$ = $\frac{1}{1 + e^{-0}}$ $\therefore$ $\sigma(z^{(2)})$ = 0.5
- $\sigma(z^{(3)})$ = $\frac{1}{1 + e^{-z}}$ = $\frac{1}{1 + e^{-0}}$ $\therefore$ $\sigma(z^{(3)})$ = 0.5
- $\sigma(z^{(4)})$ = $\frac{1}{1 + e^{-z}}$ = $\frac{1}{1 + e^{-0}}$ $\therefore$ $\sigma(z^{(4)})$ = 0.5

#### Step 3: Calculating the gradients

- $\frac{\partial{\ell}}{\partial{\beta_0}}$ = $\sum(y^{(i)} - \hat y^{(i)})$ = $(0 - 0.5) + (0 - 0.5) + (1 - 0.5) + (1 - 0.5) \therefore \frac{\partial{\ell}}{\partial{\beta_0}} = 0$
- $\frac{\partial{\ell}}{\partial{\beta_1}}$ = $\sum(y^{(i)} - \hat y^{(i)})\cdot x^{(i)}$ = $[(0 - 0.5)\cdot1] + [(0 - 0.5)\cdot2] + [(1 - 0.5)\cdot4] + [(1 - 0.5)\cdot5] \therefore \frac{\partial{\ell}}{\partial{\beta_1}} = 3$

#### Step 4: Using the gradient ascending to update the weights - like any supercomputer does

- $\beta_0^{(1)} = \beta_0^{0} + \alpha \cdot \frac{\partial{\ell}}{\partial{\beta_0}} = 0 + 0.1 \cdot 0 \therefore \beta_0^{(1)} = 0$
- $\beta_1^{(1)} = \beta_1^{0} + \alpha \cdot \frac{\partial{\ell}}{\partial{\beta_1}} = 0 + 0.1 \cdot 3 \therefore \beta_1^{(1)} = 0.3$

#### Finally, after the first iteration our model looks like this

<div align="center">
  <img src="../../figures/1iter.png" alt="logistic-summary-banner" width="800">
</div>

### Iteration 2

$\beta_0^{0}$ = 0 ; $\beta_1^{0}$ = 0.3 ; $\alpha$ = 0.1

#### Step 1: Calculating each $z^{(i)}$
- $z^{(1)} = 0 + 0.30\cdot1 = 0.30$
- $z^{(2)} = 0 + 0.30\cdot2 = 0.60$
- $z^{(3)} = 0 + 0.30\cdot4 = 1.20$
- $z^{(4)} = 0 + 0.30\cdot5 = 1.50$

#### Step 2: Predictions $\hat y^{(i)}$
- $\hat y^{(1)} = \frac{1}{1+e^{-0.30}} \approx 0.5744$
- $\hat y^{(2)} = \frac{1}{1+e^{-0.60}} \approx 0.6457$
- $\hat y^{(3)} = \frac{1}{1+e^{-1.20}} \approx 0.7685$
- $\hat y^{(4)} = \frac{1}{1+e^{-1.50}} \approx 0.8176$

#### Step 3: Calculating the gradients
- $\displaystyle \frac{\partial \ell}{\partial \beta_0}
  = (-0.5744)+(-0.6457)+(0.2315)+(0.1824)
  \approx -0.8062$
- $\displaystyle \frac{\partial \ell}{\partial \beta_1}
  = (-0.5744\cdot1)+(-0.6457\cdot2)+(0.2315\cdot4)+(0.1824\cdot5)
  \approx -0.0278$

#### Step 4: Updating the weights
- $\displaystyle \beta_{0}^{(2)}
  = 0 + 0.1 \times (-0.8062)
  \approx -0.0806$
- $\displaystyle \beta_{1}^{(2)}
  = 0.30 + 0.1 \times (-0.0278)
  \approx 0.2972$

#### The second iteration

<div align="center">
  <img src="../../figures/2iter.png" alt="logistic-summary-banner" width="800">
</div>

---

### Iteration 3

$\displaystyle \beta_{0}^{(2)} \approx -0.0806,\quad \beta_{1}^{(2)} \approx 0.2972,\quad \alpha = 0.1$

#### Step 1: Calculating each $z^{(i)}$
- $z^{(1)} = -0.0806 + 0.2972\cdot1 \approx 0.2166$
- $z^{(2)} = -0.0806 + 0.2972\cdot2 \approx 0.5138$
- $z^{(3)} = -0.0806 + 0.2972\cdot4 \approx 1.1082$
- $z^{(4)} = -0.0806 + 0.2972\cdot5 \approx 1.4054$

#### Step 2: Predictions $\hat y^{(i)}$
- $\hat y^{(1)} = \frac{1}{1+e^{-0.2166}} \approx 0.5549$
- $\hat y^{(2)} = \frac{1}{1+e^{-0.5138}} \approx 0.6257$
- $\hat y^{(3)} = \frac{1}{1+e^{-1.1082}} \approx 0.7516$
- $\hat y^{(4)} = \frac{1}{1+e^{-1.4054}} \approx 0.8030$

#### Step 3: Calculating the gradients
- $\displaystyle \frac{\partial \ell}{\partial \beta_0}
  = (-0.5549)+(-0.6257)+(0.2484)+(0.1970)
  \approx -0.7352$
- $\displaystyle \frac{\partial \ell}{\partial \beta_1}
  = (-0.5549\cdot1)+(-0.6257\cdot2)+(0.2484\cdot4)+(0.1970\cdot5)
  \approx 0.1723$

#### Step 4: Updating the weights
- $\displaystyle \beta_{0}^{(3)}
  = -0.0806 + 0.1 \times (-0.7352)
  \approx -0.1541$
- $\displaystyle \beta_{1}^{(3)}
  = 0.2972 + 0.1 \times 0.1723
  \approx 0.3144$

#### The third iteration

<div align="center">
  <img src="../../figures/3iter.png" alt="logistic-summary-banner" width="800">
</div>

### Nothing change at first glimpse, but after 1000 iterations...

<div align="center">
  <img src="../../figures/1000iter.png" alt="logistic-summary-banner" width="800">
</div>

### ...We can see how the methods actually works!!

>[!NOTE]
> Interesting fact, if with 1 and 2 hours I not pass but with 4 and 5 I pass, what should be the probability of pass if I study for 3 hours?
>
> Our final model has the following weights: $\beta_0 = -9.25348$  $\beta_1= 3.19660$, so:
>
> $$P(y=1 \mid x = 3) = \frac{1}{1 + e^{-(-9.253 + 3.196 \cdot (3))}} = 0.58$$
>
>Amazing!! The model is not linear!!