
## How to find the simple linear regression equation based on a dataset
As previously explained in [1.general_representation_of_linear_regression.md](./1.general_representation_of_linear_regression.md), the simple linear regression can be represented by the equation of a straight line: 

$$
\hat{y} = \beta_0 + \beta_1 x
$$

The values Œ≤‚ÇÅ and Œ≤‚ÇÄ are unknown parameters, where Œ≤‚ÇÅ represents the slope of the line and Œ≤‚ÇÄ represents the y-intercept.

Therefore, to determine the values of Œ≤‚ÇÅ and Œ≤‚ÇÄ, we must first collect all training data and organize it into pairs of coordinates (x, y). Ultimately, we will have a set of coordinates as represented below:

$$
\(x1, y1), (x2, y2),..., (xn, yn)
$$

Where n represents the total number of data points collected. 

As explained in [2.the_least_squares_method.md](./2.the_least_squares_method.md), we can calculate the values of $\hat{\beta_1}$ and $\hat{\beta_0}$ using the equations below:

$$
\hat{\beta_1} = \frac{\sum_{i = 1}^{n} (x_i ‚Äì \bar{x} ) ( y_i ‚Äì \bar{y} )}{\sum_{i = 1}^{n} ( x_i - \bar{x} )^2}
$$

$$
\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X}
$$

- $x_i$: x values of each coordinate
- $y_i$: y values of each coordinate
- xÃÑ: simple arithmetic mean of the x values 
- yÃÑ: simple arithmetic mean of the y values

## Example

From one of the coding examples, let's analyze the relationship between hours studied and grades of a student.

| Hours Studied | Exam Score |
|--------------|------------|
| 1.5          | 50         |
| 3.0          | 55         |
| 4.5          | 65         |
| 6.0          | 70         |
| 7.5          | 80         |
| 9.0          | 85         |

Based on the data shown in the table above, we can calculate the values of Œ≤‚ÇÅ and Œ≤‚ÇÄ and define the equation of the line. For this purpose, we will define Hours Studied as x values and Exam Scores as y values. Thus, organizing all the data into pairs of coordinates, we will have the following points: (1.5, 50), (3.0, 55), (4.5, 65), (6.0, 70), (7.5, 80), (9.0, 85).

To facilitate the development of the Œ≤‚ÇÅ calculation, we will compute the variables and the summations independently, as exemplified below:

- xÃÑ = (1.5 + 3.0 + 4.5 + 6.0 + 7.5 + 9.0)/6 = 5.25
- yÃÑ =(50 + 55 + 65 + 70 + 80 + 85)/6 = 67.5
  
$$
\sum_{i = 1}^{n} (x_i ‚Äì \bar{x} ) ( y_i ‚Äì \bar{y} ) = (1.5 - 5.25)(50 - 67.5) + (3.0 - 5.25)(55 - 67.5) + (4.5 - 5.25)(65 -  67.5) + (6.0 - 5.25)(70 - 67.5) + (7.5 - 5.25)(80 - 67.5) + (9.0 - 5.25)(85 - 67.5) = 191.25
$$

$$
\sum_{i = 1}^{n} ( x_i - \bar{x} )^2 = (1.5 - 5.25)^2 + (3.0 - 5.25)^2 + (4.5 - 5.25)^2 + (6.0 - 5.25)^2 + (7.5 - 5.25)^2 + (9.0 - 5.25)^2 = 39.375
$$

By substituting the values into the original equation, we will arrive at:

$$
\hat{\beta}_1 = \frac{191.25}{39.375} = 4.85
$$

With the preceding steps completed, we are now able to calculate the value of Œ≤‚ÇÄ, as indicated below:

$$
\hat{\beta}_0 = 67.5 - 4.85*5.25 = 42.03
$$

Upon substitution of the Œ≤‚ÇÅ and Œ≤‚ÇÄ values into the linear equation, the following expression is obtained:

$$
\hat{y} = 42.05 + 4.85 x
$$

### Least Squares Error Calculation for the Exam Score Example

The least squares error \( S \) is calculated as:

$$
S = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

#### Step 1: Calculate Predicted Values ($(\hat{y}_i$\))

For each $x_i$, compute $\hat{y}_i$:

- $x = 1.5$: $\hat{y} = 42.05 + 4.85 \times 1.5 = 49.325$
- $x = 3.0$: $\hat{y} = 42.05 + 4.85 \times 3.0 = 56.6$
- $x = 4.5$: $\hat{y} = 42.05 + 4.85 \times 4.5 = 63.875$
- $x = 6.0$: $\hat{y} = 42.05 + 4.85 \times 6.0 = 71.15$
- $x = 7.5$: $\hat{y} = 42.05 + 4.85 \times 7.5 = 78.425$
- $x = 9.0$: $\hat{y} = 42.05 + 4.85 \times 9.0 = 85.7$

#### Step 2: Compute Differences (( y_i - $\hat{y}_i$\))

Subtract the predicted values from the observed values:

- \( 50 - 49.325 = 0.675 \)
- \( 55 - 56.6 = -1.6 \)
- \( 65 - 63.875 = 1.125 \)
- \( 70 - 71.15 = -1.15 \)
- \( 80 - 78.425 = 1.575 \)
- \( 85 - 85.7 = -0.7 \)

#### Step 3: Square the Differences

Square each difference:

- \( (0.675)^2 = 0.455625 \)
- \( (-1.6)^2 = 2.56 \)
- \( (1.125)^2 = 1.265625 \)
- \( (-1.15)^2 = 1.3225 \)
- \( (1.575)^2 = 2.480625 \)
- \( (-0.7)^2 = 0.49 \)

#### Step 4: Sum the Squared Differences

Add the squared differences to find \( S \):

$$
S = 0.455625 + 2.56 + 1.265625 + 1.3225 + 2.480625 + 0.49 = 8.574375
$$

#### Result

The least squares error for this model is approximately **8.57**.


## References
Universidade Federal do Rio Grande do Sul. (n.d.). Probabilidade e Estat√≠stica. Retrieved March 30, 2025, from https://www.ufrgs.br/probabilidade-estatistica/livro/livro_completo/ch7-reg-simples.html

HASTIE, Trevor; TIBSHIRANI, Robert; FRIEDMAN, Jerome. An introduction to statistical learning. 2009.

Weisberg, S. Applied linear regression (4th ed.). Wiley. Retrieved March 30, 2025, from https://www.stat.purdue.edu/~qfsong/teaching/525/book/Weisberg-Applied-Linear-Regression-Wiley.pdf

## üëæ **Contributors**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/160762179?v=4" width=115><br><sub>Maria Eduarda Vianna</sub>](https://github.com/mevianna) |  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/197432407?v=4" width=115><br><sub>Beatriz Schuelter Tartare</sub>](https://github.com/beastartare) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/178849007?v=4" width=115><br><sub>Rafaela Savaris</sub>](https://github.com/rafasavaris) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/105316221?v=4" width=115><br><sub>Vin√≠cius Muchulski</sub>](https://github.com/vini-muchulski) | 
| :---: | :---: | :---: | :---: |
