# English version

## General representation of linear regression

Mathematically, linear regression is a model used to predict numerical values based on one or more independent variables. It assumes the relationship between the dependent variable and the features is linear.

Therefore, it is represented by a linear equation, such as:

$$
\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n
$$

- **$\hat{y}$**: The dependent variable or the predicted value.
- **$\beta_0$**: The intercept term, which corresponds to the value of $y$ when all $x$ values are zero.
- **$\beta_1, \beta_2, \dots, \beta_n$**: The coefficients for each independent variable, which represent how much each variable influences the prediction of y.
- **$x_1, x_2, \dots, x_n$**: The independent variables or features that are used to predict y.

If a model has two or more independent values, it is called **multiple linear regression**. However, this explanation will focus more on the simplest case, in which there is only one independent variable (**simple linear regression**).

![Gif1](figures/gif1.gif)

### Simple Linear Regression (two dimensions)

In two dimensions, we can think about linear regression as a straight line, which follows the equation:

$$
\hat{y} = \beta_0 + \beta_1 x_1
$$

- **$\hat{y}$**: Dependent variable (response).
- **$\beta_0$**: Intercept.
- **$\beta_1$**: Coefficient of the independent variable.
- **$x_1$**: Independent variable (feature).

This representation can be viewed in figure 1:

![Figure 1](figures/figure1.png)

The blue dots represent the actual values collected from real-world data. The straight line shows the model's predictions.

> [!IMPORTANT]
> Since the blue dots are not exactly where the straight line is, there is an associated error that we must consider when using linear regression. In order to calculate the error (in both simple and multiple linear regression), do the following:
 
$$
e = y - \hat{y}
$$

- **$e$**: Error
- **$\hat{y}$**: Predicted value from the model.
- **$y$**: Real-world value. 

### Exercise

By analyzing a graph, such as the figure 1 one, how can we determine which blue dots have a bigger error?
<details>
  <summary>Click to see the answer</summary>
  The farther from the straight line the dot is, the bigger the error. Our goal is to have the blue dots as close as possible to the line, as this makes the model more accurate.
</details>

## References
Universidade Federal do Rio Grande do Sul. (n.d.). Probabilidade e EstatÃ­stica. Retrieved March 30, 2025, from https://www.ufrgs.br/probabilidade-estatistica/livro/livro_completo/ch7-reg-simples.html

HASTIE, Trevor; TIBSHIRANI, Robert; FRIEDMAN, Jerome. An introduction to statistical learning. 2009.

Weisberg, S. Applied linear regression (4th ed.). Wiley. Retrieved March 30, 2025, from https://www.stat.purdue.edu/~qfsong/teaching/525/book/Weisberg-Applied-Linear-Regression-Wiley.pdf


## Where am I?

```text
RepoAI/
â””â”€â”€ Linear Regression/
    â”œâ”€â”€ 1.Concepts/
    â”‚   â””â”€â”€ Figures/
    â”‚   â””â”€â”€ 1.History.md
    |   â””â”€â”€ 2.Typical_problems.md
    |   â””â”€â”€ 3.Potential_issues.md
    |   â””â”€â”€ 5.Fields_of_use.md
    â”œâ”€â”€ 2.Code/
    |   â””â”€â”€ Figures/
    |   â””â”€â”€ 1.Dive_into_the_docs.md  
    |   â””â”€â”€ 2.Boston_housing_price_reg.md
    |   â””â”€â”€ 3.Student_study_hours_prediction.md
    â””â”€â”€ 3.Mathematics/
    |   â””â”€â”€ Figures/
    |   â””â”€â”€ 1.General_representation_of_linear_regression.md <---- You are here!! 
    |   â””â”€â”€ 2.The_least_square_method.md
    |   â””â”€â”€ 3.Equation_based_on_dataset.md   
```

## ğŸ‘¾ **Contributors**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/160762179?v=4" width=115><br><sub>Maria Eduarda Vianna</sub>](https://github.com/mevianna) |  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/197432407?v=4" width=115><br><sub>Beatriz Schuelter Tartare</sub>](https://github.com/beastartare) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/178849007?v=4" width=115><br><sub>Rafaela Savaris</sub>](https://github.com/rafasavaris) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/105316221?v=4" width=115><br><sub>VinÃ­cius Muchulski</sub>](https://github.com/vini-muchulski) | 
| :---: | :---: | :---: | :---: |

## **License**  
[![LicenÃ§a MIT](https://img.shields.io/badge/LicenÃ§a-MIT-blue.svg)](https://pt.wikipedia.org/wiki/Licen%C3%A7a_MIT)  
**Traslation:** Use, modify, and share at will! âœŒï¸

# Portuguese version
 
## RepresentaÃ§Ã£o geral da regressÃ£o linear

Matematicamente, a regressÃ£o linear Ã© um modelo usado para prever um valor numÃ©rico baseado em uma ou mais variÃ¡veis independentes. Ele assume que a relaÃ§Ã£o entre a variÃ¡vel dependente e as caracterÃ­ticas Ã© linear.

Portanto, Ã© representado por uma equaÃ§Ã£o linear:

$$
  \hat{y} = \beta_0 + \beta_1 x\_1 + \cdots + \beta_n x_n
$$

- **$\hat{y}$**: VariÃ¡vel dependente ou valor previsto.
- **$\beta_0$**: Valor de intersecÃ§Ã£o em $y$ para quando todos os valores de $x$ sÃ£o zero.
- **$\beta_1, \beta_2, \cdots, \beta_n$**: Coeficientes de cada variÃ¡vel independente, que representam o quanto cada variÃ¡vel influencia na prediÃ§Ã£o de y.
- **$x_1, x_2, \dots, x_n$**: VariÃ¡veis independentes ou caracterÃ­sticas que sÃ£o usadas para prever y.

Se um modelo tem dois ou mais valores independentes, Ã© chamado de **regressÃ£o linear mÃºltipla**. No entanto, esta explicaÃ§Ã£o focarÃ¡ no caso mais simples, em que hÃ¡ apenas uma variÃ¡vel independente (**regressÃ£o linear simples**).

![Gif1](figures/gif1.gif)

### RegressÃ£o linear simples (duas dimensÃµes)

Em duas dimensÃµes, podemos pensar na regressÃ£o linear como sendo uma linha reta, que segue a equaÃ§Ã£o:

$$
\hat{y} = \beta_0 + \beta_1 x_1
$$

- **$\hat{y}$**: VariÃ¡vel dependente
- **$\beta_0$**: Intercepto.
- **$\beta_1$**: Coeficiente da variÃ¡vel independente.
- **$x_1$**: VariÃ¡vel independente (caracterÃ­stica).

Esta representaÃ§Ã£o do modelo pode ser vista na figura 1:

![Figure1-pt](figures/figure1.png)

Os pontos em azul representam os valores coletados de dados reais. A linha reta mostra a prediÃ§Ã£o do modelo.

> [!IMPORTANT]
> Tendo em vista que os pontos azuis nÃ£o estÃ£o exatamente onde a linha reta estÃ¡, hÃ¡ um erro associado que devemos considerar quando utilizarmos regressÃ£o linear. Para calcular o erro (tanto na regressÃ£o linear simples quanto na mÃºltipla), faÃ§a o seguinte:

$$
e = y - \hat{y}
$$

- **$e$**: Erro
- **$\hat{y}$**: Valor previsto pelo modelo.
- **$y$**: Valor real. 


### ExercÃ­cio

Ao analisarmos um grÃ¡fico, como o da figura 1, como podemos determinar qual ponto azul tem maior erro?
<details>
  <summary>Clique para ver a resposta</summary>
  Quanto mais longe o ponto estÃ¡ da linha reta, maior o erro. Nosso objetivo Ã© ter os pontos azuis o mais perto possÃ­vel da linha, haja vista que isso faz o modelo mais preciso.
</details>

## ReferÃªncias
UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL. Probabilidade e EstatÃ­stica. DisponÃ­vel em: https://www.ufrgs.br/probabilidade-estatistica/livro/livro_completo/ch7-reg-simples.html. Acesso em: 30 mar. 2025.

HASTIE, Trevor; TIBSHIRANI, Robert; FRIEDMAN, Jerome. An introduction to statistical learning. 2009.

WEISBERG, Sanford. Applied linear regression. 4. ed. Hoboken: Wiley, [s.d.]. DisponÃ­vel em: https://www.stat.purdue.edu/~qfsong/teaching/525/book/Weisberg-Applied-Linear-Regression-Wiley.pdf. Acesso em: 30 mar. 2025.

## Onde estou?

```text
RepoAI/
â””â”€â”€ Linear Regression/
    â”œâ”€â”€ 1.Concepts/
    â”‚   â””â”€â”€ Figures/
    â”‚   â””â”€â”€ 1.History.md
    |   â””â”€â”€ 2.Typical_problems.md
    |   â””â”€â”€ 3.Potential_issues.md
    |   â””â”€â”€ 4.Fields_of_use.md
    â”œâ”€â”€ 2.Code/
    |   â””â”€â”€ Figures/
    |   â””â”€â”€ 1.Dive_into_the_docs.md  
    |   â””â”€â”€ 2.Boston_housing_price_reg.md
    |   â””â”€â”€ 3.Student_study_hours_prediction.md
    â””â”€â”€ 3.Mathematics/
    |   â””â”€â”€ Figures/
    |   â””â”€â”€ 1.General_representation_of_linear_regression.md <---- VocÃª estÃ¡ aqui!! 
    |   â””â”€â”€ 2.The_least_square_method.md
    |   â””â”€â”€ 3.Equation_based_on_dataset.md   
```

## ğŸ‘¾ **Contribuidores**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/160762179?v=4" width=115><br><sub>Maria Eduarda Vianna</sub>](https://github.com/mevianna) |  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/197432407?v=4" width=115><br><sub>Beatriz Schuelter Tartare</sub>](https://github.com/beastartare) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/178849007?v=4" width=115><br><sub>Rafaela Savaris</sub>](https://github.com/rafasavaris) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/105316221?v=4" width=115><br><sub>VinÃ­cius Muchulski</sub>](https://github.com/vini-muchulski) | 
| :---: | :---: | :---: | :---: |

## **LicenÃ§a**  
[![LicenÃ§a MIT](https://img.shields.io/badge/LicenÃ§a-MIT-blue.svg)](https://pt.wikipedia.org/wiki/Licen%C3%A7a_MIT)  
**Traslation:** Use, modifique e compartilhe Ã  vontade! âœŒï¸
