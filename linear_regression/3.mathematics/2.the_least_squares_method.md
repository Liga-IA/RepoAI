# English version

## The least squares method

Let's say we want to determine the equation of the linear regression model. It would be wise to think that we should have an equation that minimizes errors.
 
 The most common method for determining the best equation to represent the predictions is the least squares method, in which we minimize the sum of squared errors: $e_1^2 + e_2^2 + \dots + e_n^2$.

 > [!NOTE]
> As explained in [1.general_representation_of_linear_regression.md](./1.general_representation_of_linear_regression.md), the error $e_i$ represents the difference between the observed value $y_i$ and the predicted value $\hat{y}$: $e = y - \hat{y}_i$.

Therefore, more formally, the equation can be represented as:
$$S = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$

Here:
* $y_i$ represents the observed value of the dependent variable for each observation $i$;
* $\hat{y}_i$ represents the value estimated by the model;
* $S$ is the total sum of squared residuals, which we want to minimize;
* $N$ is the total number of observations.
 
 > [!IMPORTANT]
 > Think about why we minimize the squared errors instead of simply minimizing the sum of the errors ($|e_1| + |e_2| + \dots + |e_n|$).
 <details>
   <summary>Click to see the answer</summary>
   One reason is that squaring the errors makes bigger ones count more, helping the model focus on fixing those larger errors and giving a better overall fit to the data 
 </details>

### Using the least squares method for simple linear regression

As mentioned in [1.general_representation_of_linear_regression.md](./1.general_representation_of_linear_regression.md), the estimated value can be expressed using the equation of the regression line: $\hat{y} = \beta_0 + \beta_1 x_1$. By substituting this equation into the error expression, the result is:

$$
e = y - (\beta_0 + \beta_1 x_1)
$$

Therefore, the general equation for the sum of squared residuals is expressed as:

$$ S = \sum_{i=1}^{N} (y - [\beta_0 + \beta_1 x_1])^2 $$

To determine the values of $\beta_0$ and $\beta_0$, partial derivatives are taken with respect to each parameter and set to zero. The partial derivative with respect to $\beta_0$ is:

$$
\frac{\partial S}{\partial \beta_0} = -2 \sum_{i=1}^{N} (y_i - \beta_0 - \beta_1 x_i) = 0
$$

Solving for $\beta_0$, the equation simplifies to: $\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X}$, where **$(\bar{Y})$** is the mean of the observed values of the dependent variable ($y$) and **$(\bar{X})$** is the mean of the values of the independent variable ($x$). For $\beta_1$, the partial derivative is given by:

$$
\frac{\partial S}{\partial \beta_1} = -2 \sum_{i=1}^{N} (y_i - \beta_0 - \beta_1 x_i) x_i = 0
$$

Rearranging the equation leads to:

$$
\hat{\beta_1} = \frac{\sum_{i = 1}^{N} (x_i ‚Äì \bar{x} ) ( y_i ‚Äì \bar{y} )}{\sum_{i = 1}^{N} ( x_i - \bar{x} )^2}
$$

> For multiple linear regression, the least squares method is also used. However, the final equation is expressed in matrix form as:
> $\hat{B} = (X^TX)^{-1}X^TE$
> Here:
> * $X$ is a matrix containing the independent variables;
> * $E$ is the vector with the dependent variable values;
> * $\hat{B}$ is the vector with the coefficients we want (values of $A$, $B$, etc.).

## References
Universidade Federal do Rio Grande do Sul. (n.d.). Probabilidade e Estat√≠stica. Retrieved March 30, 2025, from https://www.ufrgs.br/probabilidade-estatistica/livro/livro_completo/ch7-reg-simples.html

HASTIE, Trevor; TIBSHIRANI, Robert; FRIEDMAN, Jerome. An introduction to statistical learning. 2009.

Weisberg, S. Applied linear regression (4th ed.). Wiley. Retrieved March 30, 2025, from https://www.stat.purdue.edu/~qfsong/teaching/525/book/Weisberg-Applied-Linear-Regression-Wiley.pdf


## üëæ **Contributors**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/160762179?v=4" width=115><br><sub>Maria Eduarda Vianna</sub>](https://github.com/mevianna) |  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/197432407?v=4" width=115><br><sub>Beatriz Schuelter Tartare</sub>](https://github.com/beastartare) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/178849007?v=4" width=115><br><sub>Rafaela Savaris</sub>](https://github.com/rafasavaris) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/105316221?v=4" width=115><br><sub>Vin√≠cius Muchulski</sub>](https://github.com/vini-muchulski) | 
| :---: | :---: | :---: | :---: |

# Portuguese version

## O m√©todo dos m√≠nimos quadrados

Digamos que a gente queira determinar a equa√ß√£o do modelo de regress√£o linear. Seria interessante pensarmos em termos uma equa√ß√£o que minimize erros.

O m√©todo mais comum para determin√°-la √© o dos m√≠nimos quadrados, em que minimizamos a soma do quadrado dos erros: $e_1^2 + e_2^2 + \dots + e_n^2$.

 > [!NOTE]
> Como explicado em [1.general_representation_of_linear_regression.md](./1.general_representation_of_linear_regression.md), o erro $e_i$ representa a diferen√ßa entre o valor observado $y_i$ e o valor previsto $\hat{y}$: $e = y - \hat{y}_i$.

Portanto, mais formalmente, a equa√ß√£o pode ser representada da seguinte maneira:
$$S = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$

Aqui:
* $y_i$ representa o valor observado de cada vari√°vel dependente para cada observa√ß√£o $i$;
* $\hat{y}_i$ representa o valor estimado pelo modelo;
* $S$ √© a soma total dos erros ao quadrado, a qual queremos minimizar;
* $N$ √© o n√∫mero total de observa√ß√µes.
 
 > [!IMPORTANT]
 > Pense no motivo pelo qual queremos minimizar os erros quadr√°ticos em vez de simplesmente minimizarmos a soma dos erros ($|e_1| + |e_2| + \dots + |e_n|$).
 <details>
   <summary>Click to see the answer</summary>
   Um dos motivos √© que elevar os erros ao quadrado faz com que os maiores tenham mais peso, ajudando o modelo a focar na corre√ß√£o desses erros maiores e proporcionando um ajuste geral melhor aos dados.
 </details>

### Usando o m√©todo dos m√≠nimos quadrados para regress√£o linear simples

Como mencionado em [1.general_representation_of_linear_regression.md](./1.general_representation_of_linear_regression.md), o valor estimado pode ser calculado utilizando a equa√ß√£o da regress√£o linear: $\hat{y} = \beta_0 + \beta_1 x_1$. Substituindo-a na express√£o que calcula o erro, temos:

$$
e = y - (\beta_0 + \beta_1 x_1)
$$

Portanto, a equa√ß√£o geral para a soma dos m√≠nimos quadrados √©:

$$ S = \sum_{i=1}^{N} (y - [\beta_0 + \beta_1 x_1])^2 $$

Para determinar os valores de $\beta_0$ e $\beta_0$, s√£o tomadas as derivadas parciais em rela√ß√£o a cada par√¢metro e igualadas a zero. A derivada parcial em rela√ß√£o a $\beta_0$ √©:

$$
\frac{\partial S}{\partial \beta_0} = -2 \sum_{i=1}^{N} (y_i - \beta_0 - \beta_1 x_i) = 0
$$

Resolvendo para $\beta_0$, a equa√ß√£o se simplifica para: $\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X}$, onde **$(\bar{Y})$** √© a m√©dia dos valores observados da vari√°vel dependente ($y$) e **$(\bar{X})$** √© m√©dia dos valores da vari√°vel independente ($x$). J√° para $\beta_1$, a derivada parcial √© dada por:

$$
\frac{\partial S}{\partial \beta_1} = -2 \sum_{i=1}^{N} (y_i - \beta_0 - \beta_1 x_i) x_i = 0
$$

Rearranjando a equa√ß√£o, obt√©m-se: 

$$
\hat{\beta_1} = \frac{\sum_{i = 1}^{N} (x_i ‚Äì \bar{x} ) ( y_i ‚Äì \bar{y} )}{\sum_{i = 1}^{N} ( x_i - \bar{x} )^2}
$$

> Para regress√£o linear m√∫ltipla, o m√©todo dos m√≠nimos quadrados tamb√©m √© utilizado. No entanto, a equa√ß√£o final pode ser expressada de forma matricial:
> $\hat{B} = (X^TX)^{-1}X^TE$
> Here:
> * $X$ √© a matriz contendo as vari√°veis independentes;
> * $E$ √© o vetor com os valores das vari√°veis dependentes;
> * $\hat{B}$ √© o vetor com os coeficientes que queremos (valores de $A$, $B$, etc.).

## Refer√™ncias
UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL. Probabilidade e Estat√≠stica. Dispon√≠vel em: https://www.ufrgs.br/probabilidade-estatistica/livro/livro_completo/ch7-reg-simples.html. Acesso em: 30 mar. 2025.

HASTIE, Trevor; TIBSHIRANI, Robert; FRIEDMAN, Jerome. An introduction to statistical learning. 2009.

WEISBERG, Sanford. Applied linear regression. 4. ed. Hoboken: Wiley, [s.d.]. Dispon√≠vel em: https://www.stat.purdue.edu/~qfsong/teaching/525/book/Weisberg-Applied-Linear-Regression-Wiley.pdf. Acesso em: 30 mar. 2025.


## üëæ **Contribuidores**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/160762179?v=4" width=115><br><sub>Maria Eduarda Vianna</sub>](https://github.com/mevianna) |  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/197432407?v=4" width=115><br><sub>Beatriz Schuelter Tartare</sub>](https://github.com/beastartare) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/178849007?v=4" width=115><br><sub>Rafaela Savaris</sub>](https://github.com/rafasavaris) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/105316221?v=4" width=115><br><sub>Vin√≠cius Muchulski</sub>](https://github.com/vini-muchulski) | 
| :---: | :---: | :---: | :---: |

