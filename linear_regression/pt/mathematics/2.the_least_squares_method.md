
## O m√©todo dos m√≠nimos quadrados

Digamos que a gente queira determinar a equa√ß√£o do modelo de regress√£o linear. Seria interessante pensarmos em termos uma equa√ß√£o que minimize erros.

O m√©todo mais comum para determin√°-la √© o dos m√≠nimos quadrados, em que minimizamos a soma do quadrado dos erros: $e_1^2 + e_2^2 + \dots + e_n^2$.

 > [!NOTE]
> Como explicado em [1.general_representation_of_linear_regression.md](./1.general_representation_of_linear_regression.md), o erro $e_i$ representa a diferen√ßa entre o valor observado $y_i$ e o valor previsto $\hat{y}$: $e = y - \hat{y}_i$.

Portanto, mais formalmente, a equa√ß√£o pode ser representada da seguinte maneira:
$$S = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$

Aqui:
* $y_i$ representa o valor observado de cada vari√°vel dependente para cada observa√ß√£o $i$;
* $\hat{y}_i$ representa o valor estimado pelo modelo;
* $S$ √© a soma total dos erros ao quadrado, a qual queremos minimizar;
* $N$ √© o n√∫mero total de observa√ß√µes.
 
 > [!IMPORTANT]
 > Pense no motivo pelo qual queremos minimizar os erros quadr√°ticos em vez de simplesmente minimizarmos a soma dos erros ($|e_1| + |e_2| + \dots + |e_n|$).
 <details>
   <summary>Click to see the answer</summary>
   Um dos motivos √© que elevar os erros ao quadrado faz com que os maiores tenham mais peso, ajudando o modelo a focar na corre√ß√£o desses erros maiores e proporcionando um ajuste geral melhor aos dados.
 </details>

### Usando o m√©todo dos m√≠nimos quadrados para regress√£o linear simples

Como mencionado em [1.general_representation_of_linear_regression.md](./1.general_representation_of_linear_regression.md), o valor estimado pode ser calculado utilizando a equa√ß√£o da regress√£o linear: $\hat{y} = \beta_0 + \beta_1 x_1$. Substituindo-a na express√£o que calcula o erro, temos:

$$
e = y - (\beta_0 + \beta_1 x_1)
$$

Portanto, a equa√ß√£o geral para a soma dos m√≠nimos quadrados √©:

$$ S = \sum_{i=1}^{N} (y - [\beta_0 + \beta_1 x_1])^2 $$

Para determinar os valores de $\beta_0$ e $\beta_0$, s√£o tomadas as derivadas parciais em rela√ß√£o a cada par√¢metro e igualadas a zero. A derivada parcial em rela√ß√£o a $\beta_0$ √©:

$$
\frac{\partial S}{\partial \beta_0} = -2 \sum_{i=1}^{N} (y_i - \beta_0 - \beta_1 x_i) = 0
$$

Resolvendo para $\beta_0$, a equa√ß√£o se simplifica para: $\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X}$, onde **$(\bar{Y})$** √© a m√©dia dos valores observados da vari√°vel dependente ($y$) e **$(\bar{X})$** √© m√©dia dos valores da vari√°vel independente ($x$). J√° para $\beta_1$, a derivada parcial √© dada por:

$$
\frac{\partial S}{\partial \beta_1} = -2 \sum_{i=1}^{N} (y_i - \beta_0 - \beta_1 x_i) x_i = 0
$$

Rearranjando a equa√ß√£o, obt√©m-se: 

$$
\hat{\beta_1} = \frac{\sum_{i = 1}^{N} (x_i ‚Äì \bar{x} ) ( y_i ‚Äì \bar{y} )}{\sum_{i = 1}^{N} ( x_i - \bar{x} )^2}
$$

> Para regress√£o linear m√∫ltipla, o m√©todo dos m√≠nimos quadrados tamb√©m √© utilizado. No entanto, a equa√ß√£o final pode ser expressada de forma matricial:
> $\hat{B} = (X^TX)^{-1}X^TE$
> Here:
> * $X$ √© a matriz contendo as vari√°veis independentes;
> * $E$ √© o vetor com os valores das vari√°veis dependentes;
> * $\hat{B}$ √© o vetor com os coeficientes que queremos (valores de $A$, $B$, etc.).

## Refer√™ncias
UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL. Probabilidade e Estat√≠stica. Dispon√≠vel em: https://www.ufrgs.br/probabilidade-estatistica/livro/livro_completo/ch7-reg-simples.html. Acesso em: 30 mar. 2025.

HASTIE, Trevor; TIBSHIRANI, Robert; FRIEDMAN, Jerome. An introduction to statistical learning. 2009.

WEISBERG, Sanford. Applied linear regression. 4. ed. Hoboken: Wiley, [s.d.]. Dispon√≠vel em: https://www.stat.purdue.edu/~qfsong/teaching/525/book/Weisberg-Applied-Linear-Regression-Wiley.pdf. Acesso em: 30 mar. 2025.


## üëæ **Contribuidores**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/160762179?v=4" width=115><br><sub>Maria Eduarda Vianna</sub>](https://github.com/mevianna) |  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/197432407?v=4" width=115><br><sub>Beatriz Schuelter Tartare</sub>](https://github.com/beastartare) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/178849007?v=4" width=115><br><sub>Rafaela Savaris</sub>](https://github.com/rafasavaris) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/105316221?v=4" width=115><br><sub>Vin√≠cius Muchulski</sub>](https://github.com/vini-muchulski) | 
| :---: | :---: | :---: | :---: |
