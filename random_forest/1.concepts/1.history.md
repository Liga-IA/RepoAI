# English version

# Introduction to Random Forest

## What is the Random Forest algorithm?

Random Forest is one of the most powerful and popular machine learning algorithms, widely used in classification and regression tasks. It stands out for its robustness, good accuracy, and ability to handle complex data.

If you aggregate the predictions of a set of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A set of predictors is called an ensemble, so this technique is called Ensemble Learning, and an Ensemble Learning algorithm is called an Ensemble method.

Random Forest is an ensemble learning technique based on decision trees, usually trained by the bagging (or sometimes pasting) method. The main idea is to build multiple independent decision trees and combine their predictions to obtain a more robust decision.

<div align="center">
  <img src="./img/forest.jpeg" alt="initial-banner" height="400" width="800">
</div>

## Origin and historical context (Leo Breiman, 2001)

The Random Forest algorithm was proposed by **Leo Breiman** in 2001. Breiman, a renowned statistician, had already made significant contributions to the field of machine learning, including the development of the **CART (Classification and Regression Trees)** algorithm, which is the basis for decision trees.

Before Random Forest, decision tree algorithms, although intuitive and easy to interpret, suffered from a fundamental problem: **overfitting**. A single decision tree, when trained deeply, tends to overfit the training data, capturing noise and specifics that do not generalize well to new data. Random Forest emerged as an elegant solution to mitigate this problem.

## Why is it called a "random forest"?

The Random Forest algorithm introduces an extra **randomness** when developing trees; Instead of finding the best feature when splitting a node, it searches for it among a **random subset** of those features, resulting in a highly diverse tree, which (again) trades off high bias for low variance, generally producing a better overall model. This reduces variance and generally results in a more robust model.

> [!NOTE]
>The name comes from the analogy with a "forest" of trees, where:
>Each decision tree is built from a **random sample of the data** (with replacement ‚Äî bootstrap).
>At each node in the tree, a **random draw of a subset of features** is made to decide the best split.

## Motivation

### Problems with single models

Single decision trees have great learning power, but are **prone to overfitting**, that is, they learn noise from the training data.

### Need for more robust and generalizable models

Random Forest emerges as a solution because:

* By **combining many independent trees** (*ensemble*), the model **reduces the variance** of the system.
* Each tree can make mistakes, but **these errors tend to cancel each other out** in the aggregation.

<div align="center">
  <img src="./img/arv_bag.png" alt="initial-banner" width="800">
</div>

>Figure: A single Decision Tree versus a bagging ensemble of 500 trees

## References

[1] G√âRON, A. M√£os √† Obra: Aprendizado de M√°quina com Scikit-Learn & TensorFlow. Alta Books, 2019.



## üëæ Contributors
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher)  
| :---: |
---

# Portuguese version


# Introdu√ß√£o ao Random Forest

## O que √© o algoritmo Random Forest?

O Random Forest (Floresta Aleat√≥ria) √© um dos algoritmos de aprendizado de m√°quina mais poderosos e populares, amplamente utilizado em tarefas de classifica√ß√£o e regress√£o. Ele se destaca por sua robustez, boa precis√£o e capacidade de lidar com dados complexos.

Se voc√™ agregar as previs√µes de um conjunto de previsores (como classificadores ou regressores), muitas vezes obter√° melhores previs√µes do que com o melhor previsor individual. Um conjunto de previsores √© chamado de *ensemble*, assim, esta t√©cnica √© chamada *Ensemble Learning*, e um algoritmo de *Ensemble Learning* √© chamado de *Ensemble method*.

O Random Forest √© uma t√©cnica de aprendizado por conjunto (*ensemble learning*) baseada em √°rvores de decis√£o, geralmente treinado pelo m√©todo *bagging* (ou algumas vezes *pasting*). A ideia principal √© construir m√∫ltiplas √°rvores de decis√£o independentes e combinar suas previs√µes para obter uma decis√£o mais robusta.

<div align="center">
  <img src="./img/forest.jpeg" alt="initial-banner" height="400" width="800">
</div>

## Origem e contexto hist√≥rico (Leo Breiman, 2001)

O algoritmo Random Forest foi proposto por **Leo Breiman** em 2001. Breiman, um estat√≠stico renomado, j√° havia feito contribui√ß√µes significativas para a √°rea de aprendizado de m√°quina, incluindo o desenvolvimento do algoritmo **CART (Classification and Regression Trees)**, que √© a base das √°rvores de decis√£o.

Antes do Random Forest, os algoritmos de √°rvores de decis√£o, embora intuitivos e f√°ceis de interpretar, sofriam de um problema fundamental: o **overfitting** (sobreajuste). Uma √∫nica √°rvore de decis√£o, quando treinada profundamente, tende a se ajustar excessivamente aos dados de treinamento, capturando ru√≠dos e especificidades que n√£o se generalizam bem para novos dados. O Random Forest surgiu como uma solu√ß√£o elegante para mitigar esse problema.

## Por que √© chamado de "floresta aleat√≥ria"?

O algoritmo Floresta Aleat√≥ria introduz uma **aleatoriedade extra** ao desenvolver √°rvores; em vez de buscar a melhor caracter√≠stica ao dividir um n√≥, ele a busca entre um **subconjunto aleat√≥rio** dessas caracter√≠sticas, resultando em uma grande diversidade diversidade da √°rvore, que (mais uma vez) troca um alto vi√©s por uma baixa vari√¢ncia, geralmente produzindo um melhor modelo no geral . Isso reduz a vari√¢ncia e geralmente resulta em um modelo mais robusto.

> [!NOTE]
>O nome vem da analogia com uma "floresta" de √°rvores, onde:
>Cada √°rvore de decis√£o √© constru√≠da a partir de uma **amostra aleat√≥ria dos dados** (com reposi√ß√£o ‚Äî bootstrap).
>Em cada n√≥ da √°rvore, √© feito um **sorteio aleat√≥rio de um subconjunto de atributos** para decidir a melhor divis√£o (split).

## Motiva√ß√£o

### Problemas com modelos √∫nicos

As √°rvores de decis√£o √∫nicas t√™m um grande poder de aprendizagem, mas s√£o **propensas ao overfitting**, ou seja, aprendem ru√≠dos dos dados de treinamento.

### Necessidade de modelos mais robustos e generaliz√°veis

A Random Forest surge como solu√ß√£o porque:

* Ao **combinar muitas √°rvores independentes** (*ensemble*), o modelo **reduz a vari√¢ncia** do sistema.
* Cada √°rvore pode cometer erros, mas **esses erros tendem a se anular** na agrega√ß√£o.

<div align="center">
  <img src="./img/arv_bag.png" alt="initial-banner" width="800">
</div>

>Figura: Uma √Årvore de Decis√£o √∫nica versus um bagging ensemble de 500 √°rvores

## Refer√™ncias

[1] G√âRON, A. M√£os √† Obra: Aprendizado de M√°quina com Scikit-Learn & TensorFlow. Alta Books, 2019.

## üëæ Colaboradores
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher)  
| :---: |