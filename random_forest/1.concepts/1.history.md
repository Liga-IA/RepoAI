# Portuguese version

# Introdu√ß√£o ao Random Forest

## O que √© o algoritmo Random Forest?

O Random Forest (Floresta Aleat√≥ria) √© um dos algoritmos de aprendizado de m√°quina mais poderosos e populares, amplamente utilizado em tarefas de classifica√ß√£o e regress√£o. Ele se destaca por sua robustez, boa precis√£o e capacidade de lidar com dados complexos.

Se voc√™ agregar as previs√µes de um conjunto de previsores (como classificadores ou regressores), muitas vezes obter√° melhores previs√µes do que com o melhor previsor individual. Um conjunto de previsores √© chamado de *ensemble*, assim, esta t√©cnica √© chamada *Ensemble Learning*, e um algoritmo de *Ensemble Learning* √© chamado de *Ensemble method*.

O Random Forest √© uma t√©cnica de aprendizado por conjunto (*ensemble learning*) baseada em √°rvores de decis√£o, geralmente treinado pelo m√©todo *bagging* (ou algumas vezes *pasting*). A ideia principal √© construir m√∫ltiplas √°rvores de decis√£o independentes e combinar suas previs√µes para obter uma decis√£o mais robusta.

<div align="center">
  <img src="./img/forest.jpeg" alt="initial-banner" width="800">
</div>

## Origem e contexto hist√≥rico (Leo Breiman, 2001)

O algoritmo Random Forest foi proposto por **Leo Breiman** em 2001. Breiman, um estat√≠stico renomado, j√° havia feito contribui√ß√µes significativas para a √°rea de aprendizado de m√°quina, incluindo o desenvolvimento do algoritmo **CART (Classification and Regression Trees)**, que √© a base das √°rvores de decis√£o.

Antes do Random Forest, os algoritmos de √°rvores de decis√£o, embora intuitivos e f√°ceis de interpretar, sofriam de um problema fundamental: o **overfitting** (sobreajuste). Uma √∫nica √°rvore de decis√£o, quando treinada profundamente, tende a se ajustar excessivamente aos dados de treinamento, capturando ru√≠dos e especificidades que n√£o se generalizam bem para novos dados. O Random Forest surgiu como uma solu√ß√£o elegante para mitigar esse problema.

## Por que √© chamado de "floresta aleat√≥ria"?

O algoritmo Floresta Aleat√≥ria introduz uma **aleatoriedade extra** ao desenvolver √°rvores; em vez de buscar a melhor caracter√≠stica ao dividir um n√≥, ele a busca entre um **subconjunto aleat√≥rio** dessas caracter√≠sticas, resultando em uma grande diversidade diversidade da √°rvore, que (mais uma vez) troca um alto vi√©s por uma baixa vari√¢ncia, geralmente produzindo um melhor modelo no geral . Isso reduz a vari√¢ncia e geralmente resulta em um modelo mais robusto.

> [!NOTE]O nome vem da analogia com uma "floresta" de √°rvores, onde:
>Cada √°rvore de decis√£o √© constru√≠da a partir de uma **amostra aleat√≥ria dos dados** (com reposi√ß√£o ‚Äî bootstrap).

>Em cada n√≥ da √°rvore, √© feito um **sorteio aleat√≥rio de um subconjunto de atributos** para decidir a melhor divis√£o (split).

## Motiva√ß√£o

### Problemas com modelos √∫nicos

As √°rvores de decis√£o √∫nicas t√™m um grande poder de aprendizagem, mas s√£o **propensas ao overfitting**, ou seja, aprendem ru√≠dos dos dados de treinamento.

### Necessidade de modelos mais robustos e generaliz√°veis

A Random Forest surge como solu√ß√£o porque:

* Ao **combinar muitas √°rvores independentes** (*ensemble*), o modelo **reduz a vari√¢ncia** do sistema.
* Cada √°rvore pode cometer erros, mas **esses erros tendem a se anular** na agrega√ß√£o.

<div align="center">
  <img src="./img/arv_bag.png" alt="initial-banner" width="800">
</div>


## üëæ Colaboradores
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher)  
| :---: |