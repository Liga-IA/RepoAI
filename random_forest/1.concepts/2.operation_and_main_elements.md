# English version

## How Random Forest Works

## 1. Sources of Randomness

Random Forest works based on two main sources of randomness, which are crucial to the diversity and effectiveness of the tree ensemble:

### Data Sampling (Bagging - Bootstrap Aggregation)

The algorithm uses a technique called bootstrap **aggregating or bagging**, where each tree in the forest is trained on a different data sample, which is created through the **bootstrap** method. This means that, for each tree, a subset of training data is randomly selected with replacement from the original data set[1]. This sampling with replacement ensures that some instances may appear multiple times in a sample, while others may not appear in any sample, ensuring variability between the training sets of the trees[1].

### Random Attribute Selection

During the process of building each tree, at each node where a split decision needs to be made, the algorithm does not consider all available attributes. Instead, it randomly selects a subset of attributes (typically the square root of the total number of attributes for classification, or one-third for regression) and chooses the best attribute and the best split point only within this limited subset. This randomness in the attributes forced to be considered at each split helps to decouple the trees, making them less correlated [1].

The combination of these two random factors ensures that each tree in the forest is unique and explores different aspects of the data, mitigating the risk of overfitting and increasing the generalization ability of the model [1].

## 2. Tree Building Process

The construction of each tree in Random Forest follows the general process of building decision trees, but with the modifications mentioned:

### Bootstrap Sampling

For the **k-th** tree in the forest, a new training sample is generated from the original dataset using **bootstrap** sampling. This sample will be the same size as the original set, but with some instances repeated and others omitted[2].

### Tree Growth with Random Feature Selection

Each tree is built recursively:

- At each node of the tree, instead of considering all **M** available features, a number of **m** features are randomly selected (where **m < M**).

- The best split for that node is then determined using only the **m** randomly selected features, based on an impurity criterion.

- This process continues until the tree reaches a predefined maximum depth, or until a minimum number of samples per leaf node is reached. Unlike individual decision trees, trees in **Random Forest** are often grown to their maximum depth without pruning, because the combination of many diverse trees offsets the potential for overfitting of individual trees.

The resulting diversity of these trees, trained on subsets of the data and using subsets of features, is critical to Random Forest performance[1].

## 3. Bagging (Bootstrap Aggregation) Technique

The **Bagging** (Bootstrap Aggregation)** technique is the foundation of Random Forest. **Bagging** is an ensemble method that aims to improve the stability and accuracy of machine learning algorithms by reducing variance. It works as follows:

### Generating Multiple Training Sets

From the original training dataset, multiple (usually hundreds or thousands) new training sets are generated. Each of these sets is created through **bootstrap sampling**, that is, randomly selecting instances from the original set with replacement. As a result, each **bootstrap** set is slightly different from the original and from the other bootstrap sets[1].

### Training Independent Models

A learning model (in this case, a decision tree) is trained independently on each of these bootstrap training sets.

### Aggregating Predictions

The predictions from all the individually trained models are then combined to form the final ensemble prediction.

The main benefit of **Bagging** is the reduction of variance. By training multiple models on slightly different samples of the data, prediction errors that occur due to the model's sensitivity to small fluctuations in the training data are smoothed out when the predictions are aggregated. Models with high variance tend to overfit the training data, and **Bagging** helps combat this[1].

## 4. Voting for Classification / Averaging for Regression

Once all the trees in the Random Forest have been built, they are ready to make predictions on new data. How these predictions are aggregated depends on the type of problem:

### For Classification Problems (Majority Voting)

When the Random Forest is used for a classification problem (where the goal is to predict a discrete category), each individual decision tree in the forest makes its own class prediction for a new instance. After all the trees have made their predictions, the algorithm applies a majority voting method. The class that receives the most "votes" (predictions) from the individual trees is declared as the final Random Forest prediction.

>**Example**: If you have a Random Forest with 100 trees and 65 of them predict class "A", and 35 predict class "B" for a new instance, the Random Forest will classify the instance as "A"[1].

### For Regression Problems (Averaging Predictions)

For regression problems (where the goal is to predict a continuous numeric value), each decision tree in the Random Forest makes its own prediction of the value for the new instance. The final Random Forest prediction is then calculated as the arithmetic mean (or, in some implementations, the median) of the values ‚Äã‚Äãpredicted by all the trees in the forest.

>**Example**: If you have a Random Forest with 100 trees predicting the price of a house, and the trees individually predict values ‚Äã‚Äãsuch as $500,000, $510,000, $495,000, etc., the final Random Forest prediction will be the average of these values.
>Note
>This aggregation by voting or averaging is what allows the Random Forest to achieve high accuracy and robustness, outperforming a single decision tree and being less susceptible to overfitting [2].

## Main Elements

Random Forest is a machine learning method proposed by Breiman (2001), being an extension of the Bagging method with an introduction of randomness in the choice of attributes, which reduces the correlation between the trees in the ensemble [1].

## 1. Bootstrap (Bagging): Sampling with Replacement

The first step in building a Random Forest consists of applying the Bagging (Bootstrap Aggregating) method:

- **Sampling with replacement**: several random samples are created from the original training set, with replacement. Each sample (called bootstrap) is used to train a different tree.

- For **regression**, the final prediction is the average of the tree outputs.

- For **classification**, the final prediction is based on majority voting among the trees.

- Bagging reduces the variance of high-variance, low-bias models, such as deep decision trees [2].

## 2. Randomness in the Attributes

Unlike pure Bagging, Random Forest adds a second level of randomnes[1]:

- At each node of the tree, a subset of variables is randomly selected (instead of considering all variables).

- The best split is chosen only within this subset.

- This reduces the correlation between the trees, promoting diversity and improving the variance reduction of the final model.

- Typically, you choose \( $$m = \sqrt{p} $$\) for **classification** or \( $$m = \frac {p}{3} $$\) for **regression**, where **p** is the total number of variables. Where ‚Äúp‚Äù represents the total number of attributes available in your training dataset, and ‚Äúm‚Äù represents the number of attributes randomly selected at each node of the tree [1].

## 3. Independent Trees

Each tree in the forest is built independently[1]:

- A tree is grown for each bootstrap sample, with splits based on random subsets of attributes.

- Since the data and attributes are randomly selected, each tree captures different patterns in the data.

- Combining these independent trees reduces the risk of overfitting.

- Averaging **B** independent trees reduces the overall variance without increasing bias. Where **B** is the total number of decision trees [1].

## References

[1] HASTIE, T.; TIBSHIRANI, R.; FRIEDMAN, J. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, 2¬™ ed., 2009.

[2] G√âRON, A. M√£os √† Obra: Aprendizado de M√°quina com Scikit-Learn & TensorFlow. Alta Books, 2019.

### üëæ Contributor

| [![Seidi Ducher](https://avatars.githubusercontent.com/u/153019298?v=4)](https://github.com/seidiDucher)
| :---:

---

## Portuguese version

## Funcionamento do Random Forest

## 1. Fontes de Aleatoriedade

O funcionamento do Random Forest baseia-se em duas principais fontes de aleatoriedade, que s√£o cruciais para a diversidade e efic√°cia do conjunto de √°rvores:

### Amostragem de Dados (Bagging - Bootstrap Aggregation)

O algoritmo utiliza uma t√©cnica chamada bootstrap **aggregating ou bagging**, onde cada √°rvore na floresta √© treinada em uma amostra de dados diferente, que √© criada atrav√©s do m√©todo **bootstrap**. Isso significa que, para cada √°rvore, um subconjunto de dados de treinamento √© selecionado aleatoriamente com substitui√ß√£o do conjunto de dados original[1]. Essa amostragem com substitui√ß√£o garante que algumas inst√¢ncias possam aparecer m√∫ltiplas vezes em uma amostra, enquanto outras podem n√£o aparecer em nenhuma amostra, garantindo a variabilidade entre os conjuntos de treinamento das √°rvores[1].

### Sele√ß√£o Aleat√≥ria de Atributos

Durante o processo de constru√ß√£o de cada √°rvore, em cada n√≥ onde uma decis√£o de divis√£o precisa ser feita, o algoritmo n√£o considera todos os atributos dispon√≠veis. Em vez disso, ele seleciona aleatoriamente um subconjunto de atributos (tipicamente a raiz quadrada do n√∫mero total de atributos para classifica√ß√£o, ou um ter√ßo para regress√£o) e escolhe o melhor atributo e o melhor ponto de divis√£o somente dentro desse subconjunto limitado[1]. Essa aleatoriedade nos atributos for√ßados a serem considerados em cada divis√£o ajuda a desacoplar as √°rvores, tornando-as menos correlacionadas.[1]

A combina√ß√£o desses dois fatores aleat√≥rios garante que cada √°rvore na floresta seja √∫nica e explore diferentes aspectos dos dados, mitigando o risco de overfitting e aumentando a capacidade de generaliza√ß√£o do modelo[1].

## 2. Processo de Cria√ß√£o das √Årvores

A constru√ß√£o de cada √°rvore no Random Forest segue o processo geral de constru√ß√£o de √°rvores de decis√£o, mas com as modifica√ß√µes mencionadas:

### Amostragem Bootstrap

Para a **k-√©sima** √°rvore da floresta, uma nova amostra de treinamento √© gerada a partir do conjunto de dados original por meio de amostragem **bootstrap**. Essa amostra ter√° o mesmo tamanho do conjunto original, mas com algumas inst√¢ncias repetidas e outras omitidas[2].

### Crescimento da √Årvore com Sele√ß√£o Aleat√≥ria de Atributos

Cada √°rvore √© constru√≠da de forma recursiva:

- Em cada n√≥ da √°rvore, em vez de considerar todos os **M** atributos dispon√≠veis, um n√∫mero **m** de atributos √© selecionado aleatoriamente (onde **m < M**).
- A melhor divis√£o para aquele n√≥ √© ent√£o determinada usando apenas os **m** atributos selecionados aleatoriamente, com base em um crit√©rio de impureza.
- Este processo continua at√© que a √°rvore atinja uma profundidade m√°xima predefinida, ou at√© que um n√∫mero m√≠nimo de amostras por n√≥ folha seja alcan√ßado. Ao contr√°rio de √°rvores de decis√£o individuais, as √°rvores no **Random Forest** s√£o frequentemente cultivadas at√© sua profundidade m√°xima sem poda, pois a combina√ß√£o de muitas √°rvores diversificadas compensa o potencial de overfitting de √°rvores individuais.

A diversidade resultante dessas √°rvores, treinadas em subconjuntos de dados e utilizando subconjuntos de atributos, √© fundamental para o desempenho do Random Forest[1].

## 3. T√©cnica de Bagging (Bootstrap Aggregation)

A t√©cnica de **Bagging (Bootstrap Aggregation)** √© o alicerce do Random Forest. O **Bagging** √© um m√©todo de ensemble que visa melhorar a estabilidade e a precis√£o de algoritmos de aprendizado de m√°quina, reduzindo a vari√¢ncia. Ele opera da seguinte forma:

### Gera√ß√£o de M√∫ltiplos Conjuntos de Treinamento

A partir do conjunto de dados de treinamento original, s√£o gerados m√∫ltiplos (geralmente centenas ou milhares) novos conjuntos de treinamento. Cada um desses conjuntos √© criado por meio de **amostragem bootstrap**, ou seja, selecionando aleatoriamente inst√¢ncias do conjunto original com substitui√ß√£o. Como resultado, cada conjunto de **bootstrap** √© ligeiramente diferente do original e dos outros conjuntos de bootstrap[1].

### Treinamento de Modelos Independentes

Um modelo de aprendizado (neste caso, uma √°rvore de decis√£o) √© treinado de forma independente em cada um desses conjuntos de treinamento bootstrap.

### Agrega√ß√£o das Previs√µes

As previs√µes de todos os modelos treinados individualmente s√£o ent√£o combinadas para formar a previs√£o final do ensemble.

O principal benef√≠cio do **Bagging** √© a redu√ß√£o da vari√¢ncia. Ao treinar diversos modelos em amostras ligeiramente diferentes dos dados, os erros de previs√£o que ocorrem devido √† sensibilidade do modelo a pequenas flutua√ß√µes nos dados de treinamento s√£o suavizados quando as previs√µes s√£o agregadas. Modelos com alta vari√¢ncia tendem a superajustar os dados de treinamento, e o **Bagging** ajuda a combater isso[1].

## 4. Vota√ß√£o para Classifica√ß√£o / M√©dia para Regress√£o

Uma vez que todas as √°rvores no Random Forest foram constru√≠das, elas est√£o prontas para fazer previs√µes em novos dados. A forma como essas previs√µes s√£o agregadas depende do tipo de problema:

### Para Problemas de Classifica√ß√£o (Vota√ß√£o Majorit√°ria)

Quando o Random Forest √© usado para um problema de classifica√ß√£o (onde o objetivo √© prever uma categoria discreta), cada √°rvore de decis√£o individual no bosque faz sua pr√≥pria previs√£o de classe para uma nova inst√¢ncia. Ap√≥s todas as √°rvores terem feito suas previs√µes, o algoritmo aplica um m√©todo de vota√ß√£o majorit√°ria. A classe que recebe o maior n√∫mero de "votos" (previs√µes) das √°rvores individuais √© declarada como a previs√£o final do Random Forest.

>**Exemplo**: Se voc√™ tem um Random Forest com 100 √°rvores e 65 delas preveem a classe "A", e 35 preveem a classe "B" para uma nova inst√¢ncia, o Random Forest classificar√° a inst√¢ncia como "A"[1].

### Para Problemas de Regress√£o (M√©dia das Previs√µes)

Para problemas de regress√£o (onde o objetivo √© prever um valor num√©rico cont√≠nuo), cada √°rvore de decis√£o no Random Forest faz sua pr√≥pria previs√£o de valor para a nova inst√¢ncia. A previs√£o final do Random Forest √© ent√£o calculada como a m√©dia aritm√©tica (ou, em algumas implementa√ß√µes, a mediana) dos valores previstos por todas as √°rvores no bosque.

>**Exemplo**: Se voc√™ tem um Random Forest com 100 √°rvores prevendo o pre√ßo de uma casa, e as √°rvores individualmente preveem valores como R$ 500.000, R$ 510.000, R$ 495.000, etc., a previs√£o final do Random Forest ser√° a m√©dia desses valores.
>Observa√ß√£o
>Essa agrega√ß√£o por vota√ß√£o ou m√©dia √© o que permite ao Random Forest atingir alta precis√£o e robustez, superando o desempenho de uma √∫nica √°rvore de decis√£o e sendo menos suscet√≠vel ao overfitting[2].

## Elementos Principais

Random Forest √© um m√©todo de aprendizado de m√°quina proposto por Breiman (2001), sendo uma extens√£o do m√©todo de Bagging com uma introdu√ß√£o de aleatoriedade na escolha de atributos, o que reduz a correla√ß√£o entre as √°rvores do ensemble[1].

## 1. Bootstrap (Bagging): Amostragem com Reposi√ß√£o

O primeiro passo na constru√ß√£o de uma Random Forest consiste em aplicar o m√©todo de Bagging (Bootstrap Aggregating):

- **Amostragem com reposi√ß√£o**: s√£o criadas diversas amostras aleat√≥rias do conjunto de treino original, com reposi√ß√£o. Cada amostra (chamada de bootstrap) √© usada para treinar uma √°rvore diferente[2].
  
- Para **regress√£o**, a predi√ß√£o final √© a m√©dia das sa√≠das das √°rvores[2].

- Para **classifica√ß√£o**, a predi√ß√£o final √© baseada em vota√ß√£o majorit√°ria entre as √°rvores[2].

- O Bagging reduz a vari√¢ncia de modelos de alta vari√¢ncia e baixo vi√©s, como √°rvores de decis√£o profundas[2].

## 2. Aleatoriedade nos Atributos

Ao contr√°rio do Bagging puro, o Random Forest adiciona um segundo n√≠vel de aleatoriedade:

- Em cada n√≥ da √°rvore, √© selecionado aleatoriamente um subconjunto de vari√°veis (em vez de considerar todas as vari√°veis) [1].
  
- A melhor divis√£o √© escolhida apenas dentro desse subconjunto[1].
  
- Isso reduz a correla√ß√£o entre as √°rvores, promovendo diversidade e melhorando a redu√ß√£o da vari√¢ncia do modelo final[1].

- Tipicamente, escolhe-se \( $$m = \sqrt{p} $$\) para **classifica√ß√£o** ou \( $$m = \frac {p}{3} $$\) para **regress√£o**, sendo  **p** o n√∫mero total de vari√°veis. Onde ‚Äúp‚Äù representa o n√∫mero total de atributos dispon√≠veis no seu conjunto de dados de treinamento, e ‚Äúm‚Äù representa o n√∫mero de atributos selecionados aleatoriamente em cada n√≥ da √°rvore[1].

## 3. √Årvores Independentes

Cada √°rvore da floresta √© constru√≠da de forma independente:

- Cresce-se uma √°rvore para cada bootstrap sample, com divis√µes baseadas em subconjuntos aleat√≥rios de atributos[1].
  
- Como os dados e atributos s√£o selecionados aleatoriamente, cada √°rvore captura padr√µes diferentes dos dados[1].
  
- A combina√ß√£o dessas √°rvores independentes reduz o risco de overfitting[1].
  
- A m√©dia de **B** √°rvores independentes reduz a vari√¢ncia global sem aumentar o vi√©s. Onde **B** √© o n√∫mero total de √°rvores de decis√£o[1].

## Refer√™ncias

[1] HASTIE, T.; TIBSHIRANI, R.; FRIEDMAN, J. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, 2¬™ ed., 2009.

[2] G√âRON, A. M√£os √† Obra: Aprendizado de M√°quina com Scikit-Learn & TensorFlow. Alta Books, 2019.

## üëæ Colaborador

| [![Seidi Ducher](https://avatars.githubusercontent.com/u/153019298?v=4)](https://github.com/seidiDucher)
| :---:
