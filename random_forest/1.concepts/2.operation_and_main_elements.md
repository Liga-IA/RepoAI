# Portuguese version

# Funcionamento do Random Forest

## 1. Fontes de Aleatoriedade

O funcionamento do Random Forest baseia-se em duas principais fontes de aleatoriedade, que s√£o cruciais para a diversidade e efic√°cia do conjunto de √°rvores:

### Amostragem de Dados (Bagging - Bootstrap Aggregation)

O algoritmo utiliza uma t√©cnica chamada bootstrap **aggregating ou bagging**, onde cada √°rvore na floresta √© treinada em uma amostra de dados diferente, que √© criada atrav√©s do m√©todo **bootstrap**. Isso significa que, para cada √°rvore, um subconjunto de dados de treinamento √© selecionado aleatoriamente com substitui√ß√£o do conjunto de dados original. Essa amostragem com substitui√ß√£o garante que algumas inst√¢ncias possam aparecer m√∫ltiplas vezes em uma amostra, enquanto outras podem n√£o aparecer em nenhuma amostra, garantindo a variabilidade entre os conjuntos de treinamento das √°rvores.

### Sele√ß√£o Aleat√≥ria de Atributos

Durante o processo de constru√ß√£o de cada √°rvore, em cada n√≥ onde uma decis√£o de divis√£o precisa ser feita, o algoritmo n√£o considera todos os atributos dispon√≠veis. Em vez disso, ele seleciona aleatoriamente um subconjunto de atributos (tipicamente a raiz quadrada do n√∫mero total de atributos para classifica√ß√£o, ou um ter√ßo para regress√£o) e escolhe o melhor atributo e o melhor ponto de divis√£o somente dentro desse subconjunto limitado. Essa aleatoriedade nos atributos for√ßados a serem considerados em cada divis√£o ajuda a desacoplar as √°rvores, tornando-as menos correlacionadas.

A combina√ß√£o desses dois fatores aleat√≥rios garante que cada √°rvore na floresta seja √∫nica e explore diferentes aspectos dos dados, mitigando o risco de overfitting e aumentando a capacidade de generaliza√ß√£o do modelo.

## 2. Processo de Cria√ß√£o das √Årvores

A constru√ß√£o de cada √°rvore no Random Forest segue o processo geral de constru√ß√£o de √°rvores de decis√£o, mas com as modifica√ß√µes mencionadas:

### Amostragem Bootstrap

Para a **k-√©sima** √°rvore da floresta, uma nova amostra de treinamento √© gerada a partir do conjunto de dados original por meio de amostragem **bootstrap**. Essa amostra ter√° o mesmo tamanho do conjunto original, mas com algumas inst√¢ncias repetidas e outras omitidas.

### Crescimento da √Årvore com Sele√ß√£o Aleat√≥ria de Atributos

Cada √°rvore √© constru√≠da de forma recursiva:

- Em cada n√≥ da √°rvore, em vez de considerar todos os **M** atributos dispon√≠veis, um n√∫mero **m** de atributos √© selecionado aleatoriamente (onde **m < M**).
- A melhor divis√£o para aquele n√≥ √© ent√£o determinada usando apenas os **m** atributos selecionados aleatoriamente, com base em um crit√©rio de impureza.
- Este processo continua at√© que a √°rvore atinja uma profundidade m√°xima predefinida, ou at√© que um n√∫mero m√≠nimo de amostras por n√≥ folha seja alcan√ßado. Ao contr√°rio de √°rvores de decis√£o individuais, as √°rvores no **Random Forest** s√£o frequentemente cultivadas at√© sua profundidade m√°xima sem poda, pois a combina√ß√£o de muitas √°rvores diversificadas compensa o potencial de overfitting de √°rvores individuais.

A diversidade resultante dessas √°rvores, treinadas em subconjuntos de dados e utilizando subconjuntos de atributos, √© fundamental para o desempenho do Random Forest.

## 3. T√©cnica de Bagging (Bootstrap Aggregation)

A t√©cnica de **Bagging (Bootstrap Aggregation)** √© o alicerce do Random Forest. O **Bagging** √© um m√©todo de ensemble que visa melhorar a estabilidade e a precis√£o de algoritmos de aprendizado de m√°quina, reduzindo a vari√¢ncia. Ele opera da seguinte forma:

### Gera√ß√£o de M√∫ltiplos Conjuntos de Treinamento

A partir do conjunto de dados de treinamento original, s√£o gerados m√∫ltiplos (geralmente centenas ou milhares) novos conjuntos de treinamento. Cada um desses conjuntos √© criado por meio de **amostragem bootstrap**, ou seja, selecionando aleatoriamente inst√¢ncias do conjunto original com substitui√ß√£o. Como resultado, cada conjunto de **bootstrap** √© ligeiramente diferente do original e dos outros conjuntos de bootstrap.

### Treinamento de Modelos Independentes

Um modelo de aprendizado (neste caso, uma √°rvore de decis√£o) √© treinado de forma independente em cada um desses conjuntos de treinamento bootstrap.

### Agrega√ß√£o das Previs√µes

As previs√µes de todos os modelos treinados individualmente s√£o ent√£o combinadas para formar a previs√£o final do ensemble.

O principal benef√≠cio do **Bagging** √© a redu√ß√£o da vari√¢ncia. Ao treinar diversos modelos em amostras ligeiramente diferentes dos dados, os erros de previs√£o que ocorrem devido √† sensibilidade do modelo a pequenas flutua√ß√µes nos dados de treinamento s√£o suavizados quando as previs√µes s√£o agregadas. Modelos com alta vari√¢ncia tendem a superajustar os dados de treinamento, e o **Bagging** ajuda a combater isso.

## 4. Vota√ß√£o para Classifica√ß√£o / M√©dia para Regress√£o

Uma vez que todas as √°rvores no Random Forest foram constru√≠das, elas est√£o prontas para fazer previs√µes em novos dados. A forma como essas previs√µes s√£o agregadas depende do tipo de problema:

### Para Problemas de Classifica√ß√£o (Vota√ß√£o Majorit√°ria)

Quando o Random Forest √© usado para um problema de classifica√ß√£o (onde o objetivo √© prever uma categoria discreta), cada √°rvore de decis√£o individual no bosque faz sua pr√≥pria previs√£o de classe para uma nova inst√¢ncia. Ap√≥s todas as √°rvores terem feito suas previs√µes, o algoritmo aplica um m√©todo de vota√ß√£o majorit√°ria. A classe que recebe o maior n√∫mero de "votos" (previs√µes) das √°rvores individuais √© declarada como a previs√£o final do Random Forest.

>**Exemplo**: Se voc√™ tem um Random Forest com 100 √°rvores e 65 delas preveem a classe "A", e 35 preveem a classe "B" para uma nova inst√¢ncia, o Random Forest classificar√° a inst√¢ncia como "A".

### Para Problemas de Regress√£o (M√©dia das Previs√µes)

Para problemas de regress√£o (onde o objetivo √© prever um valor num√©rico cont√≠nuo), cada √°rvore de decis√£o no Random Forest faz sua pr√≥pria previs√£o de valor para a nova inst√¢ncia. A previs√£o final do Random Forest √© ent√£o calculada como a m√©dia aritm√©tica (ou, em algumas implementa√ß√µes, a mediana) dos valores previstos por todas as √°rvores no bosque.

>**Exemplo**: Se voc√™ tem um Random Forest com 100 √°rvores prevendo o pre√ßo de uma casa, e as √°rvores individualmente preveem valores como R$ 500.000, R$ 510.000, R$ 495.000, etc., a previs√£o final do Random Forest ser√° a m√©dia desses valores.

>Note
>Essa agrega√ß√£o por vota√ß√£o ou m√©dia √© o que permite ao Random Forest atingir alta precis√£o e robustez, superando o desempenho de uma √∫nica √°rvore de decis√£o e sendo menos suscet√≠vel ao overfitting.

---

# Elementos Principais

Random Forest √© um m√©todo de aprendizado de m√°quina proposto por Breiman (2001), sendo uma extens√£o do m√©todo de Bagging com uma introdu√ß√£o de aleatoriedade na escolha de atributos, o que reduz a correla√ß√£o entre as √°rvores do ensemble.

## 1. Bootstrap (Bagging): Amostragem com Reposi√ß√£o

O primeiro passo na constru√ß√£o de uma Random Forest consiste em aplicar o m√©todo de Bagging (Bootstrap Aggregating):

- **Amostragem com reposi√ß√£o**: s√£o criadas diversas amostras aleat√≥rias do conjunto de treino original, com reposi√ß√£o. Cada amostra (chamada de bootstrap) √© usada para treinar uma √°rvore diferente.
  
- Para **regress√£o**, a predi√ß√£o final √© a m√©dia das sa√≠das das √°rvores.

- Para **classifica√ß√£o**, a predi√ß√£o final √© baseada em vota√ß√£o majorit√°ria entre as √°rvores.

- O Bagging reduz a vari√¢ncia de modelos de alta vari√¢ncia e baixo vi√©s, como √°rvores de decis√£o profundas.

## 2. Aleatoriedade nos Atributos

Ao contr√°rio do Bagging puro, o Random Forest adiciona um segundo n√≠vel de aleatoriedade:

- Em cada n√≥ da √°rvore, √© selecionado aleatoriamente um subconjunto de vari√°veis (em vez de considerar todas as vari√°veis).
  
- A melhor divis√£o √© escolhida apenas dentro desse subconjunto.
  
- Isso reduz a correla√ß√£o entre as √°rvores, promovendo diversidade e melhorando a redu√ß√£o da vari√¢ncia do modelo final.

- Tipicamente, escolhe-se \( $$m = \sqrt{p} $$\) para **classifica√ß√£o** ou \( $$m = \frac {p}{3} $$\) para **regress√£o**, sendo  **p** o n√∫mero total de vari√°veis. Onde ‚Äúp‚Äù representa o n√∫mero total de atributos dispon√≠veis no seu conjunto de dados de treinamento, e ‚Äúm‚Äù representa o n√∫mero de atributos selecionados aleatoriamente em cada n√≥ da √°rvore.

## 3. √Årvores Independentes

Cada √°rvore da floresta √© constru√≠da de forma independente:

- Cresce-se uma √°rvore para cada bootstrap sample, com divis√µes baseadas em subconjuntos aleat√≥rios de atributos.
  
- Como os dados e atributos s√£o selecionados aleatoriamente, cada √°rvore captura padr√µes diferentes dos dados.
  
- A combina√ß√£o dessas √°rvores independentes reduz o risco de overfitting.
  
- A m√©dia de **B** √°rvores independentes reduz a vari√¢ncia global sem aumentar o vi√©s. Onde **B** √© o n√∫mero total de √°rvores de decis√£o.

---



## üëæ Colaboradores
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher)  
| :---: |