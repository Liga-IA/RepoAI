# English version


# Differences between Random Forest and Simple Decision Trees

## Robustness, Accuracy and Generalization

Decision trees are highly sensitive to small variations in the training data. Small changes can lead to completely different tree structures, making them unstable models with a high risk of overfitting.

Random Forest, by combining several independent trees built on different subsets of the data and attributes, reduces this instability. This approach improves generalization and robustness by smoothing out individual errors of the trees and increasing the average accuracy of the final model.

## 2. Interpretability and Performance

Decision trees are considered white-box models: their rules are easy to understand and visualize. This makes it easier to explain the model to non-experts and make decisions based on transparent logic.

Random Forest, on the other hand, is considered a black-box model, as it aggregates predictions from hundreds (or thousands) of trees, which makes direct interpretation difficult. However, it offers superior performance in most practical scenarios, especially in tasks with a lot of noise or high dimensionality.

## 3. Variation between models

A single Decision Tree can have very irregular decision boundaries, which can lead to overfitting. Even small rotations in the data can significantly impact its performance.

Random Forest creates multiple trees with randomness in both the data and the variables chosen at each split. This allows for greater diversity among the trees, and when combined by majority voting, produces smoother and more reliable results.


## üëæ Contributors
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher)  
| :---: |
---

# Portuguese version


# Diferen√ßas entre Random Forest e √Årvores de Decis√£o Simples

## Robustez, Precis√£o e Generaliza√ß√£o

√Årvores de Decis√£o s√£o altamente sens√≠veis a pequenas varia√ß√µes nos dados de treinamento. Pequenas mudan√ßas podem levar a estruturas totalmente diferentes da √°rvore, tornando-as modelos inst√°veis com alto risco de sobreajuste (overfitting).

Random Forest, ao combinar v√°rias √°rvores independentes constru√≠das sobre subconjuntos diferentes dos dados e atributos, reduz essa instabilidade. Essa abordagem melhora a generaliza√ß√£o e robustez, ao suavizar erros individuais das √°rvores e aumentar a precis√£o m√©dia do modelo final.

## 2. Interpretabilidade e Desempenho

√Årvores de Decis√£o s√£o consideradas modelos de caixa branca: suas regras s√£o f√°ceis de entender e visualizar. Isso facilita a explica√ß√£o do modelo para n√£o especialistas e a tomada de decis√µes com base em l√≥gica transparente.

Random Forest, por outro lado, √© considerada um modelo de caixa preta, pois agrega previs√µes de centenas (ou milhares) de √°rvores, o que dificulta a interpreta√ß√£o direta. Contudo, oferece desempenho superior na maioria dos cen√°rios pr√°ticos, especialmente em tarefas com muitos ru√≠dos ou alta dimensionalidade.

## 3. Varia√ß√£o entre modelos

√Årvore de Decis√£o √∫nica pode ter fronteiras de decis√£o muito irregulares, o que pode levar a overfitting. Mesmo pequenas rota√ß√µes nos dados podem impactar significativamente seu desempenho.

Random Forest cria m√∫ltiplas √°rvores com aleatoriedade tanto nos dados quanto nas vari√°veis escolhidas em cada divis√£o. Isso permite maior diversidade entre as √°rvores, e quando combinadas por vota√ß√£o majorit√°ria, produzem resultados mais suaves e confi√°veis.


## üëæ Colaboradores
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher)  
| :---: |