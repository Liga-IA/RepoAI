As previously explained in: [5.areas_of_application.md](../../en/1.concepts/5.areas_of_application.md) , the nearest neighbor method is commonly used to find media similar to the one being searched. For example, the recommendation systems of platforms such as Netflix and Spotify use this technique to suggest content similar to what the user has shown interest in.

However, in order for these recommendation systems to be fast and efficient, the approximate nearest neighbor method is employed. Below, we explore the difference between the exact nearest neighbor method and the approximate nearest neighbor method.

## Exact Nearest Neighbor:

This algorithm compares the input with all available data points and finds the one that is closestâ€”that is, the one with the smallest distance. In this case, the computational cost is proportional to the number of existing data points. This process is known as exhaustive search or brute-force search in the context of algorithms.

Therefore, it becomes evident that for very large datasetsâ€”such as those used by the platforms mentioned earlierâ€”this technique can be extremely time-consuming and require a high amount of computational resources.

As such, the approximate nearest neighbor algorithm becomes more efficient and practical, especially when dealing with large and/or high-dimensional datasets.

## Approximate Nearest Neighbor:

This algorithm seeks a data point that is sufficiently close to the query point, though not necessarily the closest of all. Thus, this approach proves to be quite effective in the previously mentioned scenario, since recommendations that are merely similar to the userâ€™s interests and searches are usually enough to spark interest in new media.

The implementation of this algorithm involves several stages:

- Dimensionality Reduction: Reducing of the dimensionality of the data is essential for making the model more efficient, as it simplifies and speeds up the data analysis process.

- Vector Encoding: After reducing the dimensionality and storing the dataset as vectors, those vectors can be encodedâ€”i.e., transformed into more compact structuresâ€”using different data structures such as Trees, LSH, or Quantization, to improve search efficiency.

- Search: Finally, the search method is implemented, and it may vary depending on the data structure used.

## References
Compreendendo o algoritmo de vizinho mais prÃ³ximo aproximado (ANN). (2024, April 17). Elastic Blog. https://www.elastic.co/pt/blog/understanding-ann

(N.d.). Towardsdatascience.com. Retrieved May 4, 2025, from https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6/?source=post_page-----7e2c6f0778bc---------------------------------------

## ðŸ‘¾ **Contributors**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/197432407?v=4" width=115><br><sub>Beatriz Schuelter Tartare</sub>](https://github.com/beastartare) |
| :---: |