Although k-NN is a relatively simple algorithm, it presents several common challenges that become more evident as the number of samples increases. The main issues include:

## 1. High Computational Cost
As the volume of data in a k-NN algorithm grows, so do its complexity and computational cost. This is because the algorithm needs to calculate the distance between a new data point and all existing points, which requires more memory to store these distances and to perform comparisons with the k nearest neighbors. As a result, both computational cost and processing time increase, especially for larger test datasets.

### 1.1 How to mitigate the problem?
To reduce this issue, the ideal approach is to limit the amount of data processed by the algorithm. One effective strategy is data condensation, which involves selecting relevant subsets for analysis and removing redundant samples or even outliers. This helps the algorithm maintain both accuracy and efficiency.

<details>
    <summary><strong>How are these subsets selected?</strong></summary> 
    Various strategies can be used to select data subsets. One common approach is the selection of relevant features using techniques such as Principal Component Analysis (PCA), correlation analysis, or supervised learning algorithms that identify the most influential features for classification. Another strategy involves using methods like Locality-Sensitive Hashing (LSH) or structures like KD-Tree, which approximate the nearest neighbors without comparing the new point to the entire dataset, thereby significantly reducing both time and computational cost.
</details>

## 2. Curse of Dimensionality
Another major challenge of the k-NN algorithm is the curse of dimensionality. The k-NN method performs best in low-dimensional spaces, but as the number of dimensions increases, distance metrics become less effective. In high-dimensional datasets, the data becomes sparse, and the differences in distances between samples become smaller and less meaningful. Consequently, the algorithm may lose accuracy and fail to identify relevant patterns in the data.

> An intuitive way to understand this problem is to imagine that you want to find out which fruit is most similar to an orange, using color and weight as criteria. With just these two metrics, the comparison is simple and can even be done visually. However, as more features are added, such as density and water content, the analysis becomes more complex. For example, one fruit might have a very different color and weight compared to an orange but share similar density and water content, while another fruit might show the opposite. This illustrates how classification becomes challenging as the number of dimensions increases, because which fruit is truly more similar to an orange?

### 2.2 How to mitigate the problem?
Regarding the curse of dimensionality, one way to address the problem is to develop specific distance metrics tailored to the data. However, this approach can be technically complex and time-consuming to implement effectively.

> [!NOTE]
> An interesting alternative is the use of the concept of hubness. This phenomenon occurs when certain samples, called hubs, have a high likelihood of being chosen as the nearest neighbors in various contexts. As the number of dimensions increases, the tendency for hubs to appear grows. Therefore, utilizing hubs can increase the accuracy of the k-NN algorithm for datasets with a large number of dimensions, as the hubs become reference points within the set, facilitating the identification of patterns.

## 3. Overfitting and Underfitting
As presented in the previous section, overfitting and underfitting can occur due to the chosen value of k. Therefore, selecting an appropriate value for k is essential to avoid these problems and ensure good algorithm performance.

> [!IMPORTANT]
> For more information about overfitting, underfitting, and how to determine the value of k, visit: [How to determine the value of K](https://github.com/mevianna/ISA/blob/main/KNN/en/1.concepts/3.how_to_determine_the_value_of_K.md)

## References
**IBM.** _K-nearest neighbors (KNN)_. Dispon√≠vel em: https://www.ibm.com/think/topics/KNN. Acesso em: 21 de abril de 2025.
**MARIZ, Filipe Mendes.** _Avalia√ß√£o e compara√ß√£o de vers√µes modificadas do algoritmo KNN_. 2017. [s.l.], Universidade Federal de Pernambuco, Centro de Inform√°tica, 2017. Dispon√≠vel em: https://www.cin.ufpe.br/~tg/2017-2/fmm4-tg.pdf. Acesso em: 23 de abril de 2025.

## üëæ **Contributors**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/178849007?v=4" width=115><br><sub>Rafaela Savaris</sub>](https://github.com/rafasavaris) |
| :---: |