## "k" in Regression and Classification

As explained in [How to determine the value of k](../../en/1.concepts/3.how_to_determine_the_value_of_k.md), we have the following characteristics of k:

-	k, when small, tends to cause overfitting
-	k, when large, tends to cause underfitting
-	k, when even, can lead to ties in classification problems and, consequently, inaccuracies.

When choosing the best value, we can use the square root of the number of available data points as a starting point. However, a more accurate way to make this decision is to use cross-validation techniques!

But now, let’s better understand the math behind these characteristics!

To understand whether k is large or small, we need to talk about the concepts of **error**, **bias**, and **variance**.

### Error
We can calculate the error produced by a model in more than one way. However, here we’ve chosen one for regression, and one for classification.

#### Error in regression
For regression, the total error can be calculated using the "Mean Squared Error", whose formula is:

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

That is:
-	$\text{MSE}$ stands for Mean Squared Error
-	$n$ is the total number of data points
-	$y_i$ is the actual value of the $i$-th data point
-	$\hat{y}_i$ is the predicted value for the $i$-th data point
-	$(y_i - \hat{y}_i)^2$ is the squared difference between the actual and predicted values, giving more weight to larger errors

#### Error in classification

For classification, we can calculate the error rate as follows:

$$
\text{Error Rate} = \frac{\text{Number of incorrect predictions}}{\text{Total number of samples}}
$$

> [!NOTE]
> In all models, no matter how good they are, there will always be an error associated with their predictions. This component of the error, which cannot be eliminated, is called the irreducible error. The previous formulas compute the total error, which includes—among other factors—the irreducible error, as we’ll see later.

### Bias

Bias occurs when a model is too simple to capture the true behavior of the data.
-	In linear regression, for example, it’s like trying to fit a straight line to data that follows a curved pattern.
-	In k NN, it means over smoothing the model so it misses local patterns. This happens when we pick very large values of kkk, because the prediction is based on too many neighbors.
  
**large k** → **high bias** → **underfitting**

> [!NOTE]
> Therefore, we can understand bias via the same example used in “Underfitting and high k values” in the conceptual section: [How to determine the value of k](../../en/1.concepts/3.how_to_determine_the_value_of_k.md).

### Variance

Variance refers to how much the model’s predictions would change if it were trained on a different dataset.

-	In other words, with high variance, adding or removing a single point can drastically change the prediction.
-	This happens when we pick very small values of k, since the prediction relies on too few neighbors.
  
**small k** → **high variance** → **overfitting**
> [!NOTE]
> Therefore, we can understand variance via the same example used in “Overfitting and low k values” in the conceptual section: [How to determine the value of k](../../en/1.concepts/3.how_to_determine_the_value_of_k.md).

### Relationship between Error, Bias, and Variance

#### Regression
It is possible to state that the MSE - that is, the total error - can be represented as:

$$
\text{TotalError}=\text{Bias}^2+\text{Variance}+\text{IrreducibleError}
$$

#### Classification
The idea that there is a relationship between the total error and the irreducible error, bias, and variance remains. However, the decomposition of the total error in terms of these other concepts is not done in such a direct way.

> [!TIP] To find the best k, we must find the minimum value of the total error. Remembering that, when k increases:
> - Bias increases
> - Variance decreases
> - Irreducible error remains constant

## Cross-validation

For a more precise calculation of k, we use cross-validation.

Basically, when we evaluate a model, we calculate the training error (which is calculated based on the data used to train the model), and we also calculate the test error (which is calculated based on new data).

Therefore, we do cross-validation:
1.	We choose a value for k.
2.	We divide the data into subsets.
3.	For the chosen value of k, we train the data on the chosen subset and test on the ones that were not chosen.
4.	Calculate the error for the chosen k. Repeat the process until it has been done for several values of k.
5.	Choose the k based on the one that resulted in the lowest error.

## References
JAMES, Gareth et al. An introduction to statistical learning: with applications in Python. New York: Springer, 2023.

HASTIE, Trevor; TIBSHIRANI, Robert; FRIEDMAN, Jerome. The elements of statistical learning: data mining, inference, and prediction. 2. ed. New York: Springer, 2009.

## Colaborators
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/160762179?v=4" width=115><br><sub>Maria Eduarda Vianna</sub>](https://github.com/mevianna) | 
| :---: | 


