## Distâncias
Como explicado na [Seção de Ideias Gerais](../../pt/3.matematica/1.ideias_gerais.md), faremos previsões com base na proximidade dos dados no espaço de características. Então, precisamos de formas de calcular as distâncias.

A maneira mais comum de calcularmos a distância é utilizando a ideia de Distância Euclidiana.

### Distância Euclidiana
A Distância Euclidiana calcula uma linha reta entre dois pontos $P (p_1, p_2, \dots, p_n)$ e $Q (q_1, q_2, \dots, q_n)$. Ela pode ser representada da seguinte forma:

$$
d = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}
$$

No caso particular de duas dimensões, podemos representar da seguinte maneira:

$$
d = \sqrt{(q_1 - p_1)^2 + (q_2 - p_2)^2}
$$

#### Exemplo:

Para A = (1, 2) e B = (4, 6):  

$$
d_{\text{euclid}} = \sqrt{(4 - 1)^2 + (6 - 2)^2} \\
= \sqrt{3^2 + 4^2} \\
= \sqrt{9 + 16} \\
= \sqrt{25} \\
= 5
$$

Graficamente, estamos calculando a linha tracejada:

![euclideandistance](../../figures/euclideandistance.png)


### Distância de Manhattan
Ela considera a soma das diferenças absolutas entre as coordenadas dos pontos.

A Distância de Manhattan entre dois pontos $P (p_1, p_2, \dots, p_n)$ e $Q (q_1, q_2, \dots, q_n)$ é:

$$
d = \sum_{i=1}^{n} |p_i - q_i|
$$

#### Exemplo:

Para A = (1, 2) e B = (4, 6) (que são os mesmos valores do exemplo anterior): 

$$
d_{\text{manhattan}} = |4 - 1| + |6 - 2| \\
= 3 + 4 \\
= 7
$$

Graficamente, estamos calculando a linha tracejada:

![distanciamanhattan](../../figures/manhattandistance.png)


#### Quando utilizar Euclidiana ou Manhattan no contexto do KNN? 
A Distância de Manhattan atribui menos ênfase a diferenças grandes entre os pontos. Por isso, quando temos valores extremos ("outliers"), pode ser mais adequado utilizá-la. Enquanto isso, a Distância Euclidiana é mais adequada para dados com características que possuem relação mais direta e contínua.

### Distância de Minkowski
Generaliza as distâncias euclidiana e de Manhattan. Para euclidiana, utilizamos p = 2, e, para Manhattan, p = 1.

A distância de Minkowski entre dois pontos $M (m_1, m_2, \dots, m_n)$ e $Q (q_1, q_2, \dots, q_n)$ com parâmetro \(p\) é:

$$
d = \left( \sum_{i=1}^{n} |m_i - q_i|^p \right)^{1/p}
$$

### Similaridade de Cossenos
É muito utilizada em classificação de texto. Palavras são dimensões e os documentos são vetores de palavras.

$$
\cos(\theta) = \frac{\vec{x} \cdot \vec{y}}{ \|\vec{x}\| \cdot \|\vec{y}\| } 
$$

-  $\theta$ é o **ângulo** entre os vetores. Quanto menor o ângulo, maior a similaridade.
  
-  $\vec{x}$ e $\vec{y}$ são os **vetores** que queremos comparar (por exemplo, documentos representados por vetores de palavras).
  
-  $\vec{x}$ $\cdot$ $\vec{y}$ é o **produto escalar** entre os vetores.
  
- $\|\vec{x}\|$ e $\|\vec{y}\|$ são as **normas (módulos)** dos vetores, ou seja, seus comprimentos:
  
$$
\|\vec{x}\| = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
\quad\quad
\|\vec{y}\| = \sqrt{y_1^2 + y_2^2 + \cdots + y_n^2}
$$

#### Exemplo:

Utilizaremos os mesmos números dos exemplos anteriores, mas contextualizaremos para o caso de classificação de textos.

Digamos que temos dois textos com as palavras: UFSC (sigla da nossa universidade) e LIA (sigla da Liga de Inteligência Artificial, da qual fazemos parte).

- O primeiro texto contém a palavra UFSC uma vez, e a palavra LIA duas vezes. Representaremos ele com o vetor 

$$
\vec{A} = [1,\ 2] 
$$

- O segundo texto contém a palavra UFSC quatro vezes, e a palavra LIA seis vezes. Representaremos ele com o vetor [4, 6].

$$   
\vec{B} = [4,\ 6]
$$

> [!NOTE]
> Levaremos em consideração apenas duas palavras para fins didáticos. No entanto, ao analisarmos textos, podemos ter vetores com muitas dimensões!

Graficamente, temos:

![similaridadecosseno](../../figures/similaridadecosseno.png)

Calcularemos a similaridade de cossenos dos dois documentos: 

1. Produto escalar:

$$
\vec{A} \cdot \vec{B} = (1 \cdot 4) + (2 \cdot 6) = 4 + 12 = 16
$$

2. Norma dos vetores:

$$
\|\vec{A}\| = \sqrt{1^2 + 2^2} = \sqrt{1 + 4} = \sqrt{5}
$$

$$
\|\vec{B}\| = \sqrt{4^2 + 6^2} = \sqrt{16 + 36} = \sqrt{52}
$$

3. Similaridade de cosseno:

$$
\cos(\theta) = \frac{\vec{A} \cdot \vec{B}}{\|\vec{A}\| \cdot \|\vec{B}\|} = \frac{16}{\sqrt{5} \cdot \sqrt{52}} \approx \frac{16}{16.1245} \approx 0.9923
$$

Perceba que o fato de o cosseno se aproximar de 1, indica que o ângulo é próximo de zero. Isto é, temos alta similaridade entre os dois documentos!

> [!TIP]
> No contexto do KNN, utilizamos a similaridade do cosseno para que o modelo decida se um terceiro documento, por exemplo, está mais próximo do primeiro ou do segundo. Quanto menor o ângulo, maior a proximidade do vizinho!

## Referências

RASCHKA, S. Lecture notes for STAT 451: Introduction to machine learning (Lecture 2: K-Nearest Neighbors). 2020. PDF. Disponível em: https://sebastianraschka.com/pdf/lecture-notes/stat451fs20/02-KNN__notes.pdf. Acesso em: 25 abr. 2025.

MIT OPEN COURSEWARE. 6.034 Artificial Intelligence – Fall 2010: Tutorial 3: K-nearest neighbors, decision trees, neural nets. 2010. PDF. Disponível em: https://ocw.mit.edu/courses/6-034-artificial-intelligence-fall-2010/4efa5e563ccb9d54fdd72068a8dda879_MIT6_034F10_tutor03.pdf. Acesso em: 25 abr. 2025.

MIT OPEN COURSEWARE. 10. Introduction to Learning, Nearest Neighbors. Vídeo (YouTube), postado em 10 jan. 2014. Disponível em: https://www.youtube.com/watch?v=09mb78oiPkA. Acesso em: 25 abr. 2025.

MARIZ, Filipe Mendes. Avaliação e comparação de versões modificadas do algoritmo KNN. Recife: Universidade Federal de Pernambuco, 2017. Trabalho de Conclusão de Curso (Bacharelado em Ciência da Computação) — Centro de Informática, Universidade Federal de Pernambuco, 2017. Disponível em: https://www.cin.ufpe.br/~tg/2017-2/fmm4-tg.pdf. Acesso em: 25 abr. 2025.

WINSTON, P. H. Artificial intelligence. 3. ed. Addison‑Wesley, 1992.

## Colaboradores
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/160762179?v=4" width=115><br><sub>Maria Eduarda Vianna</sub>](https://github.com/mevianna) | 
| :---: | 
