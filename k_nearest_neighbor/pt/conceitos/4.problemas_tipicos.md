Embora o KNN seja um algoritmo relativamente simples, ele apresenta alguns problemas comuns que se tornam mais evidentes a medida que o n√∫mero de amostras aumenta. Os principais problemas s√£o:

## 1. Alto custo computacional
Conforme a quantidade de dados de um algoritmo KNN cresce, a complexidade do algoritmo e seu custo computacional tamb√©m crescem. Isso ocorre pois √© preciso calcular a dist√¢ncia entre o novo dado e todos os outros j√° existentes, o que exige mais mem√≥ria para armazenar essas dist√¢ncias e as compara√ß√µes feitas com os k vizinhos mais pr√≥ximos. Como resultado, al√©m do aumento do custo computacional, o tempo de processamento tamb√©m cresce para conjuntos de dados de teste maior.

### 1.1 Como amenizar o problema?
Para diminuir esse problema, o ideal √© reduzir a quantidade de dados processados pelo algoritmo. Uma das estrat√©gias para isso, √© o condensamento de informa√ß√µes, que consiste na sele√ß√£o de subconjuntos relevantes para an√°lise e na elimina√ß√£o de amostras redundantes ou at√© mesmo de outliers. Assim, o algoritmo n√£o perde sua precis√£o e mant√©m a efici√™ncia.

<details>
  <summary><strong>Como os subconjuntos s√£o escolhidos?</strong></summary>
  Para a escolha dos subconjuntos de dados, diversas estrat√©gias podem ser adotadas. Uma delas √© a sele√ß√£o de vari√°veis relevantes atrav√©s de t√©cnicas como an√°lise de componentes principais (PCA), an√°lise de correla√ß√£o entre atributos ou o uso de algoritmos de aprendizado supervisionado que identificam as caracter√≠sticas mais influentes na classifica√ß√£o. Outra estrat√©gia √© o uso de algoritmos como Locality-Sensitive Hashing (LSH) ou estruturas como KD-Tree, que aproximam os prov√°veis vizinhos pr√≥ximos, sem a necessidade de comparar o novo ponto com todos os dados do conjunto, reduzindo significativamente o tempo e o custo computacional.
</details>

## 2. Maldi√ß√£o da dimensionalidade
Outro desafio do algoritmo KNN √© a maldi√ß√£o da dimensionalidade. Como o algoritmo KNN √© mais propenso a funcionar em situa√ß√µes com poucos param√™tros (dimens√µes), √† medida que o n√∫mero de dimens√µes aumenta, menos as m√©tricas de dist√¢ncias s√£o eficazes. Isso acontece pois em conjuntos com muitas dimens√µes, os dados ficam dispersos, e as diferen√ßas de dist√¢ncias nas amostras se tornam pequenas e menos relevantes. Dessa maneira, o algoritmo perde sua precis√£o e pode n√£o identificar padr√µes relevantes nos dados.

> [!NOTE]
> Uma forma intuitiva de entender esse problema √© imaginar que voc√™ deseja descobrir qual fruta √© mais parecida com uma laranja, usando como crit√©rios a cor e o peso. Com apenas essas duas m√©tricas, a compara√ß√£o √© simples e pode at√© ser feita visualmente. No entanto, ao adicionar muitas outras caracter√≠sticas, como densidade e teor de √°gua, a an√°lise se torna mais confusa. Por exemplo, uma fruta pode ter cor e peso bem diferentes de uma laranja, mas apresentar densidade e teor de √°gua semelhantes; enquanto outra pode ter caracter√≠sticas opostas a essa. Isso mostra a dificuldade de classifica√ß√£o, quando o n√∫mero de dimens√µes √© grande, pois qual das frutas √© mais semelhante a uma laranja?

### 2.1 Como amenizar o problema?
Quanto a maldi√ß√£o da dimensionalidade, uma das formas de lidar com o problema √© desenvolver m√©tricas especificas de dist√¢ncias. No entanto, essa alternativa pode ser complexa e demorada.

> [!NOTE]
>  Uma alternativa interessante, √© o uso do conceito de Hubness. Esse fen√¥meno ocorre quando certas amostras, denominadas *hubs*, apresentam uma alta tend√™ncia a serem escolhidas como o vizinho mais pr√≥ximo de outras amostras. Assim, quanto maior o n√∫mero de dimens√µes, maior a tend√™ncia de aparecer hubs. Portanto, a utiliza√ß√£o de hubs pode aumentar a precis√£o do algoritmo KNN para algoritmos com um n√∫mero elevado de dimens√µes, pois os hubs se tornam refer√™ncias do conjunto, facilitando a identifica√ß√£o de padr√µes.

## 3. Overfitting e underfitting
Como apresentado na se√ß√£o anterior, o overfitting e o underfitting podem ocorrer devido ao valor escolhido de k. Portanto, a escolha de um valor adequado para k √© essencial para evitar esses problemas e garantir um bom desempenho do algoritmo.

> [!IMPORTANT]
> Para mais informa√ß√µes sobre overfitting, underfitting e o valor de k, acesse: [How to determine the value of K](https://github.com/mevianna/ISA/blob/main/KNN/1.concepts/3.how_to_determine_the_value_of_K.md)

### Refer√™ncias
**IBM.** _K-nearest neighbors (KNN)_. Dispon√≠vel em: https://www.ibm.com/think/topics/KNN. Acesso em: 21 de abril de 2025.
**MARIZ, Filipe Mendes.** _Avalia√ß√£o e compara√ß√£o de vers√µes modificadas do algoritmo KNN_. 2017. [s.l.], Universidade Federal de Pernambuco, Centro de Inform√°tica, 2017. Dispon√≠vel em: https://www.cin.ufpe.br/~tg/2017-2/fmm4-tg.pdf. Acesso em: 23 de abril de 2025.

## üëæ **Contribuidores**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/178849007?v=4" width=115><br><sub>Rafaela Savaris</sub>](https://github.com/rafasavaris) | 
| :---: |