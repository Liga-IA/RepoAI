#  English version

 # Computational Considerations of KNN: Challenges and Strategies

The K-Nearest Neighbors (KNN) algorithm is known for its conceptual simplicity and effectiveness in various classification and regression problems. However, its intrinsic characteristics impose significant challenges in terms of computational cost, especially in scenarios with large volumes of data and high dimensionality.

## 1. High Computational Load in the Inference Phase (Lazy Learning)

One of the main characteristics of KNN is its "lazy learner" approach. Unlike algorithms such as decision trees or linear regression, KNN does not go through an explicit training phase to build a model. Instead, it stores the entire training dataset and performs the necessary calculations only at the moment of prediction for a new instance.

> Imagine a training dataset with blue and red points in a two-dimensional space. When a new green point (to be classified) arrives, KNN needs to calculate the distance from this point to all the blue and red points in the training set to identify its k nearest neighbors.

<div align="center">
  <img src="./Figures/LazyLearning.jpeg" alt="initial-banner" width="400">
</div>

> This need to calculate distances for each training point during prediction is what makes the inference phase computationally expensive.

## 2. Time Complexity in Prediction

The time complexity for making a single prediction with KNN is approximately O(nÃ—d), where:

- n represents the number of instances (points) in the training set.
- d represents the dimensionality of the data (number of attributes or features).

This complexity arises because, for each new point to be classified, the algorithm needs to calculate the distance to all n training points and then sort these distances to find the k smallest.

> [!NOTE]
>- **Large Datasets**: In datasets with millions or billions of instances, the prediction time for each new point can become prohibitive for real-time applications.
>- **Real-Time Applications**: For systems that require quick responses (e.g., online recommendation systems, real-time fraud detection), the latency introduced by KNN can be unacceptable without optimizations.

## 3. Sensitivity to Dimensionality (The Curse of Dimensionality)

As the number of dimensions (d) increases, the performance and efficiency of KNN tend to degrade significantly. This phenomenon is known as the "curse of dimensionality."

### Reasons

- **Data Sparsity**: In high-dimensional spaces, data points tend to become sparser, and the distance between them becomes less discriminative.
- **Concentration of Distances**: Distances between points tend to concentrate within a narrow range, making it difficult to distinguish truly close neighbors from the more distant ones.
- **Relevance of Attributes**: In high-dimensional spaces, it is more likely that there are many irrelevant or redundant attributes that add noise to the distance calculation.

> Imagine two points in a one-dimensional space. The distance between them is clear. Now, add a second dimension. The Euclidean distance changes. If we add many irrelevant dimensions, the distance between these two points will be dominated by the noise from those dimensions, obscuring the real relationship between them in the relevant dimensions.

<div align="center">
  <img src="./Figures/PartDate.jpeg" alt="initial-banner" width="400">
</div>

## 4. Absence of Intrinsic Attribute Selection

The KNN algorithm treats all variables (attributes) as equally important when calculating the distance between points. If the dataset contains many irrelevant or redundant variables, they will contribute to the distance calculation in the same way as the relevant variables.

> Consider a customer classification problem based on their purchasing habits. If we include attributes like the number of letters in the customer's name (which is irrelevant to purchasing behavior), KNN will consider this characteristic when calculating similarity between customers, which may lead to "close" neighbors that are not truly similar in terms of purchasing behavior.

## Strategies to Mitigate Computational Challenges

Despite these challenges, there are several techniques and approaches that can be employed to improve the computational efficiency of KNN:

### Data Structures for Nearest Neighbor Search

- **KD-Tree and Ball-Tree**: These data structures partition the training space hierarchically, allowing for more efficient nearest neighbor searches than exhaustive search. The search complexity can be reduced to O(d log n) in many cases, although performance may degrade in very high dimensions.

<div align="center">
  <img src="./Figures/ReduDimen.jpeg" alt="initial-banner" width="400">
</div>

## **Where Am I?**
```text
RepoAI/
â””â”€â”€ KNN/
    â”œâ”€â”€ 1.Concepts/
    â”‚   â””â”€â”€ 1.History.md
    |   â””â”€â”€ 2.ClassifierVsRegression.md
    |   â””â”€â”€ 3.How_to_determine_k_value.md
    |   â””â”€â”€ 4.Typical_problems.md
    |   â””â”€â”€ 4.Fields_of_use.md
    â””â”€â”€ 2.Code/
    |   â””â”€â”€ Figures/
    |   â””â”€â”€ 
    â”œâ”€â”€ 3.Mathematics/
    |   â””â”€â”€ Figures/
    |   â””â”€â”€ 1.generalldeas.md 
    |   â””â”€â”€ 2.distances.md  
    |   â””â”€â”€ 3.regressionAndclassification.md   
    |   â””â”€â”€ 4.Regression_classification_KNN.md <---- You are here!! 
    |   â””â”€â”€ 5.Computational_Consideration_KNN.md
    |   â””â”€â”€ 6.Limits_KNN.md 
```

## ğŸ‘¾ **Contributors**
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher)
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/160762179?v=4" width=115><br><sub>Maria Eduarda Vianna</sub>](https://github.com/mevianna) | 
| :---: | 

# Portuguese version


# ConsideraÃ§Ãµes Computacionais do KNN: Desafios e EstratÃ©gias

O algoritmo K-Nearest Neighbors (KNN) Ã© conhecido por sua simplicidade conceitual e eficÃ¡cia em diversos problemas de classificaÃ§Ã£o e regressÃ£o. No entanto, suas caracterÃ­sticas intrÃ­nsecas impÃµem desafios significativos em termos de custo computacional, especialmente em cenÃ¡rios com grandes volumes de dados e alta dimensionalidade.

## 1. Alta Carga Computacional na Fase de InferÃªncia (Lazy Learning)

Uma das principais caracterÃ­sticas do KNN Ã© sua abordagem de "aprendizado preguiÃ§oso" (lazy learner). Diferentemente de algoritmos como Ã¡rvores de decisÃ£o ou regressÃ£o linear, o KNN nÃ£o passa por uma fase explÃ­cita de treinamento para construir um modelo. Em vez disso, ele armazena todo o conjunto de dados de treinamento e realiza os cÃ¡lculos necessÃ¡rios apenas no momento da prediÃ§Ã£o para uma nova instÃ¢ncia.


>Imagine um conjunto de dados de treinamento com pontos azuis e vermelhos em um espaÃ§o bidimensional. Quando um novo ponto verde (a ser classificado) chega, o KNN precisa calcular a distÃ¢ncia desse ponto para todos os pontos azuis e vermelhos no conjunto de treinamento para identificar seus k vizinhos mais prÃ³ximos.

<div align="center">
  <img src="./Figures/LazyLearning.jpeg" alt="initial-banner" width="400">
</div>

>Essa necessidade de calcular distÃ¢ncias para cada ponto de treinamento durante a prediÃ§Ã£o Ã© o que torna a fase de inferÃªncia computacionalmente custosa.

## 2. Complexidade de Tempo na PrediÃ§Ã£o

A complexidade de tempo para realizar uma Ãºnica prediÃ§Ã£o com o KNN Ã© de aproximadamente O(nÃ—d), onde:

- n representa o nÃºmero de instÃ¢ncias (pontos) no conjunto de treinamento.
- d representa a dimensionalidade dos dados (nÃºmero de atributos ou features).

Essa complexidade surge porque, para cada novo ponto a ser classificado, o algoritmo precisa calcular a distÃ¢ncia atÃ© todos os n pontos de treinamento e, em seguida, ordenar essas distÃ¢ncias para encontrar os k menores.

> [!NOTE]
>- **Grandes Bases de Dados**: Em conjuntos de dados com milhÃµes ou bilhÃµes de instÃ¢ncias, o tempo de prediÃ§Ã£o para cada novo ponto pode se tornar proibitivo para aplicaÃ§Ãµes em tempo real.
>- **AplicaÃ§Ãµes em Tempo Real**: Para sistemas que exigem respostas rÃ¡pidas (por exemplo, sistemas de recomendaÃ§Ã£o online, detecÃ§Ã£o de fraudes em tempo real), a latÃªncia introduzida pelo KNN pode ser inaceitÃ¡vel sem otimizaÃ§Ãµes.

## 3. Sensibilidade Ã  Dimensionalidade (A MaldiÃ§Ã£o da Dimensionalidade)

Ã€ medida que o nÃºmero de dimensÃµes (d) aumenta, o desempenho e a eficiÃªncia do KNN tendem a se degradar significativamente. Esse fenÃ´meno Ã© conhecido como a "maldiÃ§Ã£o da dimensionalidade".

### RazÃµes

- **EspaÃ§amento dos Dados**: Em espaÃ§os de alta dimensÃ£o, os pontos de dados tendem a se tornar mais esparsos e a distÃ¢ncia entre eles se torna menos discriminativa.
- **ConcentraÃ§Ã£o de DistÃ¢ncias**: As distÃ¢ncias entre os pontos tendem a se concentrar em uma faixa estreita, tornando difÃ­cil distinguir os vizinhos verdadeiramente prÃ³ximos dos mais distantes.
- **RelevÃ¢ncia dos Atributos**: Em espaÃ§os de alta dimensÃ£o, Ã© mais provÃ¡vel que existam muitos atributos irrelevantes ou redundantes que adicionam ruÃ­do ao cÃ¡lculo da distÃ¢ncia.

>Imagine dois pontos em um espaÃ§o unidimensional. A distÃ¢ncia entre eles Ã© clara. Agora, adicione uma segunda dimensÃ£o. A distÃ¢ncia euclidiana muda. Se adicionarmos muitas dimensÃµes irrelevantes, a distÃ¢ncia entre esses dois pontos serÃ¡ dominada pelo ruÃ­do dessas dimensÃµes, obscurecendo a relaÃ§Ã£o real entre eles nas dimensÃµes relevantes.

<div align="center">
  <img src="./Figures/PartDate.jpeg" alt="initial-banner" width="400">
</div>

## 4. AusÃªncia de SeleÃ§Ã£o de Atributos IntrÃ­nseca

O algoritmo KNN trata todas as variÃ¡veis (atributos) como igualmente importantes ao calcular a distÃ¢ncia entre os pontos. Se o conjunto de dados contiver muitas variÃ¡veis irrelevantes ou redundantes, elas contribuirÃ£o para o cÃ¡lculo da distÃ¢ncia da mesma forma que as variÃ¡veis relevantes.

>Considere um problema de classificaÃ§Ã£o de clientes com base em seus hÃ¡bitos de compra. Se incluirmos atributos como o nÃºmero de letras no nome do cliente (que Ã© irrelevante para o comportamento de compra), o KNN considerarÃ¡ essa caracterÃ­stica ao calcular a similaridade entre os clientes, o que pode levar a vizinhos "prÃ³ximos" que nÃ£o sÃ£o realmente semelhantes em termos de comportamento de compra.

## EstratÃ©gias para Mitigar os Desafios Computacionais

Apesar desses desafios, existem vÃ¡rias tÃ©cnicas e abordagens que podem ser empregadas para melhorar a eficiÃªncia computacional do KNN:

### Estruturas de Dados para Busca de Vizinhos PrÃ³ximos

- **Ãrvores KD-Tree e Ball-Tree**: Essas estruturas de dados particionam o espaÃ§o de treinamento de forma hierÃ¡rquica, permitindo encontrar os s vizinhos mais prÃ³ximos de um ponto de consulta de maneira mais eficiente do que uma busca exaustiva. A complexidade da busca pode ser reduzida para O(dlogn) em muitos casos, embora o desempenho possa degradar em dimensÃµes muito altas.

<div align="center">
  <img src="./Figures/ReduDimen.jpeg" alt="initial-banner" width="400">
</div>

## **Onde estou?**
```text
RepoAI/
â””â”€â”€ KNN/
    â”œâ”€â”€ 1.Conceitos/
    â”‚   â””â”€â”€ 1.HistÃ³ria.md
    |   â””â”€â”€ 2.ClassificadorVsRegressao.md
    |   â””â”€â”€ 3.Como_determinar_o_valor_de_k.md
    |   â””â”€â”€ 4.Problemas_tÃ­picos.md
    |   â””â”€â”€ 4.Ãreas_de_aplicaÃ§Ã£o.md
    â””â”€â”€ 2.CÃ³digo/
    |   â””â”€â”€ Figuras/
    â”œâ”€â”€ 3.MatemÃ¡tica/
    |   â””â”€â”€ Figuras/
    |   â””â”€â”€ 1.Ideias_gerais.md 
    |   â””â”€â”€ 2.DistÃ¢ncias.md 
    |   â””â”€â”€ 4.RegressaoEclassificacao.md 
    |   â””â”€â”€ 4.Regressao_classificacao.md 
    |   â””â”€â”€ 5.Computacao_ConsideraÃ§Ãµes_KNN.md <---- VocÃª estÃ¡ aqui!! 
    |   â””â”€â”€ 6.Limites_KNN.md
```
## ğŸ‘¾ Colaboradores
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher) 
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/160762179?v=4" width=115><br><sub>Maria Eduarda Vianna</sub>](https://github.com/mevianna) | 
| :---: |