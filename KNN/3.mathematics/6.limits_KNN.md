# English Version

# Mathematical Limitations of KNN

## Bias and Variance Dependent on k and Dimensionality

The expected error of the KNN estimator can be decomposed into bias and variance. The choice of k directly influences the balance between these two terms:

- **Low k** â†’ Low bias, high variance
- **High k** â†’ High bias, low variance

## The Mathematical Formula for Expected Error

The formula for expected error is given by:

$$
E\left[( r_k(X) - r(X) )^2 \right] = \frac{K k^2}{n^{2/d} + k^2}
$$

### Meaning of Each Term

- **$E\left[( r_k(X) - r(X) )^2 \right]$**: expected mean squared error.
- **k**: number of neighbors.
- **n**: sample size (number of training data).
- **d**: number of dimensions.
- **$sigma^2$**: variance of the errors (associated with noise).
- **K**: constant proportional to the smoothness of the true function **r(x)**.

## Example Calculation

Let's use the following values as an example:

- **k = 5**
- **n = 500**
- **d = 10**
- **$sigma^2 = 1$**
- **K = 1** (for simplicity)

Substituting into the formula:

$$
E\left[( r_k(X) - r(X) )^2 \right] = 1 \cdot \frac{5^2}{500^{2/10} + 5^2}
$$

Calculating:

$$
E\left[( r_k(X) - r(X) )^2 \right] = 0.589 
$$

This shows that as the dimensionality **d** increases, convergence becomes slower and the quality of the model deteriorates â€” the rate of convergence decreases exponentially with \( d \).

## Requires Smoothness of the Target Function

The theoretical analysis assumes that the regression function is L-Lipschitz, meaning it varies smoothly. Otherwise, KNN can be mathematically unstable for predictions in regions with highly nonlinear behavior.

## References
IZBICKI RAFAEL & MENDONÃ‡A D.S. TIAGO, SÃ£o Paulo, 2020. Aprendizado de MÃ¡quina: uma abordagem estatÃ­stica.

O'REILLY, Rio de Janeiro, 2019. MÃ£os Ã  Obras Aprendizado MÃ¡quina com Scikit-Learn e TensorFlow.

## **Where Am I?**
```text
RepoAI/
â””â”€â”€ KNN/
    â”œâ”€â”€ 1.Concepts/
    â”‚   â””â”€â”€ 1.History.md
    |   â””â”€â”€ 2.ClassifierVsRegression.md
    |   â””â”€â”€ 3.How_to_determine_k_value.md
    |   â””â”€â”€ 4.Typical_problems.md
    |   â””â”€â”€ 4.Fields_of_use.md
    â””â”€â”€ 2.Code/
    |   â””â”€â”€ Figures/
    |   â””â”€â”€ 
    â”œâ”€â”€ 3.Mathematics/
    |   â””â”€â”€ Figures/
    |   â””â”€â”€ 1.generalldeas.md 
    |   â””â”€â”€ 2.distances.md  
    |   â””â”€â”€ 3.regressionAndclassification.md   
    |   â””â”€â”€ 4.Regression_classification_KNN.md <---- You are here!! 
    |   â””â”€â”€ 5.Computational_Consideration_KNN.md
    |   â””â”€â”€ 6.Limits_KNN.md 
```

## ğŸ‘¾ **Contributors**
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher)
| :---: | 

# Portuguese version

# LimitaÃ§Ãµes MatemÃ¡ticas do KNN

## ViÃ©s e VariÃ¢ncia Dependentes de k e da Dimensionalidade

O erro esperado do estimador KNN pode ser decomposto em viÃ©s e variÃ¢ncia. A escolha de k influencia diretamente o equilÃ­brio entre esses dois termos:

- **Baixo k** â†’ Baixo viÃ©s, alta variÃ¢ncia
- **Alto k** â†’ Alto viÃ©s, baixa variÃ¢ncia

## A FÃ³rmula MatemÃ¡tica do Erro Esperado

A fÃ³rmula do erro esperado Ã© dada por:

$$
E\left[( r_k(X) - r(X) )^2 \right] = K \cdot \frac{{k}{n}}^{2/d} + \frac{sigma^2}{k}
$$

### Significado de Cada Termo

- **$E\left[( r_k(X) - r(X) )^2 \right]$**: erro quadrÃ¡tico mÃ©dio esperado.
- **k**: nÃºmero de vizinhos.
- **n** : tamanho da amostra (nÃºmero de dados de treinamento).
- **d**: nÃºmero de dimensÃµes.
- **$sigma^2$** : variÃ¢ncia dos erros (associada ao ruÃ­do).
- **K**: constante proporcional Ã  suavidade da funÃ§Ã£o verdadeira **r(x)**.

## Exemplo de CÃ¡lculo

Vamos usar os seguintes valores como exemplo:

- **k = 5**
- **n = 500**
- **d = 10**
- **$sigma^2 = 1$**
- **K = 1** (por simplicidade)

Substituindo na fÃ³rmula:

$$
E\left[( r_k(X) - r(X) )^2 \right] = 1 \cdot \frac{5^2}{500}^{2/10} + 5^2
$$

Calculando:

$$
E\left[( r_k(X) - r(X) )^2 \right] = 0,589 
$$

Isso mostra que, Ã  medida que a dimensionalidade **d** aumenta, a convergÃªncia fica mais lenta e a qualidade do modelo piora â€” a taxa de convergÃªncia cai exponencialmente com \( d \).

## Exige Suavidade da FunÃ§Ã£o Alvo

A anÃ¡lise teÃ³rica assume que a funÃ§Ã£o de regressÃ£o Ã© L-Lipschitz, ou seja, suavemente variÃ¡vel. Caso contrÃ¡rio, o KNN pode ser matematicamente instÃ¡vel para prever em regiÃµes com comportamento muito nÃ£o linear.

## ReferÃªncias
IZBICKI RAFAEL & MENDONÃ‡A D.S. TIAGO, SÃ£o Paulo, 2020. Aprendizado de MÃ¡quina: uma abordagem estatÃ­stica.

O'REILLY, Rio de Janeiro, 2019. MÃ£os Ã  Obras Aprendizado MÃ¡quina com Scikit-Learn e TensorFlow.

## **Onde estou?**
```text
RepoAI/
â””â”€â”€ KNN/
    â”œâ”€â”€ 1.Conceitos/
    â”‚   â””â”€â”€ 1.HistÃ³ria.md
    |   â””â”€â”€ 2.ClassificadorVsRegressao.md
    |   â””â”€â”€ 3.Como_determinar_o_valor_de_k.md
    |   â””â”€â”€ 4.Problemas_tÃ­picos.md
    |   â””â”€â”€ 4.Ãreas_de_aplicaÃ§Ã£o.md
    â””â”€â”€ 2.CÃ³digo/
    |   â””â”€â”€ Figuras/
    â”œâ”€â”€ 3.MatemÃ¡tica/
    |   â””â”€â”€ Figuras/
    |   â””â”€â”€ 1.Ideias_gerais.md 
    |   â””â”€â”€ 2.DistÃ¢ncias.md 
    |   â””â”€â”€ 4.RegressaoEclassificacao.md 
    |   â””â”€â”€ 4.Regressao_classificacao.md 
    |   â””â”€â”€ 5.Computacao_ConsideraÃ§Ãµes_KNN.md <---- VocÃª estÃ¡ aqui!! 
    |   â””â”€â”€ 6.Limites_KNN.md<---- VocÃª estÃ¡ aqui!!
```
## ğŸ‘¾ Colaboradores
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher)  
| :---: |