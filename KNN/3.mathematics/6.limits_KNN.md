# English Version

# Mathematical Limitations of KNN

## Bias and Variance Dependent on k and Dimensionality

The expected error of the KNN estimator can be decomposed into bias and variance. The choice of k directly influences the balance between these two terms:

- **Low k** → Low bias, high variance
- **High k** → High bias, low variance

## The Mathematical Formula for Expected Error

The formula for expected error is given by:

$$
E\left[( r_k(X) - r(X) )^2 \right] = \frac{K k^2}{n^{2/d} + k^2}
$$

### Meaning of Each Term

- **$E\left[( r_k(X) - r(X) )^2 \right]$**: expected mean squared error.
- **k**: number of neighbors.
- **n**: sample size (number of training data).
- **d**: number of dimensions.
- **$sigma^2$**: variance of the errors (associated with noise).
- **K**: constant proportional to the smoothness of the true function **r(x)**.

## Example Calculation

Let's use the following values as an example:

- **k = 5**
- **n = 500**
- **d = 10**
- **$sigma^2 = 1$**
- **K = 1** (for simplicity)

Substituting into the formula:

$$
E\left[( r_k(X) - r(X) )^2 \right] = 1 \cdot \frac{5^2}{500^{2/10} + 5^2}
$$

Calculating:

$$
E\left[( r_k(X) - r(X) )^2 \right] = 0.589 
$$

This shows that as the dimensionality **d** increases, convergence becomes slower and the quality of the model deteriorates — the rate of convergence decreases exponentially with \( d \).

## Requires Smoothness of the Target Function

The theoretical analysis assumes that the regression function is L-Lipschitz, meaning it varies smoothly. Otherwise, KNN can be mathematically unstable for predictions in regions with highly nonlinear behavior.

## References
IZBICKI RAFAEL & MENDONÇA D.S. TIAGO, São Paulo, 2020. Aprendizado de Máquina: uma abordagem estatística.

O'REILLY, Rio de Janeiro, 2019. Mãos à Obras Aprendizado Máquina com Scikit-Learn e TensorFlow.

## **Where Am I?**
```text
RepoAI/
└── KNN/
    ├── 1.Concepts/
    │   └── 1.History.md
    |   └── 2.ClassifierVsRegression.md
    |   └── 3.How_to_determine_k_value.md
    |   └── 4.Typical_problems.md
    |   └── 4.Fields_of_use.md
    └── 2.Code/
    |   └── Figures/
    |   └── 
    ├── 3.Mathematics/
    |   └── Figures/
    |   └── 1.generalldeas.md 
    |   └── 2.distances.md  
    |   └── 3.regressionAndclassification.md   
    |   └── 4.Regression_classification_KNN.md <---- You are here!! 
    |   └── 5.Computational_Consideration_KNN.md
    |   └── 6.Limits_KNN.md 
```

## 👾 **Contributors**
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher)
| :---: | 

# Portuguese version

# Limitações Matemáticas do KNN

## Viés e Variância Dependentes de k e da Dimensionalidade

O erro esperado do estimador KNN pode ser decomposto em viés e variância. A escolha de k influencia diretamente o equilíbrio entre esses dois termos:

- **Baixo k** → Baixo viés, alta variância
- **Alto k** → Alto viés, baixa variância

## A Fórmula Matemática do Erro Esperado

A fórmula do erro esperado é dada por:

$$
E\left[( r_k(X) - r(X) )^2 \right] = K \cdot \frac{{k}{n}}^{2/d} + \frac{sigma^2}{k}
$$

### Significado de Cada Termo

- **$E\left[( r_k(X) - r(X) )^2 \right]$**: erro quadrático médio esperado.
- **k**: número de vizinhos.
- **n** : tamanho da amostra (número de dados de treinamento).
- **d**: número de dimensões.
- **$sigma^2$** : variância dos erros (associada ao ruído).
- **K**: constante proporcional à suavidade da função verdadeira **r(x)**.

## Exemplo de Cálculo

Vamos usar os seguintes valores como exemplo:

- **k = 5**
- **n = 500**
- **d = 10**
- **$sigma^2 = 1$**
- **K = 1** (por simplicidade)

Substituindo na fórmula:

$$
E\left[( r_k(X) - r(X) )^2 \right] = 1 \cdot \frac{5^2}{500}^{2/10} + 5^2
$$

Calculando:

$$
E\left[( r_k(X) - r(X) )^2 \right] = 0,589 
$$

Isso mostra que, à medida que a dimensionalidade **d** aumenta, a convergência fica mais lenta e a qualidade do modelo piora — a taxa de convergência cai exponencialmente com \( d \).

## Exige Suavidade da Função Alvo

A análise teórica assume que a função de regressão é L-Lipschitz, ou seja, suavemente variável. Caso contrário, o KNN pode ser matematicamente instável para prever em regiões com comportamento muito não linear.

## Referências
IZBICKI RAFAEL & MENDONÇA D.S. TIAGO, São Paulo, 2020. Aprendizado de Máquina: uma abordagem estatística.

O'REILLY, Rio de Janeiro, 2019. Mãos à Obras Aprendizado Máquina com Scikit-Learn e TensorFlow.

## **Onde estou?**
```text
RepoAI/
└── KNN/
    ├── 1.Conceitos/
    │   └── 1.História.md
    |   └── 2.ClassificadorVsRegressao.md
    |   └── 3.Como_determinar_o_valor_de_k.md
    |   └── 4.Problemas_típicos.md
    |   └── 4.Áreas_de_aplicação.md
    └── 2.Código/
    |   └── Figuras/
    ├── 3.Matemática/
    |   └── Figuras/
    |   └── 1.Ideias_gerais.md 
    |   └── 2.Distâncias.md 
    |   └── 4.RegressaoEclassificacao.md 
    |   └── 4.Regressao_classificacao.md 
    |   └── 5.Computacao_Considerações_KNN.md <---- Você está aqui!! 
    |   └── 6.Limites_KNN.md<---- Você está aqui!!
```
## 👾 Colaboradores
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher)  
| :---: |