# English version

## Regression Formulation

The **k-nearest neighbours (KNN)** method is one of the most popular in the machine learning community.

The idea is to estimate the regression function **r(x)** for a given configuration **x** , using the average of the responses **$y_i$** of the  k  nearest neighbors to  x :

$$
g(x) = \frac{1}{k} \sum_{i‚ààN_x} y_i
$$


Where:
- **$N_x$**: set of the **k** observations closest to **x** 

**$N_x$** = \{i ‚àà \{1,..., n\} : d($x_i$, x) **$d_x^k$**\}

- **$d_x^k$**: distance from the **k** -th nearest neighbor of  **x**

The choice of **k** is made via cross-validation:
- Large **k**: high bias, low variance
- Small **k**: low bias, high variance


### Example: Estimating Property Price

| Property | Size (m¬≤) | Rooms | Price (R$) |
|----------|------------|--------|------------|
| 01       | 110        | 3      | 550,000    |
| 02       | 80         | 2      | 400,000    |
| 03       | 140        | 4      | 650,000    |
| 04       | 70         | 2      | 300,000    |
| 05       | 90         | 3      | 450,000    |

New property: Size = 100 m¬≤, Rooms = 3, k = 3

Euclidean Distance:  

$$
\text{dist} = \sqrt{(size_i - 100)^2 + (rooms_i - 3)^2}
$$

| Property | Distance to (100,3) |
|----------|----------------------|
| 01       | 10,00                |
| 02       | 20,02                |
| 03       | 40,01                |
| 04       | 30,02                |
| 05       | 10,00                |

Selected neighbors: 01, 05, 02  
Prices: 550.000  ; 450.000  ; 400.000

Now, considering all values of k from 1 to 5 and comparing the results, we can analyze the impact of \(k\) on the price estimation.

### k = 1:
The nearest neighbor is Property 01 (or 05, as they have the same distance), with a distance of 10.00 and a price of R$ 550,000 (or R$ 450,000). We will choose Property 01 for this case.

Estimated price (k=1): R$ 550,000

### k = 2:
The two nearest neighbors are Properties 01 and 05, with prices of R$ 550,000 and R$ 450,000.

$$
g(x) = \frac{1}{2}(550,000 + 450,000) = R$ 500,000
$$

Estimated price (k=2): R$ 500,000

### k = 3:
The three nearest neighbors are Properties 01, 05, and 02, with prices of R$ 550,000, R$ 450,000, and R$ 400,000.

$$
g(x) = \frac{1}{3}(550,000 + 450,000 + 400,000) = R$ 466,666.67
$$

Estimated price (k=3): R$ 466,666.67

### k = 4:

The four nearest neighbors are Properties 01, 05, 02, and 04, with prices of R$ 550,000, R$ 450,000, R$ 400,000, and R$ 300,000.

$$
g(x) = \frac{1}{4}(550,000 + 450,000 + 400,000 + 300,000) = R$ 425,000
$$

Estimated price (k=4): R$ 425,000

### k = 5:

The five nearest neighbors are Properties 01, 05, 02, 04, and 03, with prices of R$ 550,000, R$ 450,000, R$ 400,000, R$ 300,000, and R$ 650,000.

$$
g(x) = \frac{1}{5}(550,000 + 450,000 + 400,000 + 300,000 + 650,000) = R$ 470,000
$$

Estimated price (k=5): R$ 470,000

$$
g(x) = \frac{1}{3}(550.000 + 450.000 + 400.000) = 466.666,67~R\$
$$
<div align="center">
  <img src="./Figures/grafic_P.png" alt="initial-banner" width="800">
</div>
---

## Classification Formulation

KNN can also be used for classification. The classifier is:

$$
g(x) = \text{mode}_{i \in N_x} y_i
$$

That is, the most frequent class among the \( k \) nearest neighbors of \( x \).

### Example: Heart Disease Diagnosis

| ID | Resting Heart Rate (bpm) | Cholesterol (mg/dL) | Diagnosis        |
|----|---------------------------|----------------------|------------------|
| 01 | 82                        | 220                  | Heart Disease    |
| 02 | 70                        | 190                  | No Disease       |
| 03 | 90                        | 210                  | Heart Disease    |
| 04 | 72                        | 180                  | No Disease       |
| 05 | 85                        | 220                  | Heart Disease    |

## KNN Classification for New Patient (Heart Disease Diagnosis)

**New Patient Data:** Heart rate = 78 bpm, Cholesterol = 200 mg/dL

### 1. KNN with \( k = 1 \)

**Euclidean Distance:**

$$
\text{dist} = \sqrt{(freq_i - 78)^2 + (colest_i - 200)^2}
$$

| ID | Distance | Diagnosis      |
|----|----------|----------------|
| 02 | 12.81    | No Heart Disease |

**Mode:** **No Heart Disease**

The new patient is classified as **No Heart Disease** with KNN of \( k = 1 \).

### 2. KNN with \( k = 2 \)

**Euclidean Distance:**

$$
\text{dist} = \sqrt{(freq_i - 78)^2 + (colest_i - 200)^2}
$$

| ID | Distance | Diagnosis      |
|----|----------|----------------|
| 02 | 12.81    | No Heart Disease |
| 03 | 15.62    | Heart Disease    |

We have one vote for **No Heart Disease** and one vote for **Heart Disease**. In case of a tie, we can consider the nearest neighbor **(ID 02)**.

The new patient is classified as **No Heart Disease** with KNN of \( k = 2 \).

### 3. KNN with \( k = 3 \)

**Euclidean Distance:**

$$
\text{dist} = \sqrt{(freq_i - 78)^2 + (colest_i - 200)^2}
$$

| ID | Distance | Diagnosis      |
|----|----------|----------------|
| 02 | 12.81    | No Heart Disease |
| 03 | 15.62    | Heart Disease    |
| 01 | 20.40    | Heart Disease    |

**Mode:** **Heart Disease**

The new patient is classified as **Heart Disease** with KNN of \( k = 3 \).

### 4. KNN with \( k = 4 \)

**Euclidean Distance:**

$$
\text{dist} = \sqrt{(freq_i - 78)^2 + (colest_i - 200)^2}
$$

| ID | Distance | Diagnosis      |
|----|----------|----------------|
| 02 | 12.81    | No Heart Disease |
| 03 | 15.62    | Heart Disease    |
| 01 | 20.40    | Heart Disease    |
| 04 | 20.40    | No Heart Disease |

We have two votes for **No Heart Disease** and two votes for **Heart Disease**. Again, in case of a tie, we can consider the nearest neighbors. The two closest **(ID 02 and ID 03)** have different diagnoses. Another approach would be to consider the order of distances, thus **(ID 02)**.

The new patient is classified as **No Heart Disease** with KNN of \( k = 4 \).

### 5. KNN with \( k = 5 \)

Considering all five nearest neighbors, we have two votes for **No Heart Disease** and three votes for **Heart Disease**.

The new patient is classified as **Heart Disease** with KNN of \( k = 5 \).

<div align="center">
  <img src="./Figures/grafic_C.png" alt="initial-banner" width="800">
</div>

>The graph shows the New Patient and the 5 nearest neighbors. The new patient is classified as **Heart Disease** or **No Disease** depending on how the k value is classified. The k value is a parameter that can be adjusted to improve the classification accuracy.

## References
IZBICKI RAFAEL & MENDON√áA D.S. TIAGO, S√£o Paulo, 2020. Aprendizado de M√°quina: uma abordagem estat√≠stica.

O'REILLY, Rio de Janeiro, 2019. M√£os √† Obras Aprendizado M√°quina com Scikit-Learn e TensorFlow.

## **Where am I?**
```text
RepoAI/
‚îî‚îÄ‚îÄ KNN/
    ‚îú‚îÄ‚îÄ 1.Concepts/
    ‚îÇ   ‚îî‚îÄ‚îÄ 1.History.md
    |   ‚îî‚îÄ‚îÄ 2.ClassiferVsRegression.md
    |   ‚îî‚îÄ‚îÄ 3.How_to_determine_the_value_of_k.md
    |   ‚îî‚îÄ‚îÄ 4.Typical_problems.md
    |   ‚îî‚îÄ‚îÄ 4.Fields_of_use.md
    ‚îî‚îÄ‚îÄ 2.Code/
    |   ‚îî‚îÄ‚îÄ Figures/
    |   ‚îî‚îÄ‚îÄ 
    ‚îú‚îÄ‚îÄ 3.Mathematics/
    |   ‚îî‚îÄ‚îÄ Figures/
    |   ‚îî‚îÄ‚îÄ 1.generalldeas.md 
    |   ‚îî‚îÄ‚îÄ 2.distances.md  
    |   ‚îî‚îÄ‚îÄ 3.regressionAndclassification.md   
    |   ‚îî‚îÄ‚îÄ 4.Regression_classification_KNN.md <---- You are here!! 
    |   ‚îî‚îÄ‚îÄ 5.Computational_Consideration_KNN.md
    |   ‚îî‚îÄ‚îÄ 6.Limits_KNN.md 
```
## üëæ **Contributors**
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher) 
| :---: | 


---
# Portuguese version
---

## Formula√ß√£o de Regress√£o

O m√©todo dos **k-vizinhos mais pr√≥ximos (KNN)** √© um dos mais populares na comunidade de aprendizado de m√°quina.

A ideia √© estimar a fun√ß√£o de regress√£o **r(x)** para uma determinada configura√ß√£o **x**, utilizando a m√©dia das respostas **$y_i$** dos  k  vizinhos mais pr√≥ximos de  x:

$$
g(x) = \frac{1}{k} \sum_{i‚ààN_x} y_i
$$

Onde:
- **$N_x$**: conjunto das **k** observa√ß√µes mais pr√≥ximas de **x**  

**$N_x$** = \{i ‚àà \{1,..., n\} : d($x_i$, x) ‚â§ **$d_x^k$**\}

- **$d_x^k$**: dist√¢ncia do **k**-√©simo vizinho mais pr√≥ximo de  **x**

A escolha de **k** √© feita por valida√ß√£o cruzada:
- **k** grande: alto vi√©s, baixa vari√¢ncia
- **k** pequeno: baixo vi√©s, alta vari√¢ncia

### Exemplo: Estimando o Pre√ßo de um Im√≥vel

| Im√≥vel | Tamanho (m¬≤) | Quartos | Pre√ßo (R$) |
|--------|---------------|---------|------------|
| 01     | 110           | 3       | 550.000    |
| 02     | 80            | 2       | 400.000    |
| 03     | 140           | 4       | 650.000    |
| 04     | 70            | 2       | 300.000    |
| 05     | 90            | 3       | 450.000    |

Novo im√≥vel: Tamanho = 100 m¬≤, Quartos = 3, k = 3

Dist√¢ncia Euclidiana:  

$$
\text{dist} = \sqrt{(tamanho_i - 100)^2 + (quartos_i - 3)^2}
$$

| Im√≥vel | Dist√¢ncia at√© (100,3) |
|--------|------------------------|
| 01     | 10,00                  |
| 02     | 20,02                  |
| 03     | 40,01                  |
| 04     | 30,02                  |
| 05     | 10,00                  |

Vizinhos selecionados: 01, 05, 02  
Pre√ßos: 550.000 ; 450.000 ; 400.000

$$
g(x) = \frac{1}{3}(550.000 + 450.000 + 400.000) = 466.666,67 R$
$$

Agora fazendo para todos os valores de k = 1  at√© k = 5, e comparando os resultados, podemos escolher o melhor valor de k.

###k = 1: 
O vizinho mais pr√≥ximo √© o Im√≥vel 01 (ou 05, pois t√™m a mesma dist√¢ncia), com dist√¢ncia de 10.00 e pre√ßo de R$ 550.000 (ou R$ 450.000). Vamos escolher o Im√≥vel 01 para este caso.

Pre√ßo estimado (k=1):  550.000 R$

###k = 2:
Os dois vizinhos mais pr√≥ximos s√£o os Im√≥veis 01 e 05, com pre√ßos de R$ 550.000 e R$ 450.000.

$$
g(x) = \frac{1}{2}(550.000 + 450.000 ) = 500.000¬†R$ 
$$

Pre√ßo estimado (k=2): 500.000 R$

###k = 4:

Os quatro vizinhos mais pr√≥ximos s√£o os Im√≥veis 01, 05, 02 e 04, com pre√ßos de R$ 550.000, R$ 450.000, R$ 400.000 e R$ 300.000.

$$
g(x) = \frac{1}{4}(550.000 + 450.000 + 400.000 + 300.000) = 425.000¬†R$ 
$$

Pre√ßo estimado (k=4):  425.000 R$

###k = 5:

Os cinco vizinhos mais pr√≥ximos s√£o os Im√≥veis 01, 05, 02, 04 e 03, com pre√ßos de R$ 550.000, R$ 450.000, R$ 400.000, R$ 300.000 e R$ 650.000.

$$
g(x) = \frac{1}{5}(550.000 + 450.000 + 400.000 + 300.000 + 650.000) = 470.000¬†R$ 
$$

<div align="center">
  <img src="./Figures/grafic_P.png" alt="banner-inicial" width="800">
</div>

---

## Formula√ß√£o de Classifica√ß√£o

O KNN tamb√©m pode ser usado para classifica√ß√£o. O classificador √©:

$$
g(x) = \text{moda}_{i \in N_x} y_i
$$

Ou seja, a classe mais frequente entre os \( k \) vizinhos mais pr√≥ximos de \( x \).

### Exemplo: Diagn√≥stico de Doen√ßa Card√≠aca

| ID | Freq. Card√≠aca Repouso (bpm) | Colesterol (mg/dL) | Diagn√≥stico     |
|----|-------------------------------|----------------------|------------------|
| 01 | 82                            | 220                  | Doen√ßa Card√≠aca |
| 02 | 70                            | 190                  | Sem Doen√ßa      |
| 03 | 90                            | 210                  | Doen√ßa Card√≠aca |
| 04 | 72                            | 180                  | Sem Doen√ßa      |
| 05 | 85                            | 220                  | Doen√ßa Card√≠aca |

1. Novo paciente: Freq. card√≠aca = 78 bpm, Colesterol = 200 mg/dL, \( k = 1 \)

Dist√¢ncia Euclidiana:

$$
\text{dist} = \sqrt{(freq_i - 78)^2 + (colest_i - 200)^2}
$$

| ID | Dist√¢ncia | Diagn√≥stico     |
|----|-----------|------------------|
| 02 | 12,81     | Sem Doen√ßa      |

Moda: **Sem Doen√ßa**

O novo paciente √© classificado como **Sem Doen√ßa** com KNN de \( k = 1 \).

2. Novo paciente: Freq. card√≠aca = 78 bpm, Colesterol = 200 mg/dL, \( k = 2 \)

Dist√¢ncia Euclidiana:

$$
\text{dist} = \sqrt{(freq_i - 78)^2 + (colest_i - 200)^2}
$$

| ID | Dist√¢ncia | Diagn√≥stico     |
|----|-----------|------------------|
| 02 | 12,81     | Sem Doen√ßa      |
| 03 | 15,62     | Doen√ßa Card√≠aca |

Temos um voto para **Sem Doen√ßa** e um voto para **Doen√ßa Card√≠aca**. Em caso de empate, podemos considerar o vizinho mais pr√≥ximo **(ID 02)**.

O novo paciente √© classificado como **Sem Doen√ßa** com KNN de \( k = 2 \).

3. Novo paciente: Freq. card√≠aca = 78 bpm, Colesterol = 200 mg/dL, \( k = 3 \)

Dist√¢ncia Euclidiana:

$$
\text{dist} = \sqrt{(freq_i - 78)^2 + (colest_i - 200)^2}
$$

| ID | Dist√¢ncia | Diagn√≥stico     |
|----|-----------|------------------|
| 02 | 12,81     | Sem Doen√ßa      |
| 03 | 15,62     | Doen√ßa Card√≠aca |
| 01 | 20,40     | Doen√ßa Card√≠aca |

Moda: **Doen√ßa Card√≠aca**

O novo paciente √© classificado como **Doen√ßa Card√≠aca** com KNN de \( k = 3 \).

4. Novo paciente: Freq. card√≠aca = 78 bpm, Colesterol = 200 mg/dL, \( k = 4 \)

Dist√¢ncia Euclidiana:

$$
\text{dist} = \sqrt{(freq_i - 78)^2 + (colest_i - 200)^2}
$$

| ID | Dist√¢ncia | Diagn√≥stico     |
|----|-----------|------------------|
| 02 | 12,81     | Sem Doen√ßa      |
| 03 | 15,62     | Doen√ßa Card√≠aca |
| 01 | 20,40     | Doen√ßa Card√≠aca |
| 04 | 20,40     | Sem Doen√ßa      |

Temos dois votos para **Sem Doen√ßa** e dois votos para **Doen√ßa Card√≠aca**. Novamente, em caso de empate, podemos considerar os vizinhos mais pr√≥ximos. Os dois mais pr√≥ximos **(ID 02 e ID 03)** t√™m diagn√≥sticos diferentes. Uma outra abordagem seria considerar a ordem das dist√¢ncias, com isso **\(ID 02\).

O novo paciente √© classificado como **Sem Doen√ßa** com KNN de \( k = 4 \).

5. Novo paciente: Freq. card√≠aca = 78 bpm, Colesterol = 200 mg/dL, \( k = 5 \)

Os cinco vizinhos mais pr√≥ximos (todos os pacientes), temos dois votos para **Sem Doen√ßa** e tr√™s votos para **Doen√ßa Card√≠aca**.

O novo paciente √© classificado como **Doen√ßa Cardiaca** com KNN de \( k = 5 \).

<div align="center">
  <img src="./Figures/grafic_C.png" alt="banner-inicial" width="800">
</div>

>O gr√°fico mostra o Novo Paciente e os 5 vizinhos mais pr√≥ximos. O novo paciente √© classificado como **Doen√ßa Card√≠aca** ou **Sem Doen√ßa** dependo de como vai ser feito a classifica√ß√£o do valor de k. O valor de k √© um par√¢metro que pode ser ajustado para melhorar a precis√£o da classifica√ß√£o.

## Refer√™ncias
IZBICKI RAFAEL & MENDON√áA D.S. TIAGO, S√£o Paulo, 2020. Aprendizado de M√°quina: uma abordagem estat√≠stica.

O'REILLY, Rio de Janeiro, 2019. M√£os √† Obras Aprendizado M√°quina com Scikit-Learn e TensorFlow.

## **Onde estou?**
```text
RepoAI/
‚îî‚îÄ‚îÄ KNN/
    ‚îú‚îÄ‚îÄ 1.Conceitos/
    ‚îÇ   ‚îî‚îÄ‚îÄ 1.Hist√≥ria.md
    |   ‚îî‚îÄ‚îÄ 2.ClassificadorVsRegressao.md
    |   ‚îî‚îÄ‚îÄ 3.Como_determinar_o_valor_de_k.md
    |   ‚îî‚îÄ‚îÄ 4.Problemas_t√≠picos.md
    |   ‚îî‚îÄ‚îÄ 4.√Åreas_de_aplica√ß√£o.md
    ‚îî‚îÄ‚îÄ 2.C√≥digo/
    |   ‚îî‚îÄ‚îÄ Figuras/
    ‚îú‚îÄ‚îÄ 3.Matem√°tica/
    |   ‚îî‚îÄ‚îÄ Figuras/
    |   ‚îî‚îÄ‚îÄ 1.Ideias_gerais.md 
    |   ‚îî‚îÄ‚îÄ 2.Dist√¢ncias.md 
    |   ‚îî‚îÄ‚îÄ 3.RegressaoEclassificacao.md 
    |   ‚îî‚îÄ‚îÄ 4.Regressao_classificacao.md <---- Voc√™ est√° aqui!! 
    |   ‚îî‚îÄ‚îÄ 5.Computacao_Considera√ß√µes_KNN.md
    |   ‚îî‚îÄ‚îÄ 6.Limites_KNN.md     
```
## üëæ Colaboradores
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher)  
| :---: | 


