# English Version

## Distances

# Portuguese Version

## Distâncias
Como explicado em [1.generalIdeas.md](./1.generalIdeas.md), faremos previsões com base na proximidade dos dados no espaço de características. Então, precisamos de formas de calcular as distâncias.

A maneira mais comum de calcularmos a distância é utilizando a ideia de Distância Euclidiana.

### Distância Euclidiana
A Distância Euclidiana calcula uma linha reta entre dois pontos $(x_1, x_2, \dots, x_n)$ e $(y_1, y_2, \dots, y_n)$. Ela pode ser representada da seguinte forma:

$$
d = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
$$

No caso particular de duas dimensões, podemos representar da seguinte maneira:

$$
d = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
$$

#### Exemplo:

Para A = (1, 2) e B = (4, 6):  

$$
d_{\text{euclid}} = \sqrt{(4 - 1)^2 + (6 - 2)^2} \\
= \sqrt{3^2 + 4^2} \\
= \sqrt{9 + 16} \\
= \sqrt{25} \\
= 5
$$

Graficamente, estamos calculando a linha tracejada:

![distanciaeuclideana](Figures/distanciaeuclideana.png)


### Distância de Manhattan
Ela considera a soma das diferenças absolutas entre as coordenadas dos pontos.

A Distância de Manhattan entre dois pontos $(x_1, x_2, \dots, x_n)$ e $(y_1, y_2, \dots, y_n)$ é:

$$
d = \sum_{i=1}^{n} |x_i - y_i|
$$

#### Exemplo:

Para A = (1, 2) e B = (4, 6) (que são os mesmos valores do exemplo anterior): 

$$
d_{\text{manhattan}} = |4 - 1| + |6 - 2| \\
= 3 + 4 \\
= 7
$$

Graficamente, estamos calculando a linha tracejada:

![distanciamanhattan](Figures/distanciamanhattan.png)


#### Quando utilizar Euclidiana ou Manhattan no contexto do KNN? 
A Distância de Manhattan atribui menos ênfase a diferenças grandes entre os pontos. Por isso, quando temos valores extremos ("outliers"), pode ser mais adequado utilizá-la. Enquanto isso, a Distância Euclidiana é mais adequada para dados com características que possuem relação mais direta e contínua.

### Distância de Minkowski
Generaliza as distâncias euclidiana e de Manhattan. Para euclidiana, utilizamos p = 2, e, para Manhattan, p = 1.

A distância de Minkowski entre dois pontos $(x_1, x_2, \dots, x_n)$ e $(y_1, y_2, \dots, y_n)$ com parâmetro \(p\) é:

$$
d = \left( \sum_{i=1}^{n} |x_i - y_i|^p \right)^{1/p}
$$

### Similaridade de Cossenos
É muito utilizada em classificação de texto. Palavras são dimensões e os documentos são vetores de palavras.

$$
\cos(\theta) = \frac{\vec{x} \cdot \vec{y}}{ \|\vec{x}\| \cdot \|\vec{y}\| } 
$$

-  $\theta$ é o **ângulo** entre os vetores. Quanto menor o ângulo, maior a similaridade.
  
-  $\vec{x}$ e $\vec{y}$ são os **vetores** que queremos comparar (por exemplo, documentos representados por vetores de palavras).
  
-  $\vec{x}$ $\cdot$ $\vec{y}$ é o **produto escalar** entre os vetores.
  
- $\|\vec{x}\|$ e $\|\vec{y}\|$ são as **normas (módulos)** dos vetores, ou seja, seus comprimentos:
  
$$
\|\vec{x}\| = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
\quad\quad
\|\vec{y}\| = \sqrt{y_1^2 + y_2^2 + \cdots + y_n^2}
$$

#### Exemplo:

Utilizaremos os mesmos números dos exemplos anteriores, mas contextualizaremos para o caso de classificação de textos.

Digamos que temos dois textos com as palavras: UFSC (sigla da nossa universidade) e LIA (digla da Liga de Inteligência Artificial, da qual fazemos parte).

- O primeiro texto contém a palavra UFSC uma vez, e a palavra LIA duas vezes. Representaremos ele com o vetor 

$$
\vec{A} = [1,\ 2] 
$$

- O segundo texto contém a palavra UFSC quatro vezes, e a palavra LIA seis vezes. Representaremos ele com o vetor [4, 6].

$$   
\vec{B} = [4,\ 6]
$$

> [!NOTE]
> Levaremos em consideração apenas duas palavras para fins didáticos. No entanto, ao analisarmos textos, podemos ter vetores com muitas dimensões!

Graficamente, temos:

![similaridadecosseno](Figures/similaridadecosseno.png)

Calcularemos a similaridade de cossenos dos dois documentos: 

1. Produto escalar:

$$
\vec{A} \cdot \vec{B} = (1 \cdot 4) + (2 \cdot 6) = 4 + 12 = 16
$$

2. Norma dos vetores:

$$
\|\vec{A}\| = \sqrt{1^2 + 2^2} = \sqrt{1 + 4} = \sqrt{5}
$$

$$
\|\vec{B}\| = \sqrt{4^2 + 6^2} = \sqrt{16 + 36} = \sqrt{52}
$$

3. Similaridade de cosseno:

$$
\cos(\theta) = \frac{\vec{A} \cdot \vec{B}}{\|\vec{A}\| \cdot \|\vec{B}\|} = \frac{16}{\sqrt{5} \cdot \sqrt{52}} \approx \frac{16}{16.1245} \approx 0.9923
$$

Perceba que o fato de o cosseno se aproximar de 1, indica que o ângulo é próximo de zero. Isto é, temos alta similaridade entre os dois documentos!

> [!TIP]
> No contexto do KNN, utilizamos a similaridade do cosseno para que o modelo decida se um terceiro documento, por exemplo, está mais próximo do primeiro ou do segundo. Quanto menor o ângulo, maior a proximidade do vizinho!

