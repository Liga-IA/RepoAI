# English Version

## "k" in Regression and Classification

# Portuguese Version

## "k" em Regressão e Classificação

Como explicado em (link para conceitual), temos três características de k:
- k, quando pequeno, tende a gerar overfitting
- k, quando grande, tende a gerar underfitting
- k, quando par, pode gerar empates nos problemas de classificação e, consequentemente, imprecisões.

Ao escolhermos o melhor valor, podemos utilizar como ponto de partida a raiz quadrada do número de dados disponíveis. No entanto, uma forma mais assertiva para essa decisão é utilizar técnicas de validação cruzada!

Mas, agora, vamos entender melhor a matemática por trás dessas características!

Para entendermos se k é grande ou pequeno, precisamos abordar os conceitos de **erro**, **viés** e **variância**.

### Erro

Podemos calcular o erro produzido por um modelo de mais de uma forma. No entanto, aqui foi escolhida uma forma para regressão, e uma para classificação.

#### Erro em regressão
Para regressão, o erro total pode ser calculado por meio do "Erro Quadrático Médio" (já abordado em Regressao Linear - link para pagina), cuja fórmula é: 

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

Em que:
- $\text{MSE}$ é o erro quadrático médio (Mean Squared Error, em inglês)
- $n$ é o número total de amostras no conjunto de dados.  
- $y_i$ é o valor real observado da $i$-ésima amostra.  
- $\hat{y}_i$ é o valor predito pelo modelo para a $i$-ésima amostra.  
- $(y_i - \hat{y}_i)^2$ é o erro ao quadrado da $i$-ésima amostra, que penaliza mais fortemente discrepâncias maiores.  

#### Erro em classificação
Para classificação, podemos calcular a taxa de erro da seguinte maneira:

$$
\text{Taxa de Erro} = \frac{\text{Número de previsões incorretas}}{\text{Número total de amostras}}
$$

### Viés

