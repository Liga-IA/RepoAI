# English Version

## "k" in Regression and Classification

# Portuguese Version

## "k" em Regressão e Classificação

Como explicado em (link para conceitual), temos três características de k:
- k, quando pequeno, tende a gerar overfitting
- k, quando grande, tende a gerar underfitting
- k, quando par, pode gerar empates nos problemas de classificação e, consequentemente, imprecisões.

Ao escolhermos o melhor valor, podemos utilizar como ponto de partida a raiz quadrada do número de dados disponíveis. No entanto, uma forma mais assertiva para essa decisão é utilizar técnicas de validação cruzada!

Mas, agora, vamos entender melhor a matemática por trás dessas características!

Para entendermos se k é grande ou pequeno, precisamos abordar os conceitos de **erro**, **viés** e **variância**.

### Erro

Podemos calcular o erro produzido por um modelo de mais de uma forma. No entanto, aqui foi escolhida uma forma para regressão, e uma para classificação.

#### Erro em regressão
Para regressão, o erro total pode ser calculado por meio do "Erro Quadrático Médio" (já abordado em Regressao Linear - link para pagina), cuja fórmula é: 

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

Em que:
- $\text{MSE}$ é o erro quadrático médio (Mean Squared Error, em inglês)
- $n$ é o número total de amostras no conjunto de dados.  
- $y_i$ é o valor real observado da $i$-ésima amostra.  
- $\hat{y}_i$ é o valor predito pelo modelo para a $i$-ésima amostra.  
- $(y_i - \hat{y}_i)^2$ é o erro ao quadrado da $i$-ésima amostra, que penaliza mais fortemente discrepâncias maiores.  

#### Erro em classificação
Para classificação, podemos calcular a taxa de erro da seguinte maneira:

$$
\text{Taxa de Erro} = \frac{\text{Número de previsões incorretas}}{\text{Número total de amostras}}
$$

### Viés
É quando um modelo é um muito simples para captar o comportamento real dos dados.

Em regressão linear, por exemplo, seria como tentar representar um padrão curvo com uma reta.
No contexto de KNN, seria generalizar o modelo de tal forma que ele não capta padrões locais. Isto ocorre quando usamos valores de k muito grandes, já que está considerando muitos vizinhos para chegar a uma resposta.

**alto k** -> **alto viés** -> **underfitting**

> Portanto, podemos entender o viés por meio do mesmo exemplo utilizado em "Underfitting e valores altos de k" na parte conceitual (link).

### Variância
Se refere a quanto as previsões do modelo mudariam caso fosse treinado com outro conjunto de dados. 

Ou seja, quando há alta variância, se adicionarmos ou removermos um ponto, é possível que a predição mude drasticamente. Isto ocorre quando usamos valores de k muito pequenos, já que está considerando poucos vizinhos para chegar a uma resposta.

**baixo k** -> **alta variância** -> **overfitting**

> Portanto, podemos entender a variância por meio do mesmo exemplo utilizado em "Overfitting e valores baixos de k" na parte conceitual (link).

### Relação entre Erro, Viés e Variância
