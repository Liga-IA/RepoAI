# Considerações Computacionais do KNN: Desafios e Estratégias

O algoritmo K-Nearest Neighbors (KNN) é conhecido por sua simplicidade conceitual e eficácia em diversos problemas de classificação e regressão. No entanto, suas características intrínsecas impõem desafios significativos em termos de custo computacional, especialmente em cenários com grandes volumes de dados e alta dimensionalidade.

## 1. Alta Carga Computacional na Fase de Inferência (Lazy Learning)

Uma das principais características do KNN é sua abordagem de "aprendizado preguiçoso" (lazy learner). Diferentemente de algoritmos como árvores de decisão ou regressão linear, o KNN não passa por uma fase explícita de treinamento para construir um modelo. Em vez disso, ele armazena todo o conjunto de dados de treinamento e realiza os cálculos necessários apenas no momento da predição para uma nova instância.


## 2. Complexidade de Tempo na Predição

A complexidade de tempo para realizar uma única predição com o KNN é de aproximadamente O(n×d), onde:

- n representa o número de instâncias (pontos) no conjunto de treinamento.
- d representa a dimensionalidade dos dados (número de atributos ou features).

Essa complexidade surge porque, para cada novo ponto a ser classificado, o algoritmo precisa calcular a distância até todos os n pontos de treinamento e, em seguida, ordenar essas distâncias para encontrar os k menores.

> [!NOTE]
>- **Grandes Bases de Dados**: Em conjuntos de dados com milhões ou bilhões de instâncias, o tempo de predição para cada novo ponto pode se tornar proibitivo para aplicações em tempo real.
>- **Aplicações em Tempo Real**: Para sistemas que exigem respostas rápidas (por exemplo, sistemas de recomendação online, detecção de fraudes em tempo real), a latência introduzida pelo KNN pode ser inaceitável sem otimizações.

## 3. Sensibilidade à Dimensionalidade (A Maldição da Dimensionalidade)

À medida que o número de dimensões (d) aumenta, o desempenho e a eficiência do KNN tendem a se degradar significativamente. Esse fenômeno é conhecido como a "maldição da dimensionalidade".

### Razões

- **Espaçamento dos Dados**: Em espaços de alta dimensão, os pontos de dados tendem a se tornar mais esparsos e a distância entre eles se torna menos discriminativa.
- **Concentração de Distâncias**: As distâncias entre os pontos tendem a se concentrar em uma faixa estreita, tornando difícil distinguir os vizinhos verdadeiramente próximos dos mais distantes.
- **Relevância dos Atributos**: Em espaços de alta dimensão, é mais provável que existam muitos atributos irrelevantes ou redundantes que adicionam ruído ao cálculo da distância.

## 4. Ausência de Seleção de Atributos Intrínseca

O algoritmo KNN trata todas as variáveis (atributos) como igualmente importantes ao calcular a distância entre os pontos. Se o conjunto de dados contiver muitas variáveis irrelevantes ou redundantes, elas contribuirão para o cálculo da distância da mesma forma que as variáveis relevantes.

>Considere um problema de classificação de clientes com base em seus hábitos de compra. Se incluirmos atributos como o número de letras no nome do cliente (que é irrelevante para o comportamento de compra), o KNN considerará essa característica ao calcular a similaridade entre os clientes, o que pode levar a vizinhos "próximos" que não são realmente semelhantes em termos de comportamento de compra.

## Estratégias para Mitigar os Desafios Computacionais

Apesar desses desafios, existem várias técnicas e abordagens que podem ser empregadas para melhorar a eficiência computacional do KNN:

### Estruturas de Dados para Busca de Vizinhos Próximos

- **Árvores KD-Tree e Ball-Tree**: Essas estruturas de dados particionam o espaço de treinamento de forma hierárquica, permitindo encontrar os s vizinhos mais próximos de um ponto de consulta de maneira mais eficiente do que uma busca exaustiva. A complexidade da busca pode ser reduzida para O(dlogn) em muitos casos, embora o desempenho possa degradar em dimensões muito altas.

## Referências
IZBICKI RAFAEL & MENDONÇA D.S. TIAGO, São Paulo, 2020. Aprendizado de Máquina: uma abordagem estatística.

O'REILLY, Rio de Janeiro, 2019. Mãos à Obras Aprendizado Máquina com Scikit-Learn e TensorFlow.


## Colaboradores
|  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/153019298?v=4" width=115><br><sub>Seidi Ducher</sub>](https://github.com/seidiDucher)  
| :---: |
