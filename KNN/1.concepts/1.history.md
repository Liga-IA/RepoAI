# English version

## Introduction ##
  K-Nearest Neighbor is a supervised machine learning model, meaning it learns from labeled data consisting of inputs (x) and their corresponding desired outputs (y). This model is employed for predicting both categorical and numerical data, utilizing an instance-based learning approach. This approach involves storing the training data, subsequently, when a new instance is presented, a set of related, similar instances is retrieved from memory and used to classify the novel query instance.

  In the k-Nearest Neighbor (k-NN) model, each data instance is represented as a point in a feature space. New instances are classified based on their proximity to instances in the training set, using a similarity measure. In this way, the prediction is made by considering the k nearest neighbors.

## History ##
  The k-Nearest Neighbor (k-NN) algorithm originated in the 1950s, initially proposed by Evelyn Fix and Joseph Hodges in a theoretical work presented as *Technical Report No. 21* at the University of California, Berkeley, in 1951. Years later, during the 1960s, the algorithm was refined and formally developed by Thomas M. Cover and Peter E. Hart in their 1967 paper *"Nearest Neighbor Pattern Classification."*.

# Portuguese version

# Introdução

O K-Vizinhos Mais Próximos (k-Nearest Neighbor, ou k-NN) é um modelo de aprendizado de máquina supervisionado, o que significa que ele aprende a partir de dados rotulados, compostos por entradas (x) e suas respectivas saídas desejadas (y). Esse modelo é utilizado para prever tanto dados categóricos quanto numéricos, empregando uma abordagem de aprendizado baseado em instâncias. Essa abordagem envolve o armazenamento dos dados de treinamento, posteriormente, quando uma nova instância é apresentada, um conjunto de instâncias semelhantes é recuperado da memória e utilizado para classificar a nova instância de consulta.

No modelo k-Vizinhos Mais Próximos (k-NN), cada instância de dados é representada como um ponto em um espaço de características. Novas instâncias são classificadas com base em sua proximidade com instâncias do conjunto de treinamento, utilizando uma medida de similaridade. Dessa forma, a predição é feita considerando os k vizinhos mais próximos.

# Histórico

O algoritmo k-Vizinhos Mais Próximos (k-NN) surgiu na década de 1950, proposto inicialmente por Evelyn Fix e Joseph Hodges em um trabalho teórico apresentado como *Technical Report No. 21* na Universidade da Califórnia, Berkeley, em 1951. Anos depois, durante a década de 1960, o algoritmo foi refinado e formalmente desenvolvido por Thomas M. Cover e Peter E. Hart em seu artigo de 1967, intitulado *"Nearest Neighbor Pattern Classification"*.
