# English version

Although k-NN is a relatively simple algorithm, it presents several common challenges that become more evident as the number of samples increases. The main issues include:

## 1. High Computational Cost
As the volume of data in a k-NN algorithm grows, so do its complexity and computational cost. This is because the algorithm needs to calculate the distance between a new data point and all existing points, which requires more memory to store these distances and to perform comparisons with the k nearest neighbors. As a result, both computational cost and processing time increase, especially for larger test datasets.

### 1.1 How to mitigate the problem?
To reduce this issue, the ideal approach is to limit the amount of data processed by the algorithm. One effective strategy is data condensation, which involves selecting relevant subsets for analysis and removing redundant samples or even outliers. This helps the algorithm maintain both accuracy and efficiency.

<details>
    <summary><strong>How are these subsets selected?</strong></summary> 
    Various strategies can be used to select data subsets. One common approach is the selection of relevant features using techniques such as Principal Component Analysis (PCA), correlation analysis, or supervised learning algorithms that identify the most influential features for classification. Another strategy involves using methods like Locality-Sensitive Hashing (LSH) or structures like KD-Tree, which approximate the nearest neighbors without comparing the new point to the entire dataset, thereby significantly reducing both time and computational cost.
</details>

## 2. Curse of Dimensionality
Another major challenge of the k-NN algorithm is the curse of dimensionality. The k-NN method performs best in low-dimensional spaces, but as the number of dimensions increases, distance metrics become less effective. In high-dimensional datasets, the data becomes sparse, and the differences in distances between samples become smaller and less meaningful. Consequently, the algorithm may lose accuracy and fail to identify relevant patterns in the data.

> An intuitive way to understand this problem is to imagine that you want to find out which fruit is most similar to an orange, using color and weight as criteria. With just these two metrics, the comparison is simple and can even be done visually. However, as more features are added, such as density and water content, the analysis becomes more complex. For example, one fruit might have a very different color and weight compared to an orange but share similar density and water content, while another fruit might show the opposite. This illustrates how classification becomes challenging as the number of dimensions increases, because which fruit is truly more similar to an orange?

### 2.2 How to mitigate the problem?
Regarding the curse of dimensionality, one way to address the problem is to develop specific distance metrics tailored to the data. However, this approach can be technically complex and time-consuming to implement effectively.

> [!NOTE]
> An interesting alternative is the use of the concept of hubness. This phenomenon occurs when certain samples, called hubs, have a high likelihood of being chosen as the nearest neighbors in various contexts. As the number of dimensions increases, the tendency for hubs to appear grows. Therefore, utilizing hubs can increase the accuracy of the k-NN algorithm for datasets with a large number of dimensions, as the hubs become reference points within the set, facilitating the identification of patterns.

## 3. Overfitting and Underfitting
As presented in the previous section, overfitting and underfitting can occur due to the chosen value of k. Therefore, selecting an appropriate value for k is essential to avoid these problems and ensure good algorithm performance.

> [!IMPORTANT]
> For more information about overfitting, underfitting, and how to determine the value of k, visit: [How to determine the value of K](https://github.com/mevianna/ISA/blob/main/KNN/1.concepts/3.how_to_determine_the_value_of_K.md)

## References
**IBM.** _K-nearest neighbors (KNN)_. Dispon√≠vel em: https://www.ibm.com/think/topics/knn. Acesso em: 21 de abril de 2025.
**MARIZ, Filipe Mendes.** _Avalia√ß√£o e compara√ß√£o de vers√µes modificadas do algoritmo KNN_. 2017. [s.l.], Universidade Federal de Pernambuco, Centro de Inform√°tica, 2017. Dispon√≠vel em: https://www.cin.ufpe.br/~tg/2017-2/fmm4-tg.pdf. Acesso em: 23 de abril de 2025.

## üëæ **Contributors**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/178849007?v=4" width=115><br><sub>Rafaela Savaris</sub>](https://github.com/rafasavaris) |
| :---: |

## **License**  
[![Licen√ßa MIT](https://img.shields.io/badge/Licen√ßa-MIT-blue.svg)](https://pt.wikipedia.org/wiki/Licen%C3%A7a_MIT)  
**Traslation:** Use, modify, and share at will! ‚úåÔ∏è

***

# Portuguese version

Embora o k-NN seja um algoritmo relativamente simples, ele apresenta alguns problemas comuns que se tornam mais evidentes a medida que o n√∫mero de amostras aumenta. Os principais problemas s√£o:

## 1. Alto custo computacional
Conforme a quantidade de dados de um algoritmo k-NN cresce, a complexidade do algoritmo e seu custo computacional tamb√©m crescem. Isso ocorre pois √© preciso calcular a dist√¢ncia entre o novo dado e todos os outros j√° existentes, o que exige mais mem√≥ria para armazenar essas dist√¢ncias e as compara√ß√µes feitas com os k vizinhos mais pr√≥ximos. Como resultado, al√©m do aumento do custo computacional, o tempo de processamento tamb√©m cresce para conjuntos de dados de teste maior.

### 1.1 Como amenizar o problema?
Para diminuir esse problema, o ideal √© reduzir a quantidade de dados processados pelo algoritmo. Uma das estrat√©gias para isso, √© o condensamento de informa√ß√µes, que consiste na sele√ß√£o de subconjuntos relevantes para an√°lise e na elimina√ß√£o de amostras redundantes ou at√© mesmo de outliers. Assim, o algoritmo n√£o perde sua precis√£o e mant√©m a efici√™ncia.

<details>
  <summary><strong>Como os subconjuntos s√£o escolhidos?</strong></summary>
  Para a escolha dos subconjuntos de dados, diversas estrat√©gias podem ser adotadas. Uma delas √© a sele√ß√£o de vari√°veis relevantes atrav√©s de t√©cnicas como an√°lise de componentes principais (PCA), an√°lise de correla√ß√£o entre atributos ou o uso de algoritmos de aprendizado supervisionado que identificam as caracter√≠sticas mais influentes na classifica√ß√£o. Outra estrat√©gia √© o uso de algoritmos como Locality-Sensitive Hashing (LSH) ou estruturas como KD-Tree, que aproximam os prov√°veis vizinhos pr√≥ximos, sem a necessidade de comparar o novo ponto com todos os dados do conjunto, reduzindo significativamente o tempo e o custo computacional.
</details>

## 2. Maldi√ß√£o da dimensionalidade
Outro desafio do algoritmo k-NN √© a maldi√ß√£o da dimensionalidade. Como o algoritmo k-NN √© mais propenso a funcionar em situa√ß√µes com poucos param√™tros (dimens√µes), √† medida que o n√∫mero de dimens√µes aumenta, menos as m√©tricas de dist√¢ncias s√£o eficazes. Isso acontece pois em conjuntos com muitas dimens√µes, os dados ficam dispersos, e as diferen√ßas de dist√¢ncias nas amostras se tornam pequenas e menos relevantes. Dessa maneira, o algoritmo perde sua precis√£o e pode n√£o identificar padr√µes relevantes nos dados.

> Uma forma intuitiva de entender esse problema √© imaginar que voc√™ deseja descobrir qual fruta √© mais parecida com uma laranja, usando como crit√©rios a cor e o peso. Com apenas essas duas m√©tricas, a compara√ß√£o √© simples e pode at√© ser feita visualmente. No entanto, ao adicionar muitas outras caracter√≠sticas, como densidade e teor de √°gua, a an√°lise se torna mais confusa. Por exemplo, uma fruta pode ter cor e peso bem diferentes de uma laranja, mas apresentar densidade e teor de √°gua semelhantes; enquanto outra pode ter caracter√≠sticas opostas a essa. Isso mostra a dificuldade de classifica√ß√£o, quando o n√∫mero de dimens√µes √© grande, pois qual das frutas √© mais semelhante a uma laranja?

### 2.1 Como amenizar o problema?
Quanto a maldi√ß√£o da dimensionalidade, uma das formas de lidar com o problema √© desenvolver m√©tricas especificas de dist√¢ncias. No entanto, essa alternativa pode ser complexa e demorada.

> [!NOTE]
>  Uma alternativa interessante, √© o uso do conceito de Hubness. Esse fen√¥meno ocorre quando certas amostras, denominadas *hubs*, apresentam uma alta tend√™ncia a serem escolhidas como o vizinho mais pr√≥ximo de outras amostras. Assim, quanto maior o n√∫mero de dimens√µes, maior a tend√™ncia de aparecer hubs. Portanto, a utiliza√ß√£o de hubs pode aumentar a precis√£o do algoritmo k-NN para algoritmos com um n√∫mero elevado de dimens√µes, pois os hubs se tornam refer√™ncias do conjunto, facilitando a identifica√ß√£o de padr√µes.

## 3. Overfitting e underfitting
Como apresentado na se√ß√£o anterior, o overfitting e o underfitting podem ocorrer devido ao valor escolhido de k. Portanto, a escolha de um valor adequado para k √© essencial para evitar esses problemas e garantir um bom desempenho do algoritmo.

> [!IMPORTANT]
> Para mais informa√ß√µes sobre overfitting, underfitting e o valor de k, acesse: [How to determine the value of K](https://github.com/mevianna/ISA/blob/main/KNN/1.concepts/3.how_to_determine_the_value_of_K.md)

### Refer√™ncias
**IBM.** _K-nearest neighbors (KNN)_. Dispon√≠vel em: https://www.ibm.com/think/topics/knn. Acesso em: 21 de abril de 2025.
**MARIZ, Filipe Mendes.** _Avalia√ß√£o e compara√ß√£o de vers√µes modificadas do algoritmo KNN_. 2017. [s.l.], Universidade Federal de Pernambuco, Centro de Inform√°tica, 2017. Dispon√≠vel em: https://www.cin.ufpe.br/~tg/2017-2/fmm4-tg.pdf. Acesso em: 23 de abril de 2025.

## üëæ **Contribuidores**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/178849007?v=4" width=115><br><sub>Rafaela Savaris</sub>](https://github.com/rafasavaris) | 
| :---: |

## **Licen√ßa**  
[![Licen√ßa MIT](https://img.shields.io/badge/Licen√ßa-MIT-blue.svg)](https://pt.wikipedia.org/wiki/Licen%C3%A7a_MIT)  
**Traslation:** Use, modifique e compartilhe √† vontade! ‚úåÔ∏è
