# English version

Although k-NN is a relatively simple algorithm, it presents several common challenges that become more evident as the number of samples increases. The main issues include:

## 1. High Computational Cost
As the volume of data in a k-NN algorithm grows, so do its complexity and computational cost. This is because the algorithm needs to calculate the distance between a new data point and all existing points, which requires more memory to store these distances and to perform comparisons with the k nearest neighbors. As a result, both computational cost and processing time increase, especially for larger test datasets.

### 1.1 How to mitigate the problem?
To reduce this issue, the ideal approach is to limit the amount of data processed by the algorithm. One effective strategy is data condensation, which involves selecting relevant subsets for analysis and removing redundant samples or even outliers. This helps the algorithm maintain both accuracy and efficiency.

<details>
    <summary><strong>How are these subsets selected?</strong></summary> 
    Various strategies can be used to select data subsets. One common approach is the selection of relevant features using techniques such as Principal Component Analysis (PCA), correlation analysis, or supervised learning algorithms that identify the most influential features for classification. Another strategy involves using methods like Locality-Sensitive Hashing (LSH) or structures like KD-Tree, which approximate the nearest neighbors without comparing the new point to the entire dataset, thereby significantly reducing both time and computational cost.
</details>

## 2. Curse of Dimensionality
Another major challenge of the k-NN algorithm is the curse of dimensionality. The k-NN method performs best in low-dimensional spaces, but as the number of dimensions increases, distance metrics become less effective. In high-dimensional datasets, the data becomes sparse, and the differences in distances between samples become smaller and less meaningful. Consequently, the algorithm may lose accuracy and fail to identify relevant patterns in the data.

> An intuitive way to understand this problem is to imagine that you want to find out which fruit is most similar to an orange, using color and weight as criteria. With just these two metrics, the comparison is simple and can even be done visually. However, as more features are added, such as density and water content, the analysis becomes more complex. For example, one fruit might have a very different color and weight compared to an orange but share similar density and water content, while another fruit might show the opposite. This illustrates how classification becomes challenging as the number of dimensions increases, because which fruit is truly more similar to an orange?

### 2.2 How to mitigate the problem?
Regarding the curse of dimensionality, one way to address the problem is to develop specific distance metrics tailored to the data. However, this approach can be technically complex and time-consuming to implement effectively.

> An interesting alternative is the use of the concept of hubness. This phenomenon occurs when certain samples, called hubs, have a high likelihood of being chosen as the nearest neighbors in various contexts. As the number of dimensions increases, the tendency for hubs to appear grows. Therefore, utilizing hubs can increase the accuracy of the k-NN algorithm for datasets with a large number of dimensions, as the hubs become reference points within the set, facilitating the identification of patterns.

## 3. Overfitting and Underfitting
As presented in the previous section, overfitting and underfitting can occur due to the chosen value of k. Therefore, selecting an appropriate value for k is essential to avoid these problems and ensure good algorithm performance.

> [!IMPORTANT]
> For more information about overfitting, underfitting, and how to determine the value of k, visit: [How to determine the value of K](https://github.com/mevianna/ISA/blob/main/KNN/1.concepts/3.How_to_determine_the_value_of_K.md)

***

# Portuguese version

Embora o k-NN seja um algoritmo relativamente simples, ele apresenta alguns problemas comuns que se tornam mais evidentes a medida que o número de amostras aumenta. Os principais problemas são:

## 1. Alto custo computacional
Conforme a quantidade de dados de um algoritmo k-NN cresce, a complexidade do algoritmo e seu custo computacional também crescem. Isso ocorre pois é preciso calcular a distância entre o novo dado e todos os outros já existentes, o que exige mais memória para armazenar essas distâncias e as comparações feitas com os k vizinhos mais próximos. Como resultado, além do aumento do custo computacional, o tempo de processamento também cresce para conjuntos de dados de teste maior.

### 1.1 Como amenizar o problema?
Para diminuir esse problema, o ideal é reduzir a quantidade de dados processados pelo algoritmo. Uma das estratégias para isso, é o condensamento de informações, que consiste na seleção de subconjuntos relevantes para análise e na eliminação de amostras redundantes ou até mesmo de outliers. Assim, o algoritmo não perde sua precisão e mantém a eficiência.

<details>
  <summary><strong>Como os subconjuntos são escolhidos?</strong></summary>
  Para a escolha dos subconjuntos de dados, diversas estratégias podem ser adotadas. Uma delas é a seleção de variáveis relevantes através de técnicas como análise de componentes principais (PCA), análise de correlação entre atributos ou o uso de algoritmos de aprendizado supervisionado que identificam as características mais influentes na classificação. Outra estratégia é o uso de algoritmos como Locality-Sensitive Hashing (LSH) ou estruturas como KD-Tree, que aproximam os prováveis vizinhos próximos, sem a necessidade de comparar o novo ponto com todos os dados do conjunto, reduzindo significativamente o tempo e o custo computacional.
</details>

## 2. Maldição da dimensionalidade
Outro desafio do algoritmo k-NN é a maldição da dimensionalidade. Como o algoritmo k-NN é mais propenso a funcionar em situações com poucos paramêtros (dimensões), à medida que o número de dimensões aumenta, menos as métricas de distâncias são eficazes. Isso acontece pois em conjuntos com muitas dimensões, os dados ficam dispersos, e as diferenças de distâncias nas amostras se tornam pequenas e menos relevantes. Dessa maneira, o algoritmo perde sua precisão e pode não identificar padrões relevantes nos dados.

> Uma forma intuitiva de entender esse problema é imaginar que você deseja descobrir qual fruta é mais parecida com uma laranja, usando como critérios a cor e o peso. Com apenas essas duas métricas, a comparação é simples e pode até ser feita visualmente. No entanto, ao adicionar muitas outras características, como densidade e teor de água, a análise se torna mais confusa. Por exemplo, uma fruta pode ter cor e peso bem diferentes de uma laranja, mas apresentar densidade e teor de água semelhantes; enquanto outra pode ter características opostas a essa. Isso mostra a dificuldade de classificação, quando o número de dimensões é grande, pois qual das frutas é mais semelhante a uma laranja?

### 2.1 Como amenizar o problema?
Quanto a maldição da dimensionalidade, uma das formas de lidar com o problema é desenvolver métricas especificas de distâncias. No entanto, essa alternativa pode ser complexa e demorada.

>  Uma alternativa interessante, é o uso do conceito de Hubness. Esse fenômeno ocorre quando certas amostras, denominadas *hubs*, apresentam uma alta tendência a serem escolhidas como o vizinho mais próximo de outras amostras. Assim, quanto maior o número de dimensões, maior a tendência de aparecer hubs. Portanto, a utilização de hubs pode aumentar a precisão do algoritmo k-NN para algoritmos com um número elevado de dimensões, pois os hubs se tornam referências do conjunto, facilitando a identificação de padrões.

## 3. Overfitting e underfitting
Como apresentado na seção anterior, o overfitting e o underfitting podem ocorrer devido ao valor escolhido de k. Portanto, a escolha de um valor adequado para k é essencial para evitar esses problemas e garantir um bom desempenho do algoritmo.

> [!IMPORTANT]
> Para mais informações sobre overfitting, underfitting e o valor de k, acesse: [How to determine the value of K](https://github.com/mevianna/ISA/blob/main/KNN/1.concepts/3.How_to_determine_the_value_of_K.md))
