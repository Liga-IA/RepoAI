# English version

Although k-NN is a relatively simple algorithm, it presents some challenges that become more evident as the number of samples increases. The main issues are:

## High Computational Cost
As the amount of data in a k-NN algorithm grows, its complexity and computational cost also increase. This happens because the distance between the new data point and all existing ones must be calculated, requiring more memory to store these distances and to perform comparisons with the k nearest neighbors. As a result, in addition to the rise in computational cost, the processing time also increases for larger test datasets.

### How to mitigate the problem?
To alleviate this issue, it is ideal to reduce the amount of data processed by the algorithm. One strategy for doing so is information condensation, which involves selecting relevant subsets for analysis and eliminating redundant samples. In this way, the algorithm maintains its accuracy while improving its efficiency.

***

## Curse of Dimensionality
Another challenge for the k-NN algorithm is the curse of dimensionality. Since k-NN is more effective in situations with few parameters (dimensions), as the number of dimensions increases, distance metrics become less effective. This occurs because the differences in distances between samples become smaller and less significant. As a result, the algorithm loses accuracy and may fail to identify important patterns in the data.

### How to mitigate the problem?
Regarding the curse of dimensionality, one way to address the issue is by developing specific formulas for distance calculation. However, this alternative can be complex and require significant effort.

> Another approach is through the concept of Hubness. This phenomenon occurs when certain samples, called hubs, have a high tendency to be selected as the nearest neighbor of other samples. Thus, the higher the number of dimensions, the more likely hubs are to appear. Therefore, utilizing hubs can improve the accuracy of the k-NN algorithm when dealing with high-dimensional datasets.

***

## Overfitting and Underfitting
As presented in the previous section, overfitting and underfitting can occur due to the chosen value of k. Therefore, selecting an appropriate value for k is essential to avoid these problems and ensure good algorithm performance.

> [!IMPORTANT]
> For more information about overfitting, underfitting, and how to determine the value of k, visit: [How to determine the value of K](https://github.com/mevianna/ISA/blob/main/KNN/1.concepts/3.How%20to%20determine%20the%20value%20of%20K.md)


# Portuguese version

Embora o k-NN seja um algoritmo relativamente simples, ele apresenta alguns desafios que se tornam mais evidentes a medida que o número de amostras aumenta. Os principais problemas são:

## Alto custo computacional
Conforme a quantidade de dados de um algoritmo k-NN cresce, a complexidade do algoritmo e seu custo computacional também aumentam. Isso ocorre pois é calculada a distância entre o novo dado à todos os outros já existentes, exigindo mais memória para armazenar essas distâncias e as comparações feitas com os k vizinhos mais próximos. Como resultado, além do aumento do custo computacional, o tempo de processamento também cresce, para um conjunto de dados de teste maior.

### Como amenizar o problema?
Para diminuir esse problema, o ideal é reduzir a quantidade de dados processados pelo algoritmo. Uma das estratégias para isso, é o condensamento de informações, que consiste na seleção de subconjuntos relevantes para análise e na eliminação de amostras redundantes. Assim, o algoritmo não perde sua precisão e mantém a eficiência.

***

## Maldição da dimensionalidade
Outro desafio do algoritmo k-NN é a maldição da dimensionalidade. Como o algoritmo k-NN é mais propenso a funcionar em situações com poucos paramêtros (dimensões), à medida que o número de dimensões aumenta, menos as métricas de distâncias são eficazes. Isso acontece pois as diferenças de distâncias nas amostras se torna pequenas e menos relevantes. Dessa maneira, o algoritmo perde sua precisão e pode não identificar padrões relevantes nos dados.

### Como amenizar o problema?
Quanto a maldição da dimensionalidade, uma das formas de lidar com o problema é desenvolver fórmulas especificas para o cálculo das distâncias. No entanto, essa alternativa pode ser complexa e demandar muito esforço.

>  Outra alternativa é através do conceito de Hubness. Esse fenômeno ocorre quando certas amostras, denominadas hubs, apresentam uma alta tendência a serem escolhidas como o vizinho mais próximo de outras amostras. Assim, quanto maior o número de dimensões, maior a tendência de aparecer hubs. Portanto, a utilização de hubs pode aumentar a precisão do algoritmo k-NN para algoritmos com um número elevado de dimensões.

***

## Overfitting e underfitting
Como apresentado na seção anterior, o overfitting e o underfitting podem ocorrer devido ao valor escolhido de k. Portanto, a escolha de um valor adequado para k é essencial para evitar esses problemas e garantir um bom desempenho do algoritmo.

> [!IMPORTANT]
> Para mais informações sobre overfitting, underfitting e o valor de k, acesse: [How to determine the value of K](https://github.com/mevianna/ISA/blob/main/KNN/1.concepts/3.How%20to%20determine%20the%20value%20of%20K.md)
