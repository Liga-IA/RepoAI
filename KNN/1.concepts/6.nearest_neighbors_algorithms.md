# English Version

As previously explained in: [5.Fields_of_use.md](https://github.com/mevianna/ISA/blob/main/KNN/1.concepts/5.Fields_of_use.md) , the nearest neighbor method is commonly used to find media similar to the one being searched. For example, the recommendation systems of platforms such as Netflix and Spotify use this technique to suggest content similar to what the user has shown interest in.

However, in order for these recommendation systems to be fast and efficient, the approximate nearest neighbor method is employed. Below, we explore the difference between the exact nearest neighbor** method and the approximate nearest neighbor method.

## Exact Nearest Neighbor:

This algorithm compares the input with all available data points and finds the one that is closest‚Äîthat is, the one with the smallest distance. In this case, the computational cost is proportional to the number of existing data points. This process is known as exhaustive search or brute-force search in the context of algorithms.

Therefore, it becomes evident that for very large datasets‚Äîsuch as those used by the platforms mentioned earlier‚Äîthis technique can be extremely time-consuming and require a high amount of computational resources.

As such, the approximate nearest neighbor algorithm becomes more efficient and practical, especially when dealing with large and/or high-dimensional datasets.

## Approximate Nearest Neighbor:

This algorithm seeks a data point that is sufficiently close to the query point, though not necessarily the closest of all. Thus, this approach proves to be quite effective in the previously mentioned scenario, since recommendations that are merely similar to the user‚Äôs interests and searches are usually enough to spark interest in new media.

The implementation of this algorithm involves several stages:

- Dimensionality Reduction: Reducing of the dimensionality of the data is essential for making the model more efficient, as it simplifies and speeds up the data analysis process.

- Vector Encoding: After reducing the dimensionality and storing the dataset as vectors, those vectors can be encoded‚Äîi.e., transformed into more compact structures‚Äîusing different data structures such as Trees, LSH, or Quantization, to improve search efficiency.

- Search: Finally, the search method is implemented, and it may vary depending on the data structure used.

## References
Compreendendo o algoritmo de vizinho mais pr√≥ximo aproximado (ANN). (2024, April 17). Elastic Blog. https://www.elastic.co/pt/blog/understanding-ann

(N.d.). Towardsdatascience.com. Retrieved May 4, 2025, from https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6/?source=post_page-----7e2c6f0778bc---------------------------------------

## üëæ **Contributors**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/197432407?v=4" width=115><br><sub>Beatriz Schuelter Tartare</sub>](https://github.com/beastartare) |
| :---: |

## **License**  
[![Licen√ßa MIT](https://img.shields.io/badge/Licen√ßa-MIT-blue.svg)](https://pt.wikipedia.org/wiki/Licen%C3%A7a_MIT)  
**Traslation:** Use, modify, and share at will! ‚úåÔ∏è

***

# Portuguese Version

Como explicado anteriormente em: [5.Fields_of_use.md](https://github.com/mevianna/ISA/blob/main/KNN/1.concepts/5.Fields_of_use.md) , o m√©todo do vizinho pr√≥ximo √© comumente utilizado para encontrar mid√≠as semelhantes √†quelas pesquisadas. Como exemplo, o sistema de recomenda√ß√£o de plataformas como Netflix e Spotify utilizam desse recurso para recomendar ao usu√°rio m√≠dias semelhantes a de interesse pesquisado por ele.
No entanto, para que esse sistema de recomenda√ß√£o seja r√°pido e eficiente utiliza-se o m√©todo do vizinho mais proximo aproximado. A seguir, podemos entender melhor a diferen√ßa entre o m√©todo do vizinho mais pr√≥ximo exato e o m√©todo do vizinho mais pr√≥ximo aproximado.

## Vizinho mais proximo exato:

Esse algoritmo compara uma entrada com todos os pontos de dados dispon√≠veis e encontra aquele mais pr√≥ximo, ou seja, com a menor dist√¢ncia. Nesse caso, o custo computacional √© proporcional ao n√∫mero de dados existentes. Tal processo √© conhecido como busca exaustiva ou busca por for√ßa bruta no contexto de algoritmos. 
Sendo assim, fica evidente que para um conjuno de dados muito grande, como o das plataformas exemplificadas anteriormente, essa t√©cnica pode ser muito demorada e necessitar de uma quantidade de processamento muito alta.

  
Desse modo, o algoritmo do vizinho mais pr√≥ximo aproximado se torna mais eficiente e √∫til, tendo em vista um conjunto de dados muito grande e/ou com muitas dimens√µes. 

## Vizinho mais proximo aproximado:

Nesse algoritmo, busca-se um ponto no conjunto de dados que esteja suficientemente pr√≥ximo do dado fornecido, n√£o sendo, necessariamente, o mais pr√≥ximo de todos. Sendo assim, essa abordagem √© bastante eficaz para o caso exemplificado anteriormente, uma vez que recomenda√ß√µes similares aos interesses e buscas do usu√°rio geralmente s√£o suficientes para despertar seu interesse por novas m√≠dias.

A implementa√ß√£o desse algoritmo se da em algumas etapas:

- Redu√ß√£o de dimensionalidade: a redu√ß√£o da dimens√£o dos dados √© fundamental para que o modelo seja mais eficiente, visto que a analise dos dados se torna mais simples e r√°pida.
  
- Codifica√ß√£o vetorial: ap√≥s reduzir a dimens√£o do conjunto de dados e armazen√°-los em um vetor, para tornar a busca mais r√°pida e eficiente, pode-se codificar o vetor,ou seja, transform√°-lo em uma estrutura mais compacta, atrav√©s de diferentes estruturas de dados, como √Årvores, LHS e Quantiza√ß√£o.
  
- Busca: por fim, implementa-se o m√©todo de busca. Esse m√©todo pode depender da estrutura de dados utilizada.

## Refer√™ncias
Compreendendo o algoritmo de vizinho mais pr√≥ximo aproximado (ANN). (2024, April 17). Elastic Blog. https://www.elastic.co/pt/blog/understanding-ann

(N.d.). Towardsdatascience.com. Retrieved May 4, 2025, from https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6/?source=post_page-----7e2c6f0778bc---------------------------------------

## üëæ **Contribuidores**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/197432407?v=4" width=115><br><sub>Beatriz Schuelter Tartare</sub>](https://github.com/beastartare) |
| :---: |

## **Licen√ßa**  
[![Licen√ßa MIT](https://img.shields.io/badge/Licen√ßa-MIT-blue.svg)](https://pt.wikipedia.org/wiki/Licen%C3%A7a_MIT)  
**Traslation:** Use, modifique e compartilhe √† vontade! ‚úåÔ∏è
