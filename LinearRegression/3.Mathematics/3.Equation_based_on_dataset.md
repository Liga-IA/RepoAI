# English version

## How to find the simple linear regression equation based on a dataset
As previously explained in [1.General_representation_of_linear_regression.md](./1.General_representation_of_linear_regression.md), the simple linear regression can be represented by the equation of a straight line: 

$$
\hat{y} = \beta_0 + \beta_1 x
$$

The values Î²â‚ and Î²â‚€ are unknown parameters, where Î²â‚ represents the slope of the line and Î²â‚€ represents the y-intercept.

Therefore, to determine the values of Î²â‚ and Î²â‚€, we must first collect all training data and organize it into pairs of coordinates (x, y). Ultimately, we will have a set of coordinates as represented below:

$$
\(x1, y1), (x2, y2),..., (xn, yn)
$$

Where n represents the total number of data points collected. 

As explained in [2.The_least_squares_method.md](./2.The_least_squares_method.md), we can calculate the values of $\hat{\beta_1}$ and $\hat{\beta_0}$ using the equations below:

$$
\hat{\beta_1} = \frac{\sum_{i = 1}^{n} (x_i â€“ \bar{x} ) ( y_i â€“ \bar{y} )}{\sum_{i = 1}^{n} ( x_i - \bar{x} )^2}
$$

$$
\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X}
$$

- $x_i$: x values of each coordinate
- $y_i$: y values of each coordinate
- xÌ„: simple arithmetic mean of the x values 
- yÌ„: simple arithmetic mean of the y values

## Example

From one of the coding examples, let's analyze the relationship between hours studied and grades of a student.

| Hours Studied | Exam Score |
|--------------|------------|
| 1.5          | 50         |
| 3.0          | 55         |
| 4.5          | 65         |
| 6.0          | 70         |
| 7.5          | 80         |
| 9.0          | 85         |

Based on the data shown in the table above, we can calculate the values of Î²â‚ and Î²â‚€ and define the equation of the line. For this purpose, we will define Hours Studied as x values and Exam Scores as y values. Thus, organizing all the data into pairs of coordinates, we will have the following points: (1.5, 50), (3.0, 55), (4.5, 65), (6.0, 70), (7.5, 80), (9.0, 85).

To facilitate the development of the Î²â‚ calculation, we will compute the variables and the summations independently, as exemplified below:

- xÌ„ = (1.5 + 3.0 + 4.5 + 6.0 + 7.5 + 9.0)/6 = 5.25
- yÌ„ =(50 + 55 + 65 + 70 + 80 + 85)/6 = 67.5
  
$$
\sum_{i = 1}^{n} (x_i â€“ \bar{x} ) ( y_i â€“ \bar{y} ) = (1.5 - 5.25)(50 - 67.5) + (3.0 - 5.25)(55 - 67.5) + (4.5 - 5.25)(65 -  67.5) + (6.0 - 5.25)(70 - 67.5) + (7.5 - 5.25)(80 - 67.5) + (9.0 - 5.25)(85 - 67.5) = 191.25
$$

$$
\sum_{i = 1}^{n} ( x_i - \bar{x} )^2 = (1.5 - 5.25)^2 + (3.0 - 5.25)^2 + (4.5 - 5.25)^2 + (6.0 - 5.25)^2 + (7.5 - 5.25)^2 + (9.0 - 5.25)^2 = 39.375
$$

By substituting the values into the original equation, we will arrive at:

$$
\hat{\beta}_1 = \frac{191.25}{39.375} = 4.85
$$

With the preceding steps completed, we are now able to calculate the value of Î²â‚€, as indicated below:

$$
\hat{\beta}_0 = 67.5 - 4.85*5.25 = 42.03
$$

Upon substitution of the Î²â‚ and Î²â‚€ values into the linear equation, the following expression is obtained:

$$
\hat{y} = 42.05 + 4.85 x
$$

### Least Squares Error Calculation for the Exam Score Example

The least squares error \( S \) is calculated as:

$$
S = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

#### Step 1: Calculate Predicted Values ($(\hat{y}_i$\))

For each $x_i$, compute $\hat{y}_i$:

- $x = 1.5$: $\hat{y} = 42.05 + 4.85 \times 1.5 = 49.325$
- $x = 3.0$: $\hat{y} = 42.05 + 4.85 \times 3.0 = 56.6$
- $x = 4.5$: $\hat{y} = 42.05 + 4.85 \times 4.5 = 63.875$
- $x = 6.0$: $\hat{y} = 42.05 + 4.85 \times 6.0 = 71.15$
- $x = 7.5$: $\hat{y} = 42.05 + 4.85 \times 7.5 = 78.425$
- $x = 9.0$: $\hat{y} = 42.05 + 4.85 \times 9.0 = 85.7$

#### Step 2: Compute Differences (( y_i - $\hat{y}_i$\))

Subtract the predicted values from the observed values:

- \( 50 - 49.325 = 0.675 \)
- \( 55 - 56.6 = -1.6 \)
- \( 65 - 63.875 = 1.125 \)
- \( 70 - 71.15 = -1.15 \)
- \( 80 - 78.425 = 1.575 \)
- \( 85 - 85.7 = -0.7 \)

#### Step 3: Square the Differences

Square each difference:

- \( (0.675)^2 = 0.455625 \)
- \( (-1.6)^2 = 2.56 \)
- \( (1.125)^2 = 1.265625 \)
- \( (-1.15)^2 = 1.3225 \)
- \( (1.575)^2 = 2.480625 \)
- \( (-0.7)^2 = 0.49 \)

#### Step 4: Sum the Squared Differences

Add the squared differences to find \( S \):

$$
S = 0.455625 + 2.56 + 1.265625 + 1.3225 + 2.480625 + 0.49 = 8.574375
$$

#### Result

The least squares error for this model is approximately **8.57**.


## References
Universidade Federal do Rio Grande do Sul. (n.d.). Probabilidade e EstatÃ­stica. Retrieved March 30, 2025, from https://www.ufrgs.br/probabilidade-estatistica/livro/livro_completo/ch7-reg-simples.html

HASTIE, Trevor; TIBSHIRANI, Robert; FRIEDMAN, Jerome. An introduction to statistical learning. 2009.

Weisberg, S. Applied linear regression (4th ed.). Wiley. Retrieved March 30, 2025, from https://www.stat.purdue.edu/~qfsong/teaching/525/book/Weisberg-Applied-Linear-Regression-Wiley.pdf

## Where am I?

```text
RepoAI/
â””â”€â”€ Linear Regression/
    â”œâ”€â”€ 1.Concepts/
    â”‚   â””â”€â”€ Figures/
    â”‚   â””â”€â”€ 1.History.md
    |   â””â”€â”€ 2.Typical_problems.md
    |   â””â”€â”€ 3.Potential_issues.md
    |   â””â”€â”€ 4.Fields_of_use.md
    â”œâ”€â”€ 2.Code/
    |   â””â”€â”€ Figures/
    |   â””â”€â”€ 1.Dive_into_the_docs.md  
    |   â””â”€â”€ 2.Boston_housing_price_reg.md
    |   â””â”€â”€ 3.Student_study_hours_prediction.md
    â””â”€â”€ 3.Mathematics/
    |   â””â”€â”€ Figures/
    |   â””â”€â”€ 1.General_representation_of_linear_regression.md 
    |   â””â”€â”€ 2.The_least_square_method.md 
    |   â””â”€â”€ 3.Equation_based_on_dataset.md   <---- You are here!!
```

## ğŸ‘¾ **Contributors**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/160762179?v=4" width=115><br><sub>Maria Eduarda Vianna</sub>](https://github.com/mevianna) |  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/197432407?v=4" width=115><br><sub>Beatriz Schuelter Tartare</sub>](https://github.com/beastartare) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/178849007?v=4" width=115><br><sub>Rafaela Savaris</sub>](https://github.com/rafasavaris) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/105316221?v=4" width=115><br><sub>VinÃ­cius Muchulski</sub>](https://github.com/vini-muchulski) | 
| :---: | :---: | :---: | :---: |

## **License**  
[![LicenÃ§a MIT](https://img.shields.io/badge/LicenÃ§a-MIT-blue.svg)](https://pt.wikipedia.org/wiki/Licen%C3%A7a_MIT)  
**Traslation:** Use, modify, and share at will! âœŒï¸

# Portuguese version

## Como encontrar a equaÃ§Ã£o com base em um conjunto de dados

Como explicado anteriormente em [1.General_representation_of_linear_regression.md](./1.General_representation_of_linear_regression.md), a regressÃ£o linear simples pode ser representada por uma reta:

$$
\hat{y} = \beta_0 + \beta_1 x
$$

Os valores Î²â‚ and Î²â‚€ sÃ£o parÃ¢metros desconhecidos, enquanto Î²â‚ representa a inclinaÃ§Ã£o da reta e Î²â‚€ representa a intersecÃ§Ã£o com o eixo y.

Portanto, para determinar os valores de Î²â‚ e Î²â‚€, devemos primeiro coletar todos os dados de treinamento e organizÃ¡-los em pares de coordenadas (x, y). No final, teremos um conjunto de coordenadas assim como representado abaixo:

$$
\(x1, y1), (x2, y2),..., (xn, yn)
$$

Em que n representa o nÃºmero total de ponto de dados coletados. Enfim, podemos calcular os valores Î²1 e Î²0 utilizando as duas equaÃ§Ãµes abaixo:

As explained in [2.The_least_squares_method.md](./2.The_least_squares_method.md), we can calculate the values of $\hat{\beta_1}$ and $\hat{\beta_0}$ using the equations below:

$$
\hat{\beta_1} = \frac{\sum_{i = 1}^{n} (x_i â€“ \bar{x} ) ( y_i â€“ \bar{y} )}{\sum_{i = 1}^{n} ( x_i - \bar{x} )^2}
$$

$$
\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X}
$$

- $x_i$: x values of each coordinate
- $y_i$: y values of each coordinate
- xÌ„: simple arithmetic mean of the x values 
- yÌ„: simple arithmetic mean of the y values

## Exemplo

A partir de um dos exemplos de cÃ³digo, vamos analisar a relaÃ§Ã£o entre horas de estudo e notas de um aluno.

| Hours Studied | Exam Score |
|--------------|------------|
| 1.5          | 50         |
| 3.0          | 55         |
| 4.5          | 65         |
| 6.0          | 70         |
| 7.5          | 80         |
| 9.0          | 85         |

Com base nos dados mostrados na tabela acima, podemos calcular os valores de Î²â‚ e Î²â‚€ e definir a equaÃ§Ã£o da reta. Para isso, definiremos Horas de Estudo como valores x e Notas do Exame como valores y. Assim, organizando todos os dados em pares de coordenadas, teremos os seguintes pontos: (1.5, 50), (3.0, 55), (4.5, 65), (6.0, 70), (7.5, 80), (9.0, 85).

Para facilitar o desenvolvimento do cÃ¡lculo de Î²â‚, calcularemos as variÃ¡veis e os somatÃ³rios separadamente, conforme exemplificado abaixo:

- xÌ„ = (1.5 + 3.0 + 4.5 + 6.0 + 7.5 + 9.0)/6 = 5.25
- yÌ„ =(50 + 55 + 65 + 70 + 80 + 85)/6 = 67.5
  
$$
\sum_{i = 1}^{n} (x_i â€“ \bar{x} ) ( y_i â€“ \bar{y} ) = (1.5 - 5.25)(50 - 67.5) + (3.0 - 5.25)(55 - 67.5) + (4.5 - 5.25)(65 - 67.5) + (6.0 - 5.25)(70 - 67.5) + (7.5 - 5.25)(80 - 67.5) + (9.0 - 5.25)(85 - 67.5) = 191.25
$$

$$
\sum_{i = 1}^{n} ( x_i - \bar{x} )^2 = (1.5 - 5.25)^2 + (3.0 - 5.25)^2 + (4.5 - 5.25)^2 + (6.0 - 5.25)^2 + (7.5 - 5.25)^2 + (9.0 - 5.25)^2 = 39.375
$$

Substituindo os valores na equaÃ§Ã£o original, chegaremos a:

$$
\hat{\beta}_1 = \frac{191.25}{39.375} = 4.85
$$

Com os passos anteriores concluÃ­dos, agora podemos calcular o valor de Î²â‚€, conforme indicado abaixo:

$$
\hat{\beta}_0 = 67.5 - 4.85*5.25 = 42.03
$$

ApÃ³s a substituiÃ§Ã£o dos valores de Î²â‚ e Î²â‚€ na equaÃ§Ã£o linear, a seguinte expressÃ£o Ã© obtida:

$$
\hat{y} = 42.05 + 4.85 x
$$

### MÃ©todo dos mÃ­nimos quadrados para o exemplo

A soma dos mÃ­nimos quadrados \( S \) Ã© calculada como:

$$
S = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

#### Passo 1: Calcular valores previstos ($(\hat{y}_i$\))

Para cada $x_i$, computar $\hat{y}_i$:

- $x = 1.5$: $\hat{y} = 42.05 + 4.85 \times 1.5 = 49.325$
- $x = 3.0$: $\hat{y} = 42.05 + 4.85 \times 3.0 = 56.6$
- $x = 4.5$: $\hat{y} = 42.05 + 4.85 \times 4.5 = 63.875$
- $x = 6.0$: $\hat{y} = 42.05 + 4.85 \times 6.0 = 71.15$
- $x = 7.5$: $\hat{y} = 42.05 + 4.85 \times 7.5 = 78.425$
- $x = 9.0$: $\hat{y} = 42.05 + 4.85 \times 9.0 = 85.7$

#### Passo 2: Calcular a diferenÃ§a (( y_i - $\hat{y}_i$\))

- \( 50 - 49.325 = 0.675 \)
- \( 55 - 56.6 = -1.6 \)
- \( 65 - 63.875 = 1.125 \)
- \( 70 - 71.15 = -1.15 \)
- \( 80 - 78.425 = 1.575 \)
- \( 85 - 85.7 = -0.7 \)

#### Passo 3: Fazer a diferenÃ§a ao quadrado

- \( (0.675)^2 = 0.455625 \)
- \( (-1.6)^2 = 2.56 \)
- \( (1.125)^2 = 1.265625 \)
- \( (-1.15)^2 = 1.3225 \)
- \( (1.575)^2 = 2.480625 \)
- \( (-0.7)^2 = 0.49 \)

#### Passo 4: Somar os quadrados das diferenÃ§as

$$
S = 0.455625 + 2.56 + 1.265625 + 1.3225 + 2.480625 + 0.49 = 8.574375
$$

#### Resultado

O erro dos mÃ­nimos quadrados para este modelo Ã© aproximadamente **8.57**.

## ReferÃªncias
Universidade Federal do Rio Grande do Sul. (n.d.). Probabilidade e EstatÃ­stica. Retrieved March 30, 2025, from https://www.ufrgs.br/probabilidade-estatistica/livro/livro_completo/ch7-reg-simples.html

HASTIE, Trevor; TIBSHIRANI, Robert; FRIEDMAN, Jerome. An introduction to statistical learning. 2009.

Weisberg, S. Applied linear regression (4th ed.). Wiley. Retrieved March 30, 2025, from https://www.stat.purdue.edu/~qfsong/teaching/525/book/Weisberg-Applied-Linear-Regression-Wiley.pdf

## Onde estou?

```text
RepoAI/
â””â”€â”€ Linear Regression/
    â”œâ”€â”€ 1.Concepts/
    â”‚   â””â”€â”€ Figures/
    â”‚   â””â”€â”€ 1.History.md
    |   â””â”€â”€ 2.Typical_problems.md
    |   â””â”€â”€ 3.Potential_issues.md
    |   â””â”€â”€ 4.Fields_of_use.md
    â”œâ”€â”€ 2.Code/
    |   â””â”€â”€ Figures/
    |   â””â”€â”€ 1.Dive_into_the_docs.md  
    |   â””â”€â”€ 2.Boston_housing_price_reg.md
    |   â””â”€â”€ 3.Student_study_hours_prediction.md
    â””â”€â”€ 3.Mathematics/
    |   â””â”€â”€ Figures/
    |   â””â”€â”€ 1.General_representation_of_linear_regression.md 
    |   â””â”€â”€ 2.The_least_square_method.md 
    |   â””â”€â”€ 3.Equation_based_on_dataset.md   <---- VocÃª estÃ¡ aqui!!
```

## ğŸ‘¾ **Contribuidores**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/160762179?v=4" width=115><br><sub>Maria Eduarda Vianna</sub>](https://github.com/mevianna) |  [<img loading="lazy" src="https://avatars.githubusercontent.com/u/197432407?v=4" width=115><br><sub>Beatriz Schuelter Tartare</sub>](https://github.com/beastartare) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/178849007?v=4" width=115><br><sub>Rafaela Savaris</sub>](https://github.com/rafasavaris) | [<img loading="lazy" src="https://avatars.githubusercontent.com/u/105316221?v=4" width=115><br><sub>VinÃ­cius Muchulski</sub>](https://github.com/vini-muchulski) | 
| :---: | :---: | :---: | :---: |

## **LicenÃ§a**  
[![LicenÃ§a MIT](https://img.shields.io/badge/LicenÃ§a-MIT-blue.svg)](https://pt.wikipedia.org/wiki/Licen%C3%A7a_MIT)  
**Traslation:** Use, modifique e compartilhe Ã  vontade! âœŒï¸
