# English version

## The least squares method

Let's say we want to determine the straight line equation of the linear regression model. It would be wise to think that we should have an equation that minimizes errors.
 
 The most common method for determining the best equation to represent the predictions is the least squares method, in which we minimize the sum of squared errors: $e_1^2 + e_2^2 + \dots + e_n^2$.

 > [!NOTE]
> As explained in [1.General_representation_of_linear_regression.md](./1.General_representation_of_linear_regression.md), the error $e_i$ represents the difference between the observed value $y_i$ and the predicted value $\hat{y}$: $e = y - \hat{y}_i$.

Therefore, more formally, the equation can be represented as:
$$S = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$

Here:
* $y_i$ represents the observed value of the dependent variable for each observation $i$;
* $\hat{y}_i$ represents the value estimated by the model;
* $S$ is the total sum of squared residuals, which we want to minimize;
* $N$ is the total number of observations.
 
 > [!IMPORTANT]
 > Think about why we minimize the squared errors instead of simply minimizing the sum of the errors ($|e_1| + |e_2| + \dots + |e_n|$).
 <details>
   <summary>Click to see the answer</summary>
   One reason is that squaring the errors makes bigger ones count more, helping the model focus on fixing those larger errors and giving a better overall fit to the data 
 </details>


As mentioned in [1.General_representation_of_linear_regression.md](./1.General_representation_of_linear_regression.md), the estimated value can be expressed using the equation of the regression line: $\hat{y} = \beta_0 + \beta_1 x_1$. By substituting this equation into the error expression, the result is:

$$
e = y - (\beta_0 + \beta_1 x_1)
$$

Therefore, the general equation for the sum of squared residuals is expressed as:

$$ S = \sum_{i=1}^{N} (y - [\beta_0 + \beta_1 x_1])^2 $$

To determine the values of $\beta_0$ and $\beta_0$, partial derivatives are taken with respect to each parameter and set to zero. The partial derivative with respect to $\beta_0$ is:

$$
\frac{\partial S}{\partial B_0} = -2 \sum_{i=1}^{N} (y_i - \beta_0 - \beta_1 x_i) = 0
$$

Solving for $\beta_0$, the equation simplifies to: $\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X}$, where **$(\bar{Y})$** is the mean of the observed values of the dependent variable ($y$) and **$(\bar{X})$** is the mean of the values of the independent variable ($x$). For $\beta_1$, the partial derivative is given by:

$$
\frac{\partial S}{\partial \beta_1} = -2 \sum_{i=1}^{N} (y_i - \beta_0 - \beta_1 x_i) x_i = 0
$$

Rearranging the equation leads to:

$$
\hat{\beta_1} = \frac{\sum_{i = 1}^{N} (x_i – \bar{x} ) ( y_i – \bar{y} )}{\sum_{i = 1}^{N} ( x_i - \bar{x} )^2}
$$
