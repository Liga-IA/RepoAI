## Practical Example — Study Hours vs. Probability of Passing

In this practical example, we use a hypothetical dataset that relates each student’s number of study hours to the probability of passing an exam, applying logistic regression to model this relationship in a binary classification setting.  
The dataset consists of four representative observations—students who studied for 2, 4, 6, and 8 hours—and their corresponding outcomes (passed = 1 or failed = 0), illustrating how continuous variables can be mapped to probabilities of a discrete event.

## Application
| Study Hours (x) | Passed (y) |
|------------------|------------|
|        1         |     0      | 
|        2         |     0      |
|        4         |     1      |
|        5         |     1      |

Initially, both $\beta_0$ and $\beta_1$ are assumed to be 0, along with the learning rate (0.1). The learning rate is chosen by the practitioner and not derived mathematically from the model, as it influences the optimization process rather than the statistical structure.

---

### Iteration 1

$\beta_0^{0}$ = 0 ; $\beta_1^{0}$ = 0 ; $\alpha$ = 0.1

#### Step 1: Calculating each $z^{(i)}$:

- $z^{(1)}$ = $\beta_0$ + $\beta_1\cdot x_1$ = $0 + 0\cdot1$ = 0 $\therefore$ $z^{(1)}$ = 0
- $z^{(2)}$ = $\beta_0$ + $\beta_1\cdot x_2$ = $0 + 0\cdot2$ = 0 $\therefore$ $z^{(2)}$ = 0
- $z^{(3)}$ = $\beta_0$ + $\beta_1\cdot x_3$ = $0 + 0\cdot4$ = 0 $\therefore$ $z^{(3)}$ = 0
- $z^{(4)}$ = $\beta_0$ + $\beta_1\cdot x_4$ = $0 + 0\cdot5$ = 0 $\therefore$ $z^{(4)}$ = 0

#### Step 2: Found each value of $\sigma$ given $z^{i}$ - a.k.a doing predictions - $\hat y^{(i)}$

- $\sigma(z^{(1)})$ = $\frac{1}{1 + e^{-z}}$ = $\frac{1}{1 + e^{-0}}$ $\therefore$ $\sigma(z^{(1)})$ = 0.5
- $\sigma(z^{(2)})$ = $\frac{1}{1 + e^{-z}}$ = $\frac{1}{1 + e^{-0}}$ $\therefore$ $\sigma(z^{(2)})$ = 0.5
- $\sigma(z^{(3)})$ = $\frac{1}{1 + e^{-z}}$ = $\frac{1}{1 + e^{-0}}$ $\therefore$ $\sigma(z^{(3)})$ = 0.5
- $\sigma(z^{(4)})$ = $\frac{1}{1 + e^{-z}}$ = $\frac{1}{1 + e^{-0}}$ $\therefore$ $\sigma(z^{(4)})$ = 0.5

#### Step 3: Calculating the gradients

- $\frac{\partial{\ell}}{\partial{\beta_0}}$ = $\sum(y^{(i)} - \hat y^{(i)})$ = $(0 - 0.5) + (0 - 0.5) + (1 - 0.5) + (1 - 0.5) \therefore \frac{\partial{\ell}}{\partial{\beta_0}} = 0$
- $\frac{\partial{\ell}}{\partial{\beta_1}}$ = $\sum(y^{(i)} - \hat y^{(i)})\cdot x^{(i)}$ = $[(0 - 0.5)\cdot1] + [(0 - 0.5)\cdot2] + [(1 - 0.5)\cdot4] + [(1 - 0.5)\cdot5] \therefore \frac{\partial{\ell}}{\partial{\beta_1}} = 3$

#### Step 4: Using the gradient ascending to update the weights - like any supercomputer does

- $\beta_0^{(1)} = \beta_0^{0} + \alpha \cdot \frac{\partial{\ell}}{\partial{\beta_0}} = 0 + 0.1 \cdot 0 \therefore \beta_0^{(1)} = 0$
- $\beta_1^{(1)} = \beta_1^{0} + \alpha \cdot \frac{\partial{\ell}}{\partial{\beta_1}} = 0 + 0.1 \cdot 3 \therefore \beta_1^{(1)} = 0.3$

#### Finally, after the first iteration our model looks like this


### Iteration 2

$\displaystyle \beta_{0}^{(1)} = 0,\quad \beta_{1}^{(1)} = 0.30,\quad \alpha = 0.1$

#### Step 1: Calculating each $z^{(i)}$
- $z^{(1)} = 0 + 0.30\cdot1 = 0.30$
- $z^{(2)} = 0 + 0.30\cdot2 = 0.60$
- $z^{(3)} = 0 + 0.30\cdot4 = 1.20$
- $z^{(4)} = 0 + 0.30\cdot5 = 1.50$

#### Step 2: Predictions $\hat y^{(i)}$
- $\hat y^{(1)} = \frac{1}{1+e^{-0.30}} \approx 0.5744$
- $\hat y^{(2)} = \frac{1}{1+e^{-0.60}} \approx 0.6457$
- $\hat y^{(3)} = \frac{1}{1+e^{-1.20}} \approx 0.7685$
- $\hat y^{(4)} = \frac{1}{1+e^{-1.50}} \approx 0.8176$

#### Step 3: Calculating the gradients
- $\displaystyle \frac{\partial \ell}{\partial \beta_0}
  = (-0.5744)+(-0.6457)+(0.2315)+(0.1824)
  \approx -0.8062$
- $\displaystyle \frac{\partial \ell}{\partial \beta_1}
  = (-0.5744\cdot1)+(-0.6457\cdot2)+(0.2315\cdot4)+(0.1824\cdot5)
  \approx -0.0278$

#### Step 4: Updating the weights
- $\displaystyle \beta_{0}^{(2)}
  = 0 + 0.1 \times (-0.8062)
  \approx -0.0806$
- $\displaystyle \beta_{1}^{(2)}
  = 0.30 + 0.1 \times (-0.0278)
  \approx 0.2972$

---

### Iteration 3

$\displaystyle \beta_{0}^{(2)} \approx -0.0806,\quad \beta_{1}^{(2)} \approx 0.2972,\quad \alpha = 0.1$

#### Step 1: Calculating each $z^{(i)}$
- $z^{(1)} = -0.0806 + 0.2972\cdot1 \approx 0.2166$
- $z^{(2)} = -0.0806 + 0.2972\cdot2 \approx 0.5138$
- $z^{(3)} = -0.0806 + 0.2972\cdot4 \approx 1.1082$
- $z^{(4)} = -0.0806 + 0.2972\cdot5 \approx 1.4054$

#### Step 2: Predictions $\hat y^{(i)}$
- $\hat y^{(1)} = \frac{1}{1+e^{-0.2166}} \approx 0.5549$
- $\hat y^{(2)} = \frac{1}{1+e^{-0.5138}} \approx 0.6257$
- $\hat y^{(3)} = \frac{1}{1+e^{-1.1082}} \approx 0.7516$
- $\hat y^{(4)} = \frac{1}{1+e^{-1.4054}} \approx 0.8030$

#### Step 3: Calculating the gradients
- $\displaystyle \frac{\partial \ell}{\partial \beta_0}
  = (-0.5549)+(-0.6257)+(0.2484)+(0.1970)
  \approx -0.7352$
- $\displaystyle \frac{\partial \ell}{\partial \beta_1}
  = (-0.5549\cdot1)+(-0.6257\cdot2)+(0.2484\cdot4)+(0.1970\cdot5)
  \approx 0.1723$

#### Step 4: Updating the weights
- $\displaystyle \beta_{0}^{(3)}
  = -0.0806 + 0.1 \times (-0.7352)
  \approx -0.1541$
- $\displaystyle \beta_{1}^{(3)}
  = 0.2972 + 0.1 \times 0.1723
  \approx 0.3144$



| $x^{(i)}$ | $y^{(i)}$ | $z^{(i)}$ | $\hat y^{(i)}$ | Erro = $y-\hat y$ | Erro = $x^{(i)}$ |
|:---------:|:---------:|:---------:|:--------------:|:-----------------:|:--------------:|
|     1     |     0     |     0     |      0,5       |       −0,5        |      −0,5      |
|     2     |     0     |     0     |      0,5       |       −0,5        |      −1,0      |
|     4     |     1     |     0     |      0,5       |       +0,5        |      +2,0      |
|     5     |     1     |     0     |      0,5       |       +0,5        |      +2,5      |


---

Thus, we use the sigmoid function:  
$$P(y=1 \mid x) = \sigma(\theta^T x) = \frac{1}{1 + e^{-(0 + 0 \cdot 2)}}$$

This probability will be the same for the other study hours.  
After calculating the probabilities, we use the gradients to compute the values of $b_0$ and $b_1$:

- $b_0$: $(0 - 0.5) + (0 - 0.5) + (1 - 0.5) + (1 - 0.5) = 0$  
- $b_1$: $2(0 - 0.5) + 4(0 - 0.5) + 6(1 - 0.5) + 8(1 - 0.5) = 4$

With the values of $b_0$ and $b_1$, we now update the coefficients:

- $b_0 = b_0^{(0)} + \text{learning rate} \cdot 0 = 0$  
- $b_1 = b_1^{(0)} + 0.1 \cdot 4 = 0.4$

These new values will be used in the next iteration.

*Probabilities:*

- $x=2 \Rightarrow 0.640$  
- $x=4 \Rightarrow 0.832$  
- $x=6 \Rightarrow 0.917$  
- $x=8 \Rightarrow 0.961$

*Gradients:*

- $b_0$: $(0 - 0.640) + (0 - 0.832) + (1 - 0.917) + (1 - 0.961) = -1.400$  
- $b_1$: $2(-0.640) + 4(-0.832) + 6(0.083) + 8(0.039) = -3.898$

*Updated coefficients:*

- $b_0 = 0 + 0.1 \cdot (-1.400) = -0.140$  
- $b_1 = 0.4 + 0.1 \cdot (-3.898) = 0.0102$
---
## References

- [Scielo - Regressão logística na saúde pública](https://www.scielo.br/j/rsocp/a/RWjPthhKDYbFQYydbDr3MgH/?lang=pt)
- [Aula 2 – Introdução à Regressão Logística – Geraldo Góes e Alexandre Ywata (ENAP)](https://repositorio.enap.gov.br/jspui/bitstream/1/3452/3/Aula%202%20-%20Geraldo%20Goes%20e%20Alexandre%20Ywata%20-%20Introdu%C3%A7%C3%A3o%20%C3%A0%20Regress%C3%A3o%20Log%C3%ADstica.pdf)
- [Monografia – Leandro Gonzalez (UFMA)](https://monografias.ufma.br/jspui/bitstream/123456789/3572/1/LEANDRO-GONZALEZ.pdf)
- [YouTube - Curso de Regressão Logística](https://www.youtube.com/watch?v=Qyg8jD2cWxA)

---

## Exemplo prático - Horas de estudo x Probabilidade de ser aprovado
Neste exemplo prático, utilizamos um conjunto de dados hipotético que relaciona o número de horas de estudo de cada estudante à probabilidade de aprovação em um exame, aplicando a regressão logística para modelar essa relação em um cenário de classificação binária.
A base de dados é composta por quatro observações representativas — estudantes que dedicaram 2, 4, 6 e 8 horas de estudo — e seus respectivos resultados (aprovado = 1 ou reprovado = 0), ilustrando como variáveis contínuas podem ser mapeadas em probabilidades de ocorrência de um evento discreto.

---

### Aplicação

| Horas de Estudo (x) | Aprovado (y) |
|---------------------|--------------|
| 2                   | 0            |
| 4                   | 1            |
| 6                   | 1            |
| 8                   | 1            |

Inicialmente, assume‑se $b_0$ e $b_1$ iguais a 0, assim como a taxa de aprendizado (0,1). A taxa de aprendizado é escolhida pelo praticante e não é derivada matematicamente do modelo, pois influencia o processo de otimização, não a estrutura estatística.

Assim, utilizamos a função sigmoide:  
$$
P(y=1 \mid x) = \sigma(\theta^T x) = \frac{1}{1 + e^{-(0 + 0 \cdot 2)}}
$$

Essa probabilidade será a mesma para as demais horas de estudo.  
Após calcular as probabilidades, usamos os gradientes para determinar os valores de $b_0$ e $b_1$:

- $b_0$: $(0 - 0,5) + (0 - 0,5) + (1 - 0,5) + (1 - 0,5) = 0$  
- $b_1$: $2(0 - 0,5) + 4(0 - 0,5) + 6(1 - 0,5) + 8(1 - 0,5) = 4$

Com esses valores, atualizamos os coeficientes:

- $b_0 = b_0^{(0)} + \text{taxa de aprendizado} \cdot 0 = 0$  
- $b_1 = b_1^{(0)} + 0,1 \cdot 4 = 0,4$

Esses novos valores serão usados na próxima iteração.

**Probabilidades:**

- $x=2 \;\Rightarrow\; 0,640$  
- $x=4 \;\Rightarrow\; 0,832$  
- $x=6 \;\Rightarrow\; 0,917$  
- $x=8 \;\Rightarrow\; 0,961$

**Gradientes:**

- $b_0$: $(0 - 0,640) + (0 - 0,832) + (1 - 0,917) + (1 - 0,961) = -1,400$  
- $b_1$: $2(-0,640) + 4(-0,832) + 6(0,083) + 8(0,039) = -3,898$

**Coeficientes atualizados:**

- $b_0 = 0 + 0,1 \cdot (-1,400) = -0,140$  
- $b_1 = 0,4 + 0,1 \cdot (-3,898) = 0,0102$
